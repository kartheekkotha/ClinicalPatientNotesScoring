{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68bd99b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:44.039297Z",
     "iopub.status.busy": "2022-04-25T11:25:44.037249Z",
     "iopub.status.idle": "2022-04-25T11:25:44.049812Z",
     "shell.execute_reply": "2022-04-25T11:25:44.049192Z"
    },
    "papermill": {
     "duration": 0.072122,
     "end_time": "2022-04-25T11:25:44.050017",
     "exception": false,
     "start_time": "2022-04-25T11:25:43.977895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6399208a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:44.161281Z",
     "iopub.status.busy": "2022-04-25T11:25:44.160220Z",
     "iopub.status.idle": "2022-04-25T11:25:44.162606Z",
     "shell.execute_reply": "2022-04-25T11:25:44.163301Z"
    },
    "papermill": {
     "duration": 0.060944,
     "end_time": "2022-04-25T11:25:44.163450",
     "exception": false,
     "start_time": "2022-04-25T11:25:44.102506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_coeff = {\n",
    "    'nbme_182': 0.55 * 0.5,\n",
    "    'nbme_182_full': 0.55 * 0.5,\n",
    "    'nbme_254': 0.55 * 0.5,\n",
    "    'nbme_254_full': 0.55 * 0.5,\n",
    "    'nbme_187': 0.45 * 0.60,\n",
    "    'nbme_187_full': 0.45 * 0.60,\n",
    "    'nbme_256': 0.45 * 0.40,\n",
    "    'nbme_256_full': 0.45 * 0.40,\n",
    "}\n",
    "\n",
    "pred_thr = -0.20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f4c332",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:44.274934Z",
     "iopub.status.busy": "2022-04-25T11:25:44.273986Z",
     "iopub.status.idle": "2022-04-25T11:25:44.276834Z",
     "shell.execute_reply": "2022-04-25T11:25:44.277356Z"
    },
    "papermill": {
     "duration": 0.060134,
     "end_time": "2022-04-25T11:25:44.277524",
     "exception": false,
     "start_time": "2022-04-25T11:25:44.217390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_coeff = {\n",
    "    'nbme_315': 1,\n",
    "}\n",
    "\n",
    "pred_thr = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9c2e338",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:44.393382Z",
     "iopub.status.busy": "2022-04-25T11:25:44.392407Z",
     "iopub.status.idle": "2022-04-25T11:25:44.416238Z",
     "shell.execute_reply": "2022-04-25T11:25:44.415655Z"
    },
    "papermill": {
     "duration": 0.085727,
     "end_time": "2022-04-25T11:25:44.416394",
     "exception": false,
     "start_time": "2022-04-25T11:25:44.330667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The following is necessary if you want to use the fast tokenizer for deberta v2 or v3\n",
    "# This must be done before importing transformers\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "transformers_path = Path(\"/opt/conda/lib/python3.7/site-packages/transformers\")\n",
    "\n",
    "input_dir = Path(\"../input/deberta-v2-3-fast-tokenizer\")\n",
    "\n",
    "convert_file = input_dir / \"convert_slow_tokenizer.py\"\n",
    "conversion_path = transformers_path/convert_file.name\n",
    "\n",
    "if conversion_path.exists():\n",
    "    conversion_path.unlink()\n",
    "\n",
    "shutil.copy(convert_file, transformers_path)\n",
    "deberta_v2_path = transformers_path / \"models\" / \"deberta_v2\"\n",
    "\n",
    "for filename in ['tokenization_deberta_v2.py', 'tokenization_deberta_v2_fast.py']:\n",
    "    filepath = deberta_v2_path/filename\n",
    "    if filepath.exists():\n",
    "        filepath.unlink()\n",
    "\n",
    "    shutil.copy(input_dir/filename, filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04da19eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:44.529004Z",
     "iopub.status.busy": "2022-04-25T11:25:44.523934Z",
     "iopub.status.idle": "2022-04-25T11:25:52.603016Z",
     "shell.execute_reply": "2022-04-25T11:25:52.602394Z"
    },
    "papermill": {
     "duration": 8.13489,
     "end_time": "2022-04-25T11:25:52.603176",
     "exception": false,
     "start_time": "2022-04-25T11:25:44.468286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "# For data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Pytorch Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Utils\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "# Sklearn Imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "\n",
    "# For Transformer Models\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, AdamW, AutoModelForQuestionAnswering\n",
    "from transformers import RobertaTokenizerFast\n",
    "from tokenizers import AddedToken\n",
    "\n",
    "# Suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f2441ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:52.716666Z",
     "iopub.status.busy": "2022-04-25T11:25:52.715586Z",
     "iopub.status.idle": "2022-04-25T11:25:52.718332Z",
     "shell.execute_reply": "2022-04-25T11:25:52.718873Z"
    },
    "papermill": {
     "duration": 0.061489,
     "end_time": "2022-04-25T11:25:52.719044",
     "exception": false,
     "start_time": "2022-04-25T11:25:52.657555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16426a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:52.833145Z",
     "iopub.status.busy": "2022-04-25T11:25:52.832121Z",
     "iopub.status.idle": "2022-04-25T11:25:52.859484Z",
     "shell.execute_reply": "2022-04-25T11:25:52.860168Z"
    },
    "papermill": {
     "duration": 0.087704,
     "end_time": "2022-04-25T11:25:52.860341",
     "exception": false,
     "start_time": "2022-04-25T11:25:52.772637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num\n",
       "0  00016_000         0      16            0\n",
       "1  00016_001         0      16            1\n",
       "2  00016_002         0      16            2\n",
       "3  00016_003         0      16            3\n",
       "4  00016_004         0      16            4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEBUG:\n",
    "    test = pd.read_csv('../input/nbme-score-clinical-patient-notes/train.csv')[:2000]\n",
    "else:\n",
    "    test = pd.read_csv('../input/nbme-score-clinical-patient-notes/test.csv')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c34a94ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:52.978584Z",
     "iopub.status.busy": "2022-04-25T11:25:52.977798Z",
     "iopub.status.idle": "2022-04-25T11:25:52.994121Z",
     "shell.execute_reply": "2022-04-25T11:25:52.993517Z"
    },
    "papermill": {
     "duration": 0.077536,
     "end_time": "2022-04-25T11:25:52.994277",
     "exception": false,
     "start_time": "2022-04-25T11:25:52.916741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "features = pd.read_csv('../input/nbme-score-clinical-patient-notes/features.csv')\n",
    "feature_year = features.loc[[ft.endswith('year') for ft in features.feature_text]]\n",
    "feature_year = feature_year.feature_num.values\n",
    "feature_female = features.loc[[ft == 'Female' for ft in features.feature_text]]\n",
    "feature_female = feature_female.feature_num.values\n",
    "feature_male = features.loc[[ft == 'Male' for ft in features.feature_text]]\n",
    "feature_male = feature_male.feature_num.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af1e12ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:53.108757Z",
     "iopub.status.busy": "2022-04-25T11:25:53.108043Z",
     "iopub.status.idle": "2022-04-25T11:25:53.819498Z",
     "shell.execute_reply": "2022-04-25T11:25:53.820411Z"
    },
    "papermill": {
     "duration": 0.771673,
     "end_time": "2022-04-25T11:25:53.820698",
     "exception": false,
     "start_time": "2022-04-25T11:25:53.049025",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "notes = pd.read_csv('../input/nbme-score-clinical-patient-notes/patient_notes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f78006c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:54.017630Z",
     "iopub.status.busy": "2022-04-25T11:25:54.016591Z",
     "iopub.status.idle": "2022-04-25T11:25:54.026753Z",
     "shell.execute_reply": "2022-04-25T11:25:54.028050Z"
    },
    "papermill": {
     "duration": 0.112109,
     "end_time": "2022-04-25T11:25:54.028278",
     "exception": false,
     "start_time": "2022-04-25T11:25:53.916169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_notes = notes[notes.pn_num.isin(set(test.pn_num.unique()))].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51959d9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:54.203302Z",
     "iopub.status.busy": "2022-04-25T11:25:54.201753Z",
     "iopub.status.idle": "2022-04-25T11:25:54.205515Z",
     "shell.execute_reply": "2022-04-25T11:25:54.204976Z"
    },
    "papermill": {
     "duration": 0.082348,
     "end_time": "2022-04-25T11:25:54.205649",
     "exception": false,
     "start_time": "2022-04-25T11:25:54.123301",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_notes['text_length'] = [len(pn_history) for pn_history in test_notes.pn_history]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5965ab7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:54.335679Z",
     "iopub.status.busy": "2022-04-25T11:25:54.333491Z",
     "iopub.status.idle": "2022-04-25T11:25:54.336455Z",
     "shell.execute_reply": "2022-04-25T11:25:54.337007Z"
    },
    "papermill": {
     "duration": 0.077038,
     "end_time": "2022-04-25T11:25:54.337170",
     "exception": false,
     "start_time": "2022-04-25T11:25:54.260132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test.merge(test_notes[['pn_num', 'text_length']], how='left', on='pn_num')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e8a50b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:54.455948Z",
     "iopub.status.busy": "2022-04-25T11:25:54.453792Z",
     "iopub.status.idle": "2022-04-25T11:25:54.456650Z",
     "shell.execute_reply": "2022-04-25T11:25:54.457229Z"
    },
    "papermill": {
     "duration": 0.065444,
     "end_time": "2022-04-25T11:25:54.457380",
     "exception": false,
     "start_time": "2022-04-25T11:25:54.391936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = test.sort_values(by='text_length').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fc6c1d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:54.577135Z",
     "iopub.status.busy": "2022-04-25T11:25:54.575733Z",
     "iopub.status.idle": "2022-04-25T11:25:54.580754Z",
     "shell.execute_reply": "2022-04-25T11:25:54.581278Z"
    },
    "papermill": {
     "duration": 0.070289,
     "end_time": "2022-04-25T11:25:54.581519",
     "exception": false,
     "start_time": "2022-04-25T11:25:54.511230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num  text_length\n",
       "0  00016_000         0      16            0          938\n",
       "1  00016_001         0      16            1          938\n",
       "2  00016_002         0      16            2          938\n",
       "3  00016_003         0      16            3          938\n",
       "4  00016_004         0      16            4          938"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6956413c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:54.702921Z",
     "iopub.status.busy": "2022-04-25T11:25:54.702227Z",
     "iopub.status.idle": "2022-04-25T11:25:54.707021Z",
     "shell.execute_reply": "2022-04-25T11:25:54.706423Z"
    },
    "papermill": {
     "duration": 0.069923,
     "end_time": "2022-04-25T11:25:54.707159",
     "exception": false,
     "start_time": "2022-04-25T11:25:54.637236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_lf(text, tokenizer):\n",
    "    res_l = [ tokenizer(t, \n",
    "                        return_offsets_mapping=True, \n",
    "                        return_attention_mask=False,\n",
    "                        return_token_type_ids=False,\n",
    "                        add_special_tokens=False, \n",
    "                        max_length=CONFIG['max_length'], \n",
    "                        truncation=True,\n",
    "                        padding=False,\n",
    "                       ) for t in text.split('\\n') ]\n",
    "    r = res_l[0]\n",
    "    res = {'input_ids':r['input_ids'], 'offset_mapping':r['offset_mapping']}\n",
    "    cum_len = res['offset_mapping'][-1][1]\n",
    "    for r in res_l[1:]:\n",
    "        res['input_ids'].append(tokenizer.lf_token_id)\n",
    "        res['offset_mapping'].append((cum_len, cum_len+1))\n",
    "        cum_len = cum_len + 1\n",
    "        res['input_ids'].extend(r['input_ids'])\n",
    "        res['offset_mapping'].extend([(start+cum_len, end+cum_len) for (start, end) in r['offset_mapping']])\n",
    "        cum_len = res['offset_mapping'][-1][1]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c97a6dfa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:54.828066Z",
     "iopub.status.busy": "2022-04-25T11:25:54.822241Z",
     "iopub.status.idle": "2022-04-25T11:25:54.853633Z",
     "shell.execute_reply": "2022-04-25T11:25:54.854254Z"
    },
    "papermill": {
     "duration": 0.09134,
     "end_time": "2022-04-25T11:25:54.854430",
     "exception": false,
     "start_time": "2022-04-25T11:25:54.763090",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NBMEDataset(Dataset):\n",
    "    def __init__(self, annotations, CONFIG, features, notes,):\n",
    "        super(NBMEDataset, self).__init__()\n",
    "        self.id = annotations.id.values\n",
    "        self.pn_num = annotations.pn_num.values\n",
    "        self.feature_num = annotations.feature_num.values\n",
    "        try:\n",
    "            self.location = annotations.location.values\n",
    "        except:\n",
    "            self.location = None\n",
    "        self.feature_token = self.tokenize(features, 'feature_num', 'feature_text', CONFIG)\n",
    "        \n",
    "        self.pn_history_token = self.tokenize(notes[notes.pn_num.isin(set(annotations.pn_num.unique()))], \n",
    "                                              'pn_num', 'pn_history', CONFIG)\n",
    "        self.max_length = CONFIG['max_length']\n",
    "        tokenizer = CONFIG['tokenizer']\n",
    "        self.special_tokens = {\n",
    "            \"sep\": tokenizer.sep_token_id,\n",
    "            \"cls\": tokenizer.cls_token_id,\n",
    "            \"pad\": tokenizer.pad_token_id,            \n",
    "        }\n",
    "        self.config = CONFIG\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.pn_num)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        pn_num = self.pn_num[idx]\n",
    "        pn_history_token = self.pn_history_token[pn_num]\n",
    "        feature_num = self.feature_num[idx]\n",
    "        feature_token = self.feature_token[feature_num]\n",
    "        location = None\n",
    "        if self.location is not None:\n",
    "            location = self.location[idx]\n",
    "        data = self.get_data(pn_history_token, feature_token, location, feature_num)\n",
    "        data.update({\n",
    "            'id':self.id[idx],\n",
    "        })\n",
    "        return data\n",
    "    \n",
    "    def get_data(self, pn_history_token, feature_token, location, feature_num, ):\n",
    "        max_length = self.max_length\n",
    "        text = pn_history_token['text']\n",
    "        pn_history_token = pn_history_token['tokens']\n",
    "        feature_token = feature_token['tokens']\n",
    "        \n",
    "        sep = self.special_tokens[\"sep\"]\n",
    "        cls = self.special_tokens[\"cls\"]\n",
    "        pad = self.special_tokens[\"pad\"]\n",
    "        q_input_ids = [cls] + feature_token['input_ids'] + [sep]\n",
    "        if \"roberta\" in self.config['model_name']:\n",
    "            q_input_ids = q_input_ids + [sep]       \n",
    "        input_ids = q_input_ids + pn_history_token['input_ids']\n",
    "        input_ids = input_ids[: max_length - 1] + [self.special_tokens[\"sep\"]]\n",
    "        len_token = len(input_ids)\n",
    "        \n",
    "        offset_mapping = [(0,0)] * len(q_input_ids) + pn_history_token['offset_mapping']\n",
    "        offset_mapping = offset_mapping[: max_length - 1] + [(0,0)]\n",
    "        max_token = len(text)\n",
    "        assert(len_token == len(offset_mapping))\n",
    "        \n",
    "        len_padding = max_length - len_token\n",
    "        if len_padding > 0:\n",
    "            input_ids = input_ids + [self.special_tokens[\"pad\"]] * len_padding\n",
    "            \n",
    "        attention_mask = np.zeros(max_length, dtype='int')\n",
    "        attention_mask[:len_token] = 1\n",
    "        \n",
    "        if \"roberta\" in self.config['model_name']:\n",
    "            token_type_ids = [0]\n",
    "        else:\n",
    "            token_type_ids = np.ones(max_length)\n",
    "            token_type_ids[:len(q_input_ids)] = 0\n",
    "            \n",
    "        out_dict = {\n",
    "            'input_ids' : torch.tensor(input_ids, dtype=torch.long),\n",
    "            'token_type_ids' :  torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'offset_mapping' : offset_mapping,\n",
    "            'attention_mask' : torch.tensor(attention_mask, dtype=torch.long),\n",
    "            'len_token' : len_token,\n",
    "            'max_token' : max_token,\n",
    "            'text' : text,\n",
    "            'feature_num' : feature_num,\n",
    "        }  \n",
    "        \n",
    "        if self.location is not None:\n",
    "            len_text = len(text)\n",
    "            char_type = np.zeros((len_text, ))\n",
    "            char_start = np.zeros((len_text, ))\n",
    "            char_end = np.zeros((len_text, ))\n",
    "            annots = eval(location)\n",
    "            annots = [a for ans in annots for a in ans.split(';')]\n",
    "            for annot in annots:\n",
    "                annot = annot.split()            \n",
    "                start = int(annot[0])\n",
    "                end = int(annot[1])\n",
    "                #print(feature, row['feature_num'], start, end)\n",
    "                char_type[start:end] = 1\n",
    "                char_start[start] = 1\n",
    "                char_end[end - 1] = 1\n",
    "            token_type = np.zeros((max_length, ))\n",
    "            token_start = np.zeros((max_length, ))\n",
    "            token_end = np.zeros((max_length, ))\n",
    "            for i, (start, end) in enumerate(offset_mapping):\n",
    "                if start == end:\n",
    "                    continue\n",
    "                token_type[i] = char_type[start:end].max(0)\n",
    "                token_start[i] = char_start[start:end].max(0)\n",
    "                token_end[i] = char_end[start:end].max(0)\n",
    "            out_dict.update({\n",
    "                'token_type':torch.tensor(token_type, dtype=torch.float32).unsqueeze(-1),\n",
    "                'token_start':torch.tensor(token_start, dtype=torch.float32).unsqueeze(-1),\n",
    "                'token_end':torch.tensor(token_end, dtype=torch.float32).unsqueeze(-1),\n",
    "            })\n",
    "        \n",
    "        return out_dict\n",
    "    \n",
    "    def tokenize(self, data, key, text, CONFIG):\n",
    "        tokenizer = CONFIG['tokenizer']\n",
    "        res = {k:{'tokens':tokenize_lf(text, tokenizer), 'text':text,}\n",
    "               for k,text in zip(data[key], data[text])}\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7c15fd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:54.976249Z",
     "iopub.status.busy": "2022-04-25T11:25:54.973829Z",
     "iopub.status.idle": "2022-04-25T11:25:54.977080Z",
     "shell.execute_reply": "2022-04-25T11:25:54.977647Z"
    },
    "papermill": {
     "duration": 0.06807,
     "end_time": "2022-04-25T11:25:54.977815",
     "exception": false,
     "start_time": "2022-04-25T11:25:54.909745",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "not_collate_keys = ['id', 'offset_mapping', 'len_token', 'max_token', 'text', 'feature_num']\n",
    "collate_keys = ['input_ids', 'token_type_ids', 'attention_mask', \n",
    "                #'token_type', 'token_start', 'token_end',\n",
    "               ]\n",
    "\n",
    "def feedback_collate(batch):\n",
    "    batch_dict = {}\n",
    "    len_token_max = np.max([sample['len_token'] for sample in batch])\n",
    "    for key in collate_keys:\n",
    "        try:\n",
    "            batch_dict[key] = torch.stack([b[key][:len_token_max] for b in batch])\n",
    "        except:\n",
    "            print('key not found:', key)\n",
    "    for key in not_collate_keys:\n",
    "        if key == 'offset_mapping':\n",
    "            batch_dict[key] = [b[key][:len_token_max] for b in batch]\n",
    "        else:\n",
    "            batch_dict[key] = [b[key] for b in batch]\n",
    "    return batch_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8edff9fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:55.097386Z",
     "iopub.status.busy": "2022-04-25T11:25:55.096265Z",
     "iopub.status.idle": "2022-04-25T11:25:55.098429Z",
     "shell.execute_reply": "2022-04-25T11:25:55.098990Z"
    },
    "papermill": {
     "duration": 0.06542,
     "end_time": "2022-04-25T11:25:55.099171",
     "exception": false,
     "start_time": "2022-04-25T11:25:55.033751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_loader(data, shuffle, CONFIG, features=features, notes=notes,):\n",
    "    if shuffle:\n",
    "        batch_size = CONFIG['train_batch_size']\n",
    "    else:\n",
    "        batch_size = CONFIG['valid_batch_size']\n",
    "    dataset = NBMEDataset(data, CONFIG, features, notes,)\n",
    "    data_loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=CONFIG['workers'],\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=True,\n",
    "        #worker_init_fn=worker_init_fn,\n",
    "        collate_fn=feedback_collate,\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cff174e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:55.214602Z",
     "iopub.status.busy": "2022-04-25T11:25:55.213915Z",
     "iopub.status.idle": "2022-04-25T11:25:55.217621Z",
     "shell.execute_reply": "2022-04-25T11:25:55.218327Z"
    },
    "papermill": {
     "duration": 0.064266,
     "end_time": "2022-04-25T11:25:55.218518",
     "exception": false,
     "start_time": "2022-04-25T11:25:55.154252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def criterion(pred, target):\n",
    "    return nn.BCEWithLogitsLoss(reduction='none')(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4fc19567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:55.343277Z",
     "iopub.status.busy": "2022-04-25T11:25:55.342203Z",
     "iopub.status.idle": "2022-04-25T11:25:55.344430Z",
     "shell.execute_reply": "2022-04-25T11:25:55.345021Z"
    },
    "papermill": {
     "duration": 0.071136,
     "end_time": "2022-04-25T11:25:55.345178",
     "exception": false,
     "start_time": "2022-04-25T11:25:55.274042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def glorot_uniform(parameter):\n",
    "    nn.init.xavier_uniform_(parameter.data, gain=1.0)\n",
    "    \n",
    "class NBMEHead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, loss, criterion):\n",
    "        super(NBMEHead, self).__init__()\n",
    "        self.loss = loss\n",
    "        self.criterion = criterion\n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "        self.dropout4 = nn.Dropout(0.4)\n",
    "        self.dropout5 = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Linear(input_dim, output_dim)\n",
    "        glorot_uniform(self.classifier.weight)\n",
    "        \n",
    "    def forward(self, x, attention_mask, target=None):\n",
    "        # x is B x S x C\n",
    "        logits1 = self.classifier(self.dropout1(x))\n",
    "        logits2 = self.classifier(self.dropout2(x))\n",
    "        logits3 = self.classifier(self.dropout3(x))\n",
    "        logits4 = self.classifier(self.dropout4(x))\n",
    "        logits5 = self.classifier(self.dropout5(x))\n",
    "                              \n",
    "        logits = ((logits1 + logits2 + logits3 + logits4 + logits5) / 5)\n",
    "        logits = logits * attention_mask\n",
    "        \n",
    "        if self.loss:\n",
    "            loss1 = self.criterion(logits1, target)\n",
    "            loss2 = self.criterion(logits2, target)\n",
    "            loss3 = self.criterion(logits3, target)\n",
    "            loss4 = self.criterion(logits4, target)\n",
    "            loss5 = self.criterion(logits5, target)\n",
    "            loss = (loss1 + loss2 + loss3  + loss4 + loss5) / 5\n",
    "            \n",
    "            #print(loss.shape, attention_mask.shape)\n",
    "            loss = loss * attention_mask\n",
    "            loss = loss.sum(1) / (1e-6 + attention_mask.sum(1))\n",
    "            loss = loss.mean()\n",
    "        else:\n",
    "            loss = 0\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b5423b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:55.465028Z",
     "iopub.status.busy": "2022-04-25T11:25:55.464136Z",
     "iopub.status.idle": "2022-04-25T11:25:55.467744Z",
     "shell.execute_reply": "2022-04-25T11:25:55.468384Z"
    },
    "papermill": {
     "duration": 0.066979,
     "end_time": "2022-04-25T11:25:55.468552",
     "exception": false,
     "start_time": "2022-04-25T11:25:55.401573",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['id', 'offset_mapping', 'len_token', 'max_token', 'text', 'feature_num'],\n",
       " ['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_keys = ['token_type_logits',]\n",
    "loss_keys = ['loss', 'tp_count', 'all_count', ]\n",
    "not_collate_keys, collate_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ed9a876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:55.587269Z",
     "iopub.status.busy": "2022-04-25T11:25:55.586203Z",
     "iopub.status.idle": "2022-04-25T11:25:55.588627Z",
     "shell.execute_reply": "2022-04-25T11:25:55.589235Z"
    },
    "papermill": {
     "duration": 0.065035,
     "end_time": "2022-04-25T11:25:55.589405",
     "exception": false,
     "start_time": "2022-04-25T11:25:55.524370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "keep_collate_keys = []\n",
    "test_collate_keys =  ['input_ids', 'attention_mask', 'token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1727ac8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:55.713376Z",
     "iopub.status.busy": "2022-04-25T11:25:55.706031Z",
     "iopub.status.idle": "2022-04-25T11:25:55.716314Z",
     "shell.execute_reply": "2022-04-25T11:25:55.715710Z"
    },
    "papermill": {
     "duration": 0.069933,
     "end_time": "2022-04-25T11:25:55.716468",
     "exception": false,
     "start_time": "2022-04-25T11:25:55.646535",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "spaces = ' \\n\\r'\n",
    "\n",
    "def post_process_spaces(pred, text):\n",
    "    text = text[:len(pred)]\n",
    "    pred = pred[:len(text)]\n",
    "    if text[0] in spaces:\n",
    "        pred[0] = 0\n",
    "    if text[-1] in spaces:\n",
    "        pred[-1] = 0\n",
    "\n",
    "    for i in range(1, len(text) - 1):\n",
    "        if text[i] in spaces:\n",
    "            if pred[i] and not pred[i - 1]:  # space before\n",
    "                pred[i] = 0\n",
    "\n",
    "            if pred[i] and not pred[i + 1]:  # space after\n",
    "                pred[i] = 0\n",
    "\n",
    "            if pred[i - 1] and pred[i + 1]:\n",
    "                pred[i] = 1\n",
    " \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "099ae742",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:55.837753Z",
     "iopub.status.busy": "2022-04-25T11:25:55.836668Z",
     "iopub.status.idle": "2022-04-25T11:25:55.839115Z",
     "shell.execute_reply": "2022-04-25T11:25:55.839726Z"
    },
    "papermill": {
     "duration": 0.066218,
     "end_time": "2022-04-25T11:25:55.839891",
     "exception": false,
     "start_time": "2022-04-25T11:25:55.773673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred_to_chars(token_type_logits, len_token, max_token, offset_mapping, text, feature_num):\n",
    "    token_type_logits = token_type_logits[:len_token]\n",
    "    offset_mapping = offset_mapping[:len_token]\n",
    "    char_preds = np.ones(len(text)) * -1e10\n",
    "    for i, (start,end) in enumerate(offset_mapping):\n",
    "        char_preds[start:end] = token_type_logits[i]\n",
    "    return (char_preds, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "de5eab52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:55.965698Z",
     "iopub.status.busy": "2022-04-25T11:25:55.957971Z",
     "iopub.status.idle": "2022-04-25T11:25:55.968718Z",
     "shell.execute_reply": "2022-04-25T11:25:55.968149Z"
    },
    "papermill": {
     "duration": 0.071849,
     "end_time": "2022-04-25T11:25:55.968873",
     "exception": false,
     "start_time": "2022-04-25T11:25:55.897024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred_to_chars(token_type_logits, len_token, max_token, offset_mapping, text, feature_num):\n",
    "    token_type_logits = token_type_logits[:len_token]\n",
    "    offset_mapping = offset_mapping[:len_token]\n",
    "    char_preds = np.ones(len(text)) * -1e10\n",
    "    for i, (start,end) in enumerate(offset_mapping):\n",
    "        if text[start:end] == 'of' and start > 0 and text[start-1:end] == 'yof':\n",
    "            if feature_num in feature_female:\n",
    "                char_preds[end-1:end] = 1\n",
    "            elif feature_num in feature_year:\n",
    "                char_preds[start:start+1] = token_type_logits[i-1]\n",
    "            else:\n",
    "                char_preds[start:end] = token_type_logits[i]\n",
    "        elif text[start:end] == 'om' and start > 0 and text[start-1:end] == 'yom':\n",
    "            if feature_num in feature_male:\n",
    "                char_preds[end-1:end] = 1\n",
    "            elif feature_num in feature_year:\n",
    "                char_preds[start:start+1] = token_type_logits[i-1]\n",
    "            else:\n",
    "                char_preds[start:end] = token_type_logits[i]\n",
    "        else:\n",
    "            char_preds[start:end] = token_type_logits[i]\n",
    "    return (char_preds, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e525f708",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:56.091366Z",
     "iopub.status.busy": "2022-04-25T11:25:56.090383Z",
     "iopub.status.idle": "2022-04-25T11:25:56.093473Z",
     "shell.execute_reply": "2022-04-25T11:25:56.094006Z"
    },
    "papermill": {
     "duration": 0.067555,
     "end_time": "2022-04-25T11:25:56.094166",
     "exception": false,
     "start_time": "2022-04-25T11:25:56.026611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def char_preds_to_string(char_preds, text, pred_thr):\n",
    "    char_preds = (char_preds > pred_thr) * 1\n",
    "    post_process_spaces(char_preds, text)\n",
    "    indices = np.where(char_preds == 1)[0]\n",
    "    indices_grouped = [\n",
    "        list(g) for _, g in itertools.groupby(\n",
    "            indices, key=lambda n, c=itertools.count(): n - next(c)\n",
    "        )\n",
    "    ]\n",
    "    spans = [f\"{min(r)} {max(r) + 1}\" for r in indices_grouped]\n",
    "    spans = ';'.join(spans)\n",
    "    return spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4653cabe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:56.221640Z",
     "iopub.status.busy": "2022-04-25T11:25:56.219111Z",
     "iopub.status.idle": "2022-04-25T11:25:56.225290Z",
     "shell.execute_reply": "2022-04-25T11:25:56.224681Z"
    },
    "papermill": {
     "duration": 0.073883,
     "end_time": "2022-04-25T11:25:56.225426",
     "exception": false,
     "start_time": "2022-04-25T11:25:56.151543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_epoch(loader, models, device):\n",
    "\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    char_preds = []\n",
    "    with torch.no_grad():\n",
    "        if CONFIG['verbose']:\n",
    "            bar = tqdm(range(len(loader)))\n",
    "        else:\n",
    "            bar = range(len(loader))\n",
    "        load_iter = iter(loader)\n",
    "\n",
    "        for i in bar:\n",
    "            batch = load_iter.next()\n",
    "            input_dict = {k:batch[k].to(device, non_blocking=True) for k in collate_keys}\n",
    "\n",
    "            batch_out_dict = {}\n",
    "            for key in prediction_keys :\n",
    "                batch_out_dict[key] = 0             \n",
    "            for model in models:\n",
    "                out_dict = model(input_dict)\n",
    "                for key in prediction_keys :\n",
    "                    batch_out_dict[key] = batch_out_dict[key] + out_dict[key].detach() / len(models)             \n",
    "            token_type_logits = (batch_out_dict['token_type_logits']).detach()\n",
    "            token_type_logits = token_type_logits.cpu().numpy()\n",
    "\n",
    "            char_preds.extend([\n",
    "                pred_to_chars(*p) for p in zip(token_type_logits, \n",
    "                                                batch['len_token'], \n",
    "                                                batch['max_token'], \n",
    "                                                batch['offset_mapping'],\n",
    "                                                batch['text'],\n",
    "                                                batch['feature_num'],\n",
    "                                               )\n",
    "            ])\n",
    "\n",
    "    return char_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "96576c99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:56.353243Z",
     "iopub.status.busy": "2022-04-25T11:25:56.352403Z",
     "iopub.status.idle": "2022-04-25T11:25:56.356046Z",
     "shell.execute_reply": "2022-04-25T11:25:56.355441Z"
    },
    "papermill": {
     "duration": 0.073191,
     "end_time": "2022-04-25T11:25:56.356186",
     "exception": false,
     "start_time": "2022-04-25T11:25:56.282995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_model_checkpoint(dirname, fname, fold):\n",
    "    model = NBMEModel(CONFIG['model_name'], loss=False, pretrained=False).to(device)\n",
    "    checkpoint = torch.load('../input/%s/%s_%d.pt' % (dirname, fname, fold))\n",
    "    print(dirname, fname, fold, checkpoint['epoch'])\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def get_char_preds(test, CONFIG, notes):\n",
    "    device = torch.device('cuda')\n",
    "    \n",
    "    models = [load_model_checkpoint(CONFIG['dirname'],CONFIG['fname'], fold) for fold in CONFIG[\"folds\"]]\n",
    "\n",
    "    test_data_loader = get_data_loader(test, shuffle=False, CONFIG=CONFIG, notes=notes)\n",
    "    char_preds = test_epoch(test_data_loader, models, device)   \n",
    "    del test_data_loader, models\n",
    "    gc.collect()\n",
    "    return char_preds\n",
    "\n",
    "def get_preds(char_preds, texts, pred_thr):\n",
    "    preds = [char_preds_to_string(p, text, pred_thr) for p,text in zip(char_preds, texts)]\n",
    "    df = pd.DataFrame({'id':test['id'], 'location':preds,})\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85fad591",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:56.478548Z",
     "iopub.status.busy": "2022-04-25T11:25:56.477536Z",
     "iopub.status.idle": "2022-04-25T11:25:56.480472Z",
     "shell.execute_reply": "2022-04-25T11:25:56.481018Z"
    },
    "papermill": {
     "duration": 0.066783,
     "end_time": "2022-04-25T11:25:56.481171",
     "exception": false,
     "start_time": "2022-04-25T11:25:56.414388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "char_preds_all = []\n",
    "model_coeff_all = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a890b9",
   "metadata": {
    "papermill": {
     "duration": 0.057568,
     "end_time": "2022-04-25T11:25:56.596627",
     "exception": false,
     "start_time": "2022-04-25T11:25:56.539059",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NBME 254"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3f2c62b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:56.734361Z",
     "iopub.status.busy": "2022-04-25T11:25:56.733500Z",
     "iopub.status.idle": "2022-04-25T11:25:56.738052Z",
     "shell.execute_reply": "2022-04-25T11:25:56.737459Z"
    },
    "papermill": {
     "duration": 0.082972,
     "end_time": "2022-04-25T11:25:56.738186",
     "exception": false,
     "start_time": "2022-04-25T11:25:56.655214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NBMEModel(nn.Module):\n",
    "    def __init__(self, model_name, loss=False, pretrained=True):\n",
    "        super(NBMEModel, self).__init__()\n",
    "        config = CONFIG['config']\n",
    "        self.config = config\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.backbone = AutoModel.from_config(config)\n",
    "        self.backbone.resize_token_embeddings(len(CONFIG['tokenizer']))\n",
    "        self.loss = loss\n",
    "        #self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout0 = nn.Dropout(p=CONFIG['dropout'])\n",
    "        self.rnn0 = nn.LSTM(CONFIG['config'].hidden_size,\n",
    "                           CONFIG['config'].hidden_size//2,\n",
    "                           num_layers=1,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=True,\n",
    "                          )\n",
    "        self.token_type_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.token_start_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.token_end_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.model_name = model_name\n",
    "        weight_data = torch.linspace(-5, 5, 1+self.config.num_hidden_layers)\n",
    "        weight_data = weight_data.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.weights = nn.Parameter(weight_data, requires_grad=True)\n",
    "       \n",
    "    def forward(self, input_dict):     \n",
    "        input_ids = input_dict['input_ids']\n",
    "        attention_mask = input_dict['attention_mask']\n",
    "        if 'roberta' in self.model_name:\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                output_hidden_states=True)\n",
    "        else:\n",
    "            token_type_ids = input_dict['token_type_ids']\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        x = torch.stack(out.hidden_states)\n",
    "        w = F.softmax(self.weights, 0)\n",
    "        w = self.dropout0(w)\n",
    "        x = (w * x).sum(0)\n",
    "        x = self.rnn0(x)[0]\n",
    "        #x = out.hidden_states[-1]\n",
    "        if self.loss:\n",
    "            token_type = input_dict['token_type']\n",
    "            token_start = input_dict['token_start']\n",
    "            token_end = input_dict['token_end']\n",
    "            target_mask = (attention_mask * token_type_ids).unsqueeze(-1)\n",
    "        else:\n",
    "            token_type = None\n",
    "            token_start = None\n",
    "            token_end = None\n",
    "            target_mask = 1\n",
    "        token_type_logits, loss_type = self.token_type_head(x, target_mask, token_type) \n",
    "        token_start_logits, loss_start = self.token_start_head(x, target_mask, token_start) \n",
    "        token_end_logits, loss_end = self.token_end_head(x, target_mask, token_end)\n",
    "        out_dict = {\n",
    "            'token_type_logits' : token_type_logits,\n",
    "            'token_start_logits' : token_start_logits,\n",
    "            'token_end_logits' : token_end_logits,\n",
    "        }\n",
    "        \n",
    "        if self.loss:\n",
    "            loss = loss_type + loss_start + loss_end\n",
    "            token_type_pred = ((token_type_logits >= 0) * target_mask).detach()\n",
    "            tp_count = (token_type_pred * token_type).sum().detach().item()\n",
    "            all_count = (token_type_pred.sum() + token_type.sum()).detach().item()\n",
    "            #print(tp_count, token_type_pred.sum().item(), token_type.sum().item())\n",
    "            out_dict.update({\n",
    "                'loss' : loss,\n",
    "                'tp_count' : tp_count,\n",
    "                'all_count' : all_count,\n",
    "            })\n",
    "            \n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abcf5df9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:56.861337Z",
     "iopub.status.busy": "2022-04-25T11:25:56.860460Z",
     "iopub.status.idle": "2022-04-25T11:25:56.864862Z",
     "shell.execute_reply": "2022-04-25T11:25:56.864239Z"
    },
    "papermill": {
     "duration": 0.068613,
     "end_time": "2022-04-25T11:25:56.865012",
     "exception": false,
     "start_time": "2022-04-25T11:25:56.796399",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_abbrev(text):\n",
    "    text = text.replace('FHx', 'FH ')\n",
    "    text = text.replace('FHX', 'FH ')\n",
    "    text = text.replace('PMHx', 'PMH ')\n",
    "    text = text.replace('PMHX', 'PMH ')\n",
    "    text = text.replace('SHx', 'SH ')\n",
    "    text = text.replace('SHX', 'SH ')\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a124f185",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:57.070285Z",
     "iopub.status.busy": "2022-04-25T11:25:57.068398Z",
     "iopub.status.idle": "2022-04-25T11:25:57.073764Z",
     "shell.execute_reply": "2022-04-25T11:25:57.073141Z"
    },
    "papermill": {
     "duration": 0.149771,
     "end_time": "2022-04-25T11:25:57.073957",
     "exception": false,
     "start_time": "2022-04-25T11:25:56.924186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"fname\" : 'nbme_254',\n",
    "          'dirname' : 'nbme-254',\n",
    "          \"seed\": 2021,\n",
    "          \"epochs\": 10,\n",
    "          \"model_name\": \"../input/deberta-v3-large-squadv2\",\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 32,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"folds\": [0, 1, 2, 3],\n",
    "          \"n_accumulate\": 4,\n",
    "          \"num_classes\": 2,\n",
    "          \"margin\": 0.5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          'opt_wd_non_norm_bias' : 0.01,\n",
    "          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n",
    "          'opt_beta1' : 0.9,\n",
    "          'opt_beta2' : 0.99,\n",
    "          'opt_eps' : 1e-5, # same as Adam in Fastai\n",
    "          'verbose': DEBUG,\n",
    "          'valid_check' : 8,\n",
    "          'workers':8,\n",
    "          'dropout' : 0.2,\n",
    "          'checkpointing':False,\n",
    "          }\n",
    "\n",
    "if CONFIG['fname'] in model_coeff:\n",
    "    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n",
    "    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
    "    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(CONFIG['model_name'])\n",
    "    CONFIG[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n",
    "    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    tokenizer.add_tokens([lf_token])\n",
    "    \n",
    "    test_clean_notes = test_notes.copy()\n",
    "    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n",
    "    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n",
    "    char_preds_all.append(char_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e537b013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:57.206184Z",
     "iopub.status.busy": "2022-04-25T11:25:57.205074Z",
     "iopub.status.idle": "2022-04-25T11:25:57.208679Z",
     "shell.execute_reply": "2022-04-25T11:25:57.208125Z"
    },
    "papermill": {
     "duration": 0.075517,
     "end_time": "2022-04-25T11:25:57.208854",
     "exception": false,
     "start_time": "2022-04-25T11:25:57.133337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"fname\" : 'nbme_254_full',\n",
    "          'dirname' : 'nbme-254-full',\n",
    "          \"seed\": 2021,\n",
    "          \"epochs\": 10,\n",
    "          \"model_name\": \"../input/deberta-v3-large-squadv2\",\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 32,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"folds\": [0, 1, 2, 3],\n",
    "          \"n_accumulate\": 4,\n",
    "          \"num_classes\": 2,\n",
    "          \"margin\": 0.5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          'opt_wd_non_norm_bias' : 0.01,\n",
    "          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n",
    "          'opt_beta1' : 0.9,\n",
    "          'opt_beta2' : 0.99,\n",
    "          'opt_eps' : 1e-5, # same as Adam in Fastai\n",
    "          'verbose': DEBUG,\n",
    "          'valid_check' : 8,\n",
    "          'workers':8,\n",
    "          'dropout' : 0.2,\n",
    "          'checkpointing':False,\n",
    "          }\n",
    "\n",
    "if CONFIG['fname'] in model_coeff:\n",
    "    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n",
    "    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
    "    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(CONFIG['model_name'])\n",
    "    CONFIG[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n",
    "    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    tokenizer.add_tokens([lf_token])\n",
    "    \n",
    "    test_clean_notes = test_notes.copy()\n",
    "    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n",
    "    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n",
    "    char_preds_all.append(char_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde691ee",
   "metadata": {
    "papermill": {
     "duration": 0.060014,
     "end_time": "2022-04-25T11:25:57.329079",
     "exception": false,
     "start_time": "2022-04-25T11:25:57.269065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NBME 182"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "634a0833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:57.471067Z",
     "iopub.status.busy": "2022-04-25T11:25:57.465739Z",
     "iopub.status.idle": "2022-04-25T11:25:57.477465Z",
     "shell.execute_reply": "2022-04-25T11:25:57.476680Z"
    },
    "papermill": {
     "duration": 0.08709,
     "end_time": "2022-04-25T11:25:57.477643",
     "exception": false,
     "start_time": "2022-04-25T11:25:57.390553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NBMEModel(nn.Module):\n",
    "    def __init__(self, model_name, loss=False, pretrained=True):\n",
    "        super(NBMEModel, self).__init__()\n",
    "        config = CONFIG['config']\n",
    "        self.config = config\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.backbone = AutoModel.from_config(config)\n",
    "        self.backbone.resize_token_embeddings(len(CONFIG['tokenizer']))\n",
    "        self.loss = loss\n",
    "        #self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout0 = nn.Dropout(p=CONFIG['dropout'])\n",
    "        self.rnn0 = nn.GRU(CONFIG['config'].hidden_size,\n",
    "                           CONFIG['config'].hidden_size//2,\n",
    "                           num_layers=1,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=True,\n",
    "                          )\n",
    "        self.token_type_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.token_start_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.token_end_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.model_name = model_name\n",
    "        weight_data = torch.linspace(-5, 5, 1+self.config.num_hidden_layers)\n",
    "        weight_data = weight_data.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.weights = nn.Parameter(weight_data, requires_grad=True)\n",
    "       \n",
    "    def forward(self, input_dict):     \n",
    "        input_ids = input_dict['input_ids']\n",
    "        attention_mask = input_dict['attention_mask']\n",
    "        if 'roberta' in self.model_name:\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                output_hidden_states=True)\n",
    "        else:\n",
    "            token_type_ids = input_dict['token_type_ids']\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        x = torch.stack(out.hidden_states)\n",
    "        w = F.softmax(self.weights, 0)\n",
    "        w = self.dropout0(w)\n",
    "        x = (w * x).sum(0)\n",
    "        x = self.rnn0(x)[0]\n",
    "        #x = out.hidden_states[-1]\n",
    "        if self.loss:\n",
    "            token_type = input_dict['token_type']\n",
    "            token_start = input_dict['token_start']\n",
    "            token_end = input_dict['token_end']\n",
    "            target_mask = (attention_mask * token_type_ids).unsqueeze(-1)\n",
    "        else:\n",
    "            token_type = None\n",
    "            token_start = None\n",
    "            token_end = None\n",
    "            target_mask = 1\n",
    "        token_type_logits, loss_type = self.token_type_head(x, target_mask, token_type) \n",
    "        token_start_logits, loss_start = self.token_start_head(x, target_mask, token_start) \n",
    "        token_end_logits, loss_end = self.token_end_head(x, target_mask, token_end)\n",
    "        out_dict = {\n",
    "            'token_type_logits' : token_type_logits,\n",
    "            'token_start_logits' : token_start_logits,\n",
    "            'token_end_logits' : token_end_logits,\n",
    "        }\n",
    "        \n",
    "        if self.loss:\n",
    "            loss = loss_type + loss_start + loss_end\n",
    "            token_type_pred = ((token_type_logits >= 0) * target_mask).detach()\n",
    "            tp_count = (token_type_pred * token_type).sum().detach().item()\n",
    "            all_count = (token_type_pred.sum() + token_type.sum()).detach().item()\n",
    "            #print(tp_count, token_type_pred.sum().item(), token_type.sum().item())\n",
    "            out_dict.update({\n",
    "                'loss' : loss,\n",
    "                'tp_count' : tp_count,\n",
    "                'all_count' : all_count,\n",
    "            })\n",
    "            \n",
    "        return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c15c1c50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:57.605313Z",
     "iopub.status.busy": "2022-04-25T11:25:57.603992Z",
     "iopub.status.idle": "2022-04-25T11:25:57.607053Z",
     "shell.execute_reply": "2022-04-25T11:25:57.607639Z"
    },
    "papermill": {
     "duration": 0.069916,
     "end_time": "2022-04-25T11:25:57.607795",
     "exception": false,
     "start_time": "2022-04-25T11:25:57.537879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_abbrev(text):\n",
    "    text = text.replace('FHx', 'FH ')\n",
    "    text = text.replace('FHX', 'FH ')\n",
    "    text = text.replace('PMHx', 'PMH ')\n",
    "    text = text.replace('PMHX', 'PMH ')\n",
    "    text = text.replace('SHx', 'SH ')\n",
    "    text = text.replace('SHX', 'SH ')\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cc2338d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:57.748095Z",
     "iopub.status.busy": "2022-04-25T11:25:57.747138Z",
     "iopub.status.idle": "2022-04-25T11:25:57.751236Z",
     "shell.execute_reply": "2022-04-25T11:25:57.750291Z"
    },
    "papermill": {
     "duration": 0.08376,
     "end_time": "2022-04-25T11:25:57.751420",
     "exception": false,
     "start_time": "2022-04-25T11:25:57.667660",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"fname\" : 'nbme_182',\n",
    "          'dirname' : 'nbme-182',\n",
    "          \"seed\": 2021,\n",
    "          \"epochs\": 10,\n",
    "          \"model_name\": \"../input/deberta-v3-large-squadv2\",\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 32,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"folds\": [0, 1, 2, 3],\n",
    "          \"n_accumulate\": 4,\n",
    "          \"num_classes\": 2,\n",
    "          \"margin\": 0.5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          'opt_wd_non_norm_bias' : 0.01,\n",
    "          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n",
    "          'opt_beta1' : 0.9,\n",
    "          'opt_beta2' : 0.99,\n",
    "          'opt_eps' : 1e-5, # same as Adam in Fastai\n",
    "          'verbose': DEBUG,\n",
    "          'valid_check' : 8,\n",
    "          'workers':8,\n",
    "          'dropout' : 0.2,\n",
    "          'checkpointing':False,\n",
    "          }\n",
    "\n",
    "if CONFIG['fname'] in model_coeff:\n",
    "    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n",
    "    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
    "    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(CONFIG['model_name'])\n",
    "    CONFIG[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n",
    "    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    tokenizer.add_tokens([lf_token])\n",
    "    \n",
    "    test_clean_notes = test_notes.copy()\n",
    "    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n",
    "    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n",
    "    char_preds_all.append(char_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "af758f63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:57.888058Z",
     "iopub.status.busy": "2022-04-25T11:25:57.886829Z",
     "iopub.status.idle": "2022-04-25T11:25:57.889389Z",
     "shell.execute_reply": "2022-04-25T11:25:57.889960Z"
    },
    "papermill": {
     "duration": 0.078409,
     "end_time": "2022-04-25T11:25:57.890120",
     "exception": false,
     "start_time": "2022-04-25T11:25:57.811711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"fname\" : 'nbme_182_full',\n",
    "          'dirname' : 'nbme-182-full',\n",
    "          \"seed\": 2021,\n",
    "          \"epochs\": 10,\n",
    "          \"model_name\": \"../input/deberta-v3-large-squadv2\",\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 32,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"folds\": [0, 1, 2, 3],\n",
    "          \"n_accumulate\": 4,\n",
    "          \"num_classes\": 2,\n",
    "          \"margin\": 0.5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          'opt_wd_non_norm_bias' : 0.01,\n",
    "          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n",
    "          'opt_beta1' : 0.9,\n",
    "          'opt_beta2' : 0.99,\n",
    "          'opt_eps' : 1e-5, # same as Adam in Fastai\n",
    "          'verbose': DEBUG,\n",
    "          'valid_check' : 8,\n",
    "          'workers':8,\n",
    "          'dropout' : 0.2,\n",
    "          'checkpointing':False,\n",
    "          }\n",
    "\n",
    "if CONFIG['fname'] in model_coeff:\n",
    "    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n",
    "    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
    "    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(CONFIG['model_name'])\n",
    "    CONFIG[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n",
    "    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    tokenizer.add_tokens([lf_token])\n",
    "    \n",
    "    test_clean_notes = test_notes.copy()\n",
    "    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n",
    "    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n",
    "    char_preds_all.append(char_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceac3f7",
   "metadata": {
    "papermill": {
     "duration": 0.060627,
     "end_time": "2022-04-25T11:25:58.010698",
     "exception": false,
     "start_time": "2022-04-25T11:25:57.950071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NBME 315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6f3324b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:25:58.145536Z",
     "iopub.status.busy": "2022-04-25T11:25:58.144832Z",
     "iopub.status.idle": "2022-04-25T11:27:38.742373Z",
     "shell.execute_reply": "2022-04-25T11:27:38.742916Z"
    },
    "papermill": {
     "duration": 100.671755,
     "end_time": "2022-04-25T11:27:38.743143",
     "exception": false,
     "start_time": "2022-04-25T11:25:58.071388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nbme-315 nbme_315 0 3\n",
      "nbme-315 nbme_315 1 4\n",
      "nbme-315 nbme_315 2 7\n",
      "nbme-315 nbme_315 3 5\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\"fname\" : 'nbme_315',\n",
    "          'dirname' : 'nbme-315',\n",
    "          \"seed\": 2021,\n",
    "          \"epochs\": 10,\n",
    "          \"model_name\": \"../input/deberta-v3-large-squadv2\",\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 32,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"folds\": [0, 1, 2, 3],\n",
    "          \"n_accumulate\": 4,\n",
    "          \"num_classes\": 2,\n",
    "          \"margin\": 0.5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          'opt_wd_non_norm_bias' : 0.01,\n",
    "          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n",
    "          'opt_beta1' : 0.9,\n",
    "          'opt_beta2' : 0.99,\n",
    "          'opt_eps' : 1e-5, # same as Adam in Fastai\n",
    "          'verbose': DEBUG,\n",
    "          'valid_check' : 8,\n",
    "          'workers':8,\n",
    "          'dropout' : 0.2,\n",
    "          'checkpointing':False,\n",
    "          }\n",
    "\n",
    "if CONFIG['fname'] in model_coeff:\n",
    "    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n",
    "    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
    "    from transformers.models.deberta_v2.tokenization_deberta_v2_fast import DebertaV2TokenizerFast\n",
    "\n",
    "    tokenizer = DebertaV2TokenizerFast.from_pretrained(CONFIG['model_name'])\n",
    "    CONFIG[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    lf_token = AddedToken(\"\\n\", lstrip=True, rstrip=True)\n",
    "    tokenizer.add_tokens([lf_token])\n",
    "    tokenizer.lf_token_id = tokenizer.get_added_vocab()['\\n']\n",
    "    tokenizer.lf_token_id  \n",
    "    \n",
    "    test_clean_notes = test_notes.copy()\n",
    "    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n",
    "    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n",
    "    char_preds_all.append(char_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b760842b",
   "metadata": {
    "papermill": {
     "duration": 0.060568,
     "end_time": "2022-04-25T11:27:38.866571",
     "exception": false,
     "start_time": "2022-04-25T11:27:38.806003",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NBME 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d7c8b5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:39.006996Z",
     "iopub.status.busy": "2022-04-25T11:27:39.005819Z",
     "iopub.status.idle": "2022-04-25T11:27:39.017677Z",
     "shell.execute_reply": "2022-04-25T11:27:39.017136Z"
    },
    "papermill": {
     "duration": 0.088886,
     "end_time": "2022-04-25T11:27:39.017871",
     "exception": false,
     "start_time": "2022-04-25T11:27:38.928985",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NBMEModel(nn.Module):\n",
    "    def __init__(self, model_name, loss=False, pretrained=True):\n",
    "        super(NBMEModel, self).__init__()\n",
    "        config = CONFIG['config']\n",
    "        self.config = config\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.backbone = AutoModel.from_config(config)\n",
    "        self.backbone.resize_token_embeddings(len(CONFIG['tokenizer']))\n",
    "        self.loss = loss\n",
    "        #self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout0 = nn.Dropout(p=CONFIG['dropout'])\n",
    "        self.rnn0 = nn.LSTM(CONFIG['config'].hidden_size,\n",
    "                           CONFIG['config'].hidden_size//2,\n",
    "                           num_layers=1,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=True,\n",
    "                          )\n",
    "        self.token_type_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.token_start_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.token_end_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.model_name = model_name\n",
    "        weight_data = torch.linspace(-5, 5, 1+self.config.num_hidden_layers)\n",
    "        weight_data = weight_data.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.weights = nn.Parameter(weight_data, requires_grad=True)\n",
    "       \n",
    "    def forward(self, input_dict):     \n",
    "        input_ids = input_dict['input_ids']\n",
    "        attention_mask = input_dict['attention_mask']\n",
    "        if 'roberta' in self.model_name:\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                output_hidden_states=True)\n",
    "        else:\n",
    "            token_type_ids = input_dict['token_type_ids']\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        x = torch.stack(out.hidden_states)\n",
    "        w = F.softmax(self.weights, 0)\n",
    "        w = self.dropout0(w)\n",
    "        x = (w * x).sum(0)\n",
    "        x = self.rnn0(x)[0]\n",
    "        #x = out.hidden_states[-1]\n",
    "        if self.loss:\n",
    "            token_type = input_dict['token_type']\n",
    "            token_start = input_dict['token_start']\n",
    "            token_end = input_dict['token_end']\n",
    "            target_mask = (attention_mask * token_type_ids).unsqueeze(-1)\n",
    "        else:\n",
    "            token_type = None\n",
    "            token_start = None\n",
    "            token_end = None\n",
    "            target_mask = 1\n",
    "        token_type_logits, loss_type = self.token_type_head(x, target_mask, token_type) \n",
    "        token_start_logits, loss_start = self.token_start_head(x, target_mask, token_start) \n",
    "        token_end_logits, loss_end = self.token_end_head(x, target_mask, token_end)\n",
    "        out_dict = {\n",
    "            'token_type_logits' : token_type_logits,\n",
    "            'token_start_logits' : token_start_logits,\n",
    "            'token_end_logits' : token_end_logits,\n",
    "        }\n",
    "        \n",
    "        if self.loss:\n",
    "            loss = loss_type + loss_start + loss_end\n",
    "            token_type_pred = ((token_type_logits >= 0) * target_mask).detach()\n",
    "            tp_count = (token_type_pred * token_type).sum().detach().item()\n",
    "            all_count = (token_type_pred.sum() + token_type.sum()).detach().item()\n",
    "            #print(tp_count, token_type_pred.sum().item(), token_type.sum().item())\n",
    "            out_dict.update({\n",
    "                'loss' : loss,\n",
    "                'tp_count' : tp_count,\n",
    "                'all_count' : all_count,\n",
    "            })\n",
    "            \n",
    "        return out_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0901d2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:39.158155Z",
     "iopub.status.busy": "2022-04-25T11:27:39.157009Z",
     "iopub.status.idle": "2022-04-25T11:27:39.160438Z",
     "shell.execute_reply": "2022-04-25T11:27:39.159691Z"
    },
    "papermill": {
     "duration": 0.079603,
     "end_time": "2022-04-25T11:27:39.160610",
     "exception": false,
     "start_time": "2022-04-25T11:27:39.081007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"fname\" : 'nbme_256',\n",
    "          'dirname' : 'nbme-256',\n",
    "          \"seed\": 2021,\n",
    "          \"epochs\": 10,\n",
    "          \"model_name\": \"../input/deberta-v1-large-squadv2-ep2\",\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 32,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"folds\": [0, 1, 2, 3],\n",
    "          \"n_accumulate\": 4,\n",
    "          \"num_classes\": 2,\n",
    "          \"margin\": 0.5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          'opt_wd_non_norm_bias' : 0.01,\n",
    "          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n",
    "          'opt_beta1' : 0.9,\n",
    "          'opt_beta2' : 0.99,\n",
    "          'opt_eps' : 1e-5, # same as Adam in Fastai\n",
    "          'verbose': DEBUG,\n",
    "          'valid_check' : 8,\n",
    "          'workers':8,\n",
    "          'dropout' : 0.2,\n",
    "          'checkpointing':False,\n",
    "          }\n",
    "\n",
    "if CONFIG['fname'] in model_coeff:\n",
    "    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n",
    "    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
    "    \n",
    "    tokenizer = tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "    CONFIG[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n",
    "    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    tokenizer.add_tokens([lf_token])\n",
    "    \n",
    "    test_clean_notes = test_notes.copy()\n",
    "    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n",
    "    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n",
    "    char_preds_all.append(char_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1ba3598a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:39.304715Z",
     "iopub.status.busy": "2022-04-25T11:27:39.302459Z",
     "iopub.status.idle": "2022-04-25T11:27:39.305446Z",
     "shell.execute_reply": "2022-04-25T11:27:39.306025Z"
    },
    "papermill": {
     "duration": 0.082492,
     "end_time": "2022-04-25T11:27:39.306225",
     "exception": false,
     "start_time": "2022-04-25T11:27:39.223733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"fname\" : 'nbme_256_full',\n",
    "          'dirname' : 'nbme-256-full',\n",
    "          \"seed\": 2021,\n",
    "          \"epochs\": 10,\n",
    "          \"model_name\": \"../input/deberta-v1-large-squadv2-ep2\",\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 32,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"folds\": [0, 1, 2, 3],\n",
    "          \"n_accumulate\": 4,\n",
    "          \"num_classes\": 2,\n",
    "          \"margin\": 0.5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          'opt_wd_non_norm_bias' : 0.01,\n",
    "          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n",
    "          'opt_beta1' : 0.9,\n",
    "          'opt_beta2' : 0.99,\n",
    "          'opt_eps' : 1e-5, # same as Adam in Fastai\n",
    "          'verbose': DEBUG,\n",
    "          'valid_check' : 8,\n",
    "          'workers':8,\n",
    "          'dropout' : 0.2,\n",
    "          'checkpointing':False,\n",
    "          }\n",
    "\n",
    "if CONFIG['fname'] in model_coeff:\n",
    "    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n",
    "    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
    "    \n",
    "    tokenizer = tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "    CONFIG[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n",
    "    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    tokenizer.add_tokens([lf_token])\n",
    "    \n",
    "    test_clean_notes = test_notes.copy()\n",
    "    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n",
    "    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n",
    "    char_preds_all.append(char_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48eca64",
   "metadata": {
    "papermill": {
     "duration": 0.062633,
     "end_time": "2022-04-25T11:27:39.431554",
     "exception": false,
     "start_time": "2022-04-25T11:27:39.368921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# NBME 187/ 187_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c2c53af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:39.571744Z",
     "iopub.status.busy": "2022-04-25T11:27:39.570653Z",
     "iopub.status.idle": "2022-04-25T11:27:39.582148Z",
     "shell.execute_reply": "2022-04-25T11:27:39.581603Z"
    },
    "papermill": {
     "duration": 0.088992,
     "end_time": "2022-04-25T11:27:39.582285",
     "exception": false,
     "start_time": "2022-04-25T11:27:39.493293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class NBMEModel(nn.Module):\n",
    "    def __init__(self, model_name, loss=False, pretrained=True):\n",
    "        super(NBMEModel, self).__init__()\n",
    "        config = CONFIG['config']\n",
    "        self.config = config\n",
    "        if pretrained:\n",
    "            self.backbone = AutoModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            self.backbone = AutoModel.from_config(config)\n",
    "        self.backbone.resize_token_embeddings(len(CONFIG['tokenizer']))\n",
    "        self.loss = loss\n",
    "        #self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout0 = nn.Dropout(p=CONFIG['dropout'])\n",
    "        self.rnn0 = nn.GRU(CONFIG['config'].hidden_size,\n",
    "                           CONFIG['config'].hidden_size//2,\n",
    "                           num_layers=1,\n",
    "                           batch_first=True,\n",
    "                           bidirectional=True,\n",
    "                          )\n",
    "        self.token_type_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.token_start_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.token_end_head = NBMEHead(config.hidden_size, 1,\n",
    "                                           loss, criterion)\n",
    "        self.model_name = model_name\n",
    "        weight_data = torch.linspace(-5, 5, 1+self.config.num_hidden_layers)\n",
    "        weight_data = weight_data.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "        self.weights = nn.Parameter(weight_data, requires_grad=True)\n",
    "       \n",
    "    def forward(self, input_dict):     \n",
    "        input_ids = input_dict['input_ids']\n",
    "        attention_mask = input_dict['attention_mask']\n",
    "        if 'roberta' in self.model_name:\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                output_hidden_states=True)\n",
    "        else:\n",
    "            token_type_ids = input_dict['token_type_ids']\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        x = torch.stack(out.hidden_states)\n",
    "        w = F.softmax(self.weights, 0)\n",
    "        w = self.dropout0(w)\n",
    "        x = (w * x).sum(0)\n",
    "        x = self.rnn0(x)[0]\n",
    "        #x = out.hidden_states[-1]\n",
    "        if self.loss:\n",
    "            token_type = input_dict['token_type']\n",
    "            token_start = input_dict['token_start']\n",
    "            token_end = input_dict['token_end']\n",
    "            target_mask = (attention_mask * token_type_ids).unsqueeze(-1)\n",
    "        else:\n",
    "            token_type = None\n",
    "            token_start = None\n",
    "            token_end = None\n",
    "            target_mask = 1\n",
    "        token_type_logits, loss_type = self.token_type_head(x, target_mask, token_type) \n",
    "        token_start_logits, loss_start = self.token_start_head(x, target_mask, token_start) \n",
    "        token_end_logits, loss_end = self.token_end_head(x, target_mask, token_end)\n",
    "        out_dict = {\n",
    "            'token_type_logits' : token_type_logits,\n",
    "            'token_start_logits' : token_start_logits,\n",
    "            'token_end_logits' : token_end_logits,\n",
    "        }\n",
    "        \n",
    "        if self.loss:\n",
    "            loss = loss_type + loss_start + loss_end\n",
    "            token_type_pred = ((token_type_logits >= 0) * target_mask).detach()\n",
    "            tp_count = (token_type_pred * token_type).sum().detach().item()\n",
    "            all_count = (token_type_pred.sum() + token_type.sum()).detach().item()\n",
    "            #print(tp_count, token_type_pred.sum().item(), token_type.sum().item())\n",
    "            out_dict.update({\n",
    "                'loss' : loss,\n",
    "                'tp_count' : tp_count,\n",
    "                'all_count' : all_count,\n",
    "            })\n",
    "            \n",
    "        return out_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6d02ddfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:39.719518Z",
     "iopub.status.busy": "2022-04-25T11:27:39.714010Z",
     "iopub.status.idle": "2022-04-25T11:27:39.722570Z",
     "shell.execute_reply": "2022-04-25T11:27:39.721927Z"
    },
    "papermill": {
     "duration": 0.077915,
     "end_time": "2022-04-25T11:27:39.722697",
     "exception": false,
     "start_time": "2022-04-25T11:27:39.644782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"fname\" : 'nbme_187',\n",
    "          'dirname' : 'nbme-187',\n",
    "          \"seed\": 2021,\n",
    "          \"epochs\": 10,\n",
    "          \"model_name\": \"../input/deberta-v1-large-squadv2-ep2\",\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 32,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"folds\": [0, 1, 2, 3],\n",
    "          \"n_accumulate\": 4,\n",
    "          \"num_classes\": 2,\n",
    "          \"margin\": 0.5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          'opt_wd_non_norm_bias' : 0.01,\n",
    "          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n",
    "          'opt_beta1' : 0.9,\n",
    "          'opt_beta2' : 0.99,\n",
    "          'opt_eps' : 1e-5, # same as Adam in Fastai\n",
    "          'verbose': DEBUG,\n",
    "          'valid_check' : 8,\n",
    "          'workers':8,\n",
    "          'dropout' : 0.2,\n",
    "          'checkpointing':False,\n",
    "          }\n",
    "\n",
    "if CONFIG['fname'] in model_coeff:\n",
    "    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n",
    "    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
    "    \n",
    "    tokenizer = tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "    CONFIG[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n",
    "    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    tokenizer.add_tokens([lf_token])\n",
    "    \n",
    "    test_clean_notes = test_notes.copy()\n",
    "    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n",
    "    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n",
    "    char_preds_all.append(char_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c80348be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:39.860649Z",
     "iopub.status.busy": "2022-04-25T11:27:39.850131Z",
     "iopub.status.idle": "2022-04-25T11:27:39.863523Z",
     "shell.execute_reply": "2022-04-25T11:27:39.862989Z"
    },
    "papermill": {
     "duration": 0.079365,
     "end_time": "2022-04-25T11:27:39.863662",
     "exception": false,
     "start_time": "2022-04-25T11:27:39.784297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONFIG = {\"fname\" : 'nbme_187_full',\n",
    "          'dirname' : 'nbme-187-full',\n",
    "          \"seed\": 2021,\n",
    "          \"epochs\": 10,\n",
    "          \"model_name\": \"../input/deberta-v1-large-squadv2-ep2\",\n",
    "          \"train_batch_size\": 8,\n",
    "          \"valid_batch_size\": 32,\n",
    "          \"max_length\": 512,\n",
    "          \"learning_rate\": 1e-4,\n",
    "          \"scheduler\": 'CosineAnnealingLR',\n",
    "          \"min_lr\": 1e-8,\n",
    "          \"folds\": [0, 1, 2, 3],\n",
    "          \"n_accumulate\": 4,\n",
    "          \"num_classes\": 2,\n",
    "          \"margin\": 0.5,\n",
    "          \"device\": torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\"),\n",
    "          'opt_wd_non_norm_bias' : 0.01,\n",
    "          'opt_wd_norm_bias' : 0, # same as Adam in Fastai\n",
    "          'opt_beta1' : 0.9,\n",
    "          'opt_beta2' : 0.99,\n",
    "          'opt_eps' : 1e-5, # same as Adam in Fastai\n",
    "          'verbose': DEBUG,\n",
    "          'valid_check' : 8,\n",
    "          'workers':8,\n",
    "          'dropout' : 0.2,\n",
    "          'checkpointing':False,\n",
    "          }\n",
    "\n",
    "if CONFIG['fname'] in model_coeff:\n",
    "    model_coeff_all.append(model_coeff[CONFIG['fname'] ])\n",
    "    CONFIG[\"config\"] = AutoConfig.from_pretrained(CONFIG['model_name'])\n",
    "    \n",
    "    tokenizer = tokenizer = AutoTokenizer.from_pretrained(CONFIG['model_name'])\n",
    "    CONFIG[\"tokenizer\"] = tokenizer\n",
    "\n",
    "    lf_token = AddedToken(\"\\r\\n\", lstrip=True, rstrip=True)\n",
    "    fh_token = AddedToken(\"fh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    pmh_token = AddedToken(\"pmh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    sh_token = AddedToken(\"sh\", single_word=True, lstrip=False, rstrip=True)\n",
    "    tokenizer.add_tokens([lf_token])\n",
    "    \n",
    "    test_clean_notes = test_notes.copy()\n",
    "    test_clean_notes['pn_history'] = [clean_abbrev(text) for text in test_clean_notes['pn_history'] ]\n",
    "    char_preds = get_char_preds(test, CONFIG, test_clean_notes)\n",
    "    char_preds_all.append(char_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a19496b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:39.994958Z",
     "iopub.status.busy": "2022-04-25T11:27:39.993866Z",
     "iopub.status.idle": "2022-04-25T11:27:39.998124Z",
     "shell.execute_reply": "2022-04-25T11:27:39.998838Z"
    },
    "papermill": {
     "duration": 0.072085,
     "end_time": "2022-04-25T11:27:39.999041",
     "exception": false,
     "start_time": "2022-04-25T11:27:39.926956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_coeff_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21f4e5a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:40.132420Z",
     "iopub.status.busy": "2022-04-25T11:27:40.131347Z",
     "iopub.status.idle": "2022-04-25T11:27:40.137385Z",
     "shell.execute_reply": "2022-04-25T11:27:40.136706Z"
    },
    "papermill": {
     "duration": 0.073131,
     "end_time": "2022-04-25T11:27:40.137526",
     "exception": false,
     "start_time": "2022-04-25T11:27:40.064395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_coeff_all = np.array(model_coeff_all) / np.sum(model_coeff_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c45d59e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:40.272793Z",
     "iopub.status.busy": "2022-04-25T11:27:40.271939Z",
     "iopub.status.idle": "2022-04-25T11:27:40.275585Z",
     "shell.execute_reply": "2022-04-25T11:27:40.276169Z"
    },
    "papermill": {
     "duration": 0.075018,
     "end_time": "2022-04-25T11:27:40.276354",
     "exception": false,
     "start_time": "2022-04-25T11:27:40.201336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_coeff_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce14fc01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:40.410031Z",
     "iopub.status.busy": "2022-04-25T11:27:40.409113Z",
     "iopub.status.idle": "2022-04-25T11:27:40.412634Z",
     "shell.execute_reply": "2022-04-25T11:27:40.412116Z"
    },
    "papermill": {
     "duration": 0.072703,
     "end_time": "2022-04-25T11:27:40.412762",
     "exception": false,
     "start_time": "2022-04-25T11:27:40.340059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit, logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "011d30a5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:40.570422Z",
     "iopub.status.busy": "2022-04-25T11:27:40.568558Z",
     "iopub.status.idle": "2022-04-25T11:27:40.589771Z",
     "shell.execute_reply": "2022-04-25T11:27:40.589107Z"
    },
    "papermill": {
     "duration": 0.111608,
     "end_time": "2022-04-25T11:27:40.589987",
     "exception": false,
     "start_time": "2022-04-25T11:27:40.478379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>696 724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>668 693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00016_002</td>\n",
       "      <td>203 217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016_003</td>\n",
       "      <td>70 91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00016_004</td>\n",
       "      <td>222 258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id location\n",
       "0  00016_000  696 724\n",
       "1  00016_001  668 693\n",
       "2  00016_002  203 217\n",
       "3  00016_003    70 91\n",
       "4  00016_004  222 258"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_all = [chars_preds[1] for chars_preds in char_preds_all[0]]\n",
    "text_all[:2]\n",
    "\n",
    "chars_all = [[chars_preds[0] for chars_preds in char_preds_all_i] \\\n",
    "             for char_preds_all_i in char_preds_all]\n",
    "#chars_all[0][:2]\n",
    "len(chars_all), len(chars_all[0])\n",
    "\n",
    "\n",
    "chars_all_final = [expit(p) * model_coeff_all[0] for p in chars_all[0]]\n",
    "for chars_all_i, model_coeff_all_i in zip(chars_all[1:], model_coeff_all[1:]):\n",
    "    chars_all_final = [p + (expit(p1) * model_coeff_all_i) for p,p1 in zip(chars_all_final, chars_all_i)]\n",
    "\n",
    "sub = get_preds(chars_all_final, text_all, expit(pred_thr))\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5cde0669",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-25T11:27:40.730206Z",
     "iopub.status.busy": "2022-04-25T11:27:40.729113Z",
     "iopub.status.idle": "2022-04-25T11:27:40.739526Z",
     "shell.execute_reply": "2022-04-25T11:27:40.738907Z"
    },
    "papermill": {
     "duration": 0.079398,
     "end_time": "2022-04-25T11:27:40.739663",
     "exception": false,
     "start_time": "2022-04-25T11:27:40.660265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebba843",
   "metadata": {
    "papermill": {
     "duration": 0.066949,
     "end_time": "2022-04-25T11:27:40.872514",
     "exception": false,
     "start_time": "2022-04-25T11:27:40.805565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 130.861954,
   "end_time": "2022-04-25T11:27:43.938878",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-25T11:25:33.076924",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

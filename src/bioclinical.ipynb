{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (11440, 10)\n",
      "Test shape: (2860, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mdtej\\.cache\\huggingface\\hub\\models--emilyalsentzer--Bio_ClinicalBERT. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is cpu\n",
      "[2024-04-20 15:13:04,748: INFO: 3051437145: Epoch: 1/10]\n",
      "Epoch: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1907 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:13:26,794: INFO: 3051437145: Training : batch 0 Loss: 0.7043297051433094]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1907 [00:22<11:41:15, 22.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:13:49,728: INFO: 3051437145: Training : batch 1 Loss: 0.6670677749672587]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1907 [00:44<11:56:34, 22.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:14:07,603: INFO: 3051437145: Training : batch 2 Loss: 0.6317735246786813]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1907 [01:02<10:48:02, 20.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:14:24,705: INFO: 3051437145: Training : batch 3 Loss: 0.6001182171779823]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1907 [01:19<10:06:00, 19.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:14:40,298: INFO: 3051437145: Training : batch 4 Loss: 0.5740020827844557]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1907 [01:35<9:25:38, 17.84s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:14:55,210: INFO: 3051437145: Training : batch 5 Loss: 0.5454400020998389]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1907 [01:50<8:53:39, 16.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:15:10,004: INFO: 3051437145: Training : batch 6 Loss: 0.5108826939838336]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/1907 [02:05<8:32:09, 16.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:15:25,609: INFO: 3051437145: Training : batch 7 Loss: 0.48375889400863115]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 8/1907 [02:20<8:26:11, 15.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:15:43,292: INFO: 3051437145: Training : batch 8 Loss: 0.4478633317973083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/1907 [02:38<8:42:39, 16.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:15:58,784: INFO: 3051437145: Training : batch 9 Loss: 0.4167595760969957]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1907 [02:54<8:32:16, 16.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:16:14,342: INFO: 3051437145: Training : batch 10 Loss: 0.3958309350057553]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1907 [03:09<8:25:46, 16.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 15:16:30,092: INFO: 3051437145: Training : batch 11 Loss: 0.3593943850404915]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1907 [03:41<9:42:57, 18.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 296\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, epochs))\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# first train model\u001b[39;00m\n\u001b[1;32m--> 296\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    297\u001b[0m train_loss_data\u001b[38;5;241m.\u001b[39mappend(train_loss)\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 219\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, optimizer, criterion)\u001b[0m\n\u001b[0;32m    217\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmasked_select(loss, labels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    218\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m input_ids\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m))\n\u001b[1;32m--> 219\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m    221\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from logHelper import logger\n",
    "#Import the necessary modules\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, RobertaTokenizerFast, RobertaModel\n",
    "from transformers import DebertaModel, DebertaTokenizerFast\n",
    "import nltk\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "\n",
    "base_config = {\n",
    "    \"Base_data_path\": \"../data/nbme-score-clinical-patient-notes\",\n",
    "    \"max_length\": 416,\n",
    "    \"padding\": \"max_length\",\n",
    "    \"return_offsets_mapping\": True,\n",
    "    \"truncation\": \"only_second\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"lr\": 1e-5,\n",
    "    \"test_size\": 0.2,\n",
    "    \"seed\": 1268,\n",
    "    \"batch_size\": 6,\n",
    "    \"model_name\": \"microsoft/deberta-base\"\n",
    "}\n",
    "\n",
    "class prepare_data():\n",
    "    def __init__(self , config):\n",
    "        self.config = config\n",
    "    def merge_data(self):\n",
    "        features = pd.read_csv(f\"{self.config['Base_data_path']}/features.csv\")\n",
    "        patient_notes = pd.read_csv(f\"{self.config['Base_data_path']}/patient_notes.csv\")\n",
    "        train_df = pd.read_csv(f\"{self.config['Base_data_path']}/train.csv\")\n",
    "        train_df['annotation_list'] = [literal_eval(x) for x in train_df['annotation']]\n",
    "        train_df['location_list'] = [literal_eval(x) for x in train_df['location']]\n",
    "        \n",
    "        merged = train_df.merge(patient_notes, how='left')\n",
    "        merged = merged.merge(features, how='left')\n",
    "\n",
    "        merged['pn_history'] = merged['pn_history'].apply(lambda x: x.lower())\n",
    "        merged['feature_text'] = merged['feature_text'].apply(lambda x: x.lower())\n",
    "        merged['feature_text'] = merged['feature_text'].apply(lambda x: x.replace('-', ' ').replace('-OR-', \";-\"))\n",
    "        return merged\n",
    "    \n",
    "class pre_process_data():\n",
    "    def __init__(self , config):\n",
    "        self.config = config\n",
    "    def split_loc(self , loc_list):\n",
    "        final_loc = []\n",
    "        for loc in loc_list:\n",
    "            locations = loc.split(';')\n",
    "            for location in locations:\n",
    "                start , end = location.split()\n",
    "                final_loc.append((int(start) , int(end)))\n",
    "        return final_loc\n",
    "    # so basically tokenizer divides the text so to assign whether the label belong to this or not we assign values accordingly\n",
    "    def tokenize_and_addLabels(self , data , tokenizer , config):\n",
    "        tokenized = tokenizer(\n",
    "            data['feature_text'],\n",
    "            data['pn_history'],\n",
    "            truncation= config['truncation'],\n",
    "            max_length= config['max_length'],\n",
    "            padding= config['padding'],\n",
    "            return_offsets_mapping=config['return_offsets_mapping']\n",
    "        )\n",
    "        labels = [0.0] * len(tokenized['input_ids'])\n",
    "        tokenized['location_int'] = self.split_loc(data['location_list'])\n",
    "        tokenized['sequence_ids'] = tokenized.sequence_ids()\n",
    "\n",
    "        for idx, (seq_id, offsets) in enumerate(zip(tokenized[\"sequence_ids\"], tokenized[\"offset_mapping\"])):\n",
    "            if not seq_id or seq_id == 0:\n",
    "                labels[idx] = -1\n",
    "                continue\n",
    "\n",
    "            token_start , token_end = offsets\n",
    "            for feature_start , feature_end in tokenized['location_int']:\n",
    "                if token_start >= feature_start and token_end <= feature_end:\n",
    "                    labels[idx] = 1.0\n",
    "                    break\n",
    "        \n",
    "        tokenized['labels'] = labels\n",
    "        return tokenized\n",
    "\n",
    "class score_class():\n",
    "    def __init__(self , config):\n",
    "        self.config = config\n",
    "    def get_location_predictions(self , preds , offset_mapping , sequence_ids , test = False):\n",
    "        all_predictions = []\n",
    "        for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n",
    "            pred = 1 / (1+ np.exp(-pred))\n",
    "            start_idx = None\n",
    "            end_idx = None\n",
    "            current_preds = []\n",
    "            for pred , offset , seq_id in zip(pred , offsets , seq_ids):\n",
    "                if seq_id is None or seq_id == 0:\n",
    "                    continue\n",
    "                if pred >0.5:\n",
    "                    if start_idx is None:\n",
    "                        start_idx = offset[0]\n",
    "                    end_idx = offset[1]\n",
    "                elif start_idx is not None:\n",
    "                    if test:\n",
    "                        current_preds.append(f\"{start_idx} {end_idx}\")\n",
    "                    else:\n",
    "                        current_preds.append((start_idx, end_idx))\n",
    "                    start_idx = None\n",
    "            if test:\n",
    "                all_predictions.append(\"; \".join(current_preds))\n",
    "            else:\n",
    "                all_predictions.append(current_preds)\n",
    "            \n",
    "        return all_predictions\n",
    "    def calculate_char_cv(self , predictions, offset_mapping, sequence_ids, labels):\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n",
    "\n",
    "            num_chars = max(list(chain(*offsets)))\n",
    "            char_labels = np.zeros(num_chars)\n",
    "\n",
    "            for o, s_id, label in zip(offsets, seq_ids, labels):\n",
    "                if s_id is None or s_id == 0:\n",
    "                    continue\n",
    "                if int(label) == 1:\n",
    "                    char_labels[o[0]:o[1]] = 1\n",
    "\n",
    "            char_preds = np.zeros(num_chars)\n",
    "\n",
    "            for start_idx, end_idx in preds:\n",
    "                char_preds[start_idx:end_idx] = 1\n",
    "\n",
    "            all_labels.extend(char_labels)\n",
    "            all_preds.extend(char_preds)\n",
    "\n",
    "        results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"precision\": results[0],\n",
    "            \"recall\": results[1],\n",
    "            \"f1\": results[2]\n",
    "        }\n",
    "    \n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, config):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        tokenizerObj = pre_process_data(self.config)\n",
    "        tokens = tokenizerObj.tokenize_and_addLabels( data, self.tokenizer, self.config)\n",
    "\n",
    "        input_ids = np.array(tokens[\"input_ids\"])\n",
    "        attention_mask = np.array(tokens[\"attention_mask\"])\n",
    "#         token_type_ids = __getitem__np.array(tokens[\"token_type_ids\"])\n",
    "\n",
    "        labels = np.array(tokens[\"labels\"])\n",
    "        offset_mapping = np.array(tokens['offset_mapping'])\n",
    "        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n",
    "        \n",
    "        return input_ids, attention_mask, labels, offset_mapping, sequence_ids\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        #self.deberta = DebertaModel.from_pretrained(config['model_name'])  # DeBERTa model\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "        self.dropout = nn.Dropout(p=config['dropout'])\n",
    "        self.config = config\n",
    "        self.fc1 = nn.Linear(768, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        #outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = self.fc1(outputs[0])\n",
    "        logits = self.fc2(self.dropout(logits))\n",
    "        logits = self.fc3(self.dropout(logits)).squeeze(-1)\n",
    "        return logits\n",
    "\n",
    "def train_model(model , dataloader , optimizer , criterion):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    count = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        # print(f'Batch {count}')\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(DEVICE)\n",
    "        attention_mask = batch[1].to(DEVICE)\n",
    "        labels = batch[2].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss = torch.masked_select(loss, labels > -1.0).mean()\n",
    "        train_loss.append(loss.item() * input_ids.size(0))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        logger.info(f\"Training : batch {count} Loss: {loss.item()}\")\n",
    "        count+=1\n",
    "    return sum(train_loss) / len(train_loss)\n",
    "\n",
    "def eval_model(model, dataloader, criterion):\n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        preds = []\n",
    "        offsets = []\n",
    "        seq_ids = []\n",
    "        valid_labels = []\n",
    "        count = 0 \n",
    "        for batch in tqdm(dataloader):\n",
    "            # print(f'batch {count}')\n",
    "            input_ids = batch[0].to(DEVICE)\n",
    "            attention_mask = batch[1].to(DEVICE)\n",
    "#             token_type_ids = batch[2].to(DEVICE)\n",
    "            labels = batch[2].to(DEVICE)\n",
    "            offset_mapping = batch[3]\n",
    "            sequence_ids = batch[4]\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss = torch.masked_select(loss, labels > -1.0).mean()\n",
    "            valid_loss.append(loss.item() * input_ids.size(0))\n",
    "\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "            offsets.append(offset_mapping.numpy())\n",
    "            seq_ids.append(sequence_ids.numpy())\n",
    "            valid_labels.append(labels.detach().cpu().numpy())\n",
    "            logger.info(f\"Eval Epoch : batch {count} Loss: {loss.item()}\")\n",
    "            count+=1\n",
    "\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        offsets = np.concatenate(offsets, axis=0)\n",
    "        seq_ids = np.concatenate(seq_ids, axis=0)\n",
    "        valid_labels = np.concatenate(valid_labels, axis=0)\n",
    "        score_obj = score_class(base_config)\n",
    "        location_preds = score_obj.get_location_predictions(preds, offsets, seq_ids, test=False)\n",
    "        score = score_obj.calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n",
    "        logger.info(f\"The score of the eval model is {score}\")\n",
    "        return sum(valid_loss)/len(valid_loss), score\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "    obj = prepare_data(base_config)\n",
    "    train_df = obj.merge_data()\n",
    "    X_train , X_test = train_test_split(train_df, test_size= base_config['test_size'], random_state= base_config['seed'])\n",
    "    print(f\"Train shape: {X_train.shape}\")\n",
    "    print(f\"Test shape: {X_test.shape}\")\n",
    "    #tokenizer = DebertaTokenizerFast.from_pretrained(base_config['model_name'])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "    training_data = CustomDataset(X_train, tokenizer, base_config)\n",
    "    train_dataloader = DataLoader(training_data, batch_size=base_config['batch_size'], shuffle=True)\n",
    "\n",
    "    testing_data = CustomDataset(X_test, tokenizer, base_config)\n",
    "    test_dataloader = DataLoader(testing_data, batch_size=base_config['batch_size'], shuffle=False)\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f'The device is {DEVICE}')\n",
    "    model = CustomModel(base_config).to(DEVICE)\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=base_config['lr'])\n",
    "    train_loss_data, valid_loss_data = [], []\n",
    "    score_data_list = []\n",
    "    valid_loss_min = np.Inf\n",
    "    since = time.time()\n",
    "    epochs = 10\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for i in range(epochs):\n",
    "        logger.info(f\"Epoch: {i + 1}/{epochs}\")\n",
    "        print(\"Epoch: {}/{}\".format(i + 1, epochs))\n",
    "        # first train model\n",
    "        train_loss = train_model(model, train_dataloader, optimizer, criterion)\n",
    "        train_loss_data.append(train_loss)\n",
    "        print(f\"Train loss: {train_loss}\")\n",
    "        # evaluate model\n",
    "        valid_loss, score = eval_model(model, test_dataloader, criterion)\n",
    "        valid_loss_data.append(valid_loss)\n",
    "        score_data_list.append(score)\n",
    "        print(f\"Valid loss: {valid_loss}\")\n",
    "        print(f\"Valid score: {score}\")\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), \"nbme_bert_v2.pth\")\n",
    "\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    pd.to_pickle(train_loss_data, \"train_loss_data.pkl\")\n",
    "    pd.to_pickle(valid_loss_data, \"valid_loss_data.pkl\")\n",
    "    plt.plot(train_loss_data, label=\"Training loss\")\n",
    "    plt.plot(valid_loss_data, label=\"validation loss\")\n",
    "    plt.legend(frameon=False)\n",
    "    score_df = pd.DataFrame.from_dict(score_data_list)\n",
    "    score_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from logHelper import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 14:35:22,941: INFO: utils: NumExpr defaulting to 8 threads.]\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary modules\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from itertools import chain\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModel, AutoTokenizer, RobertaTokenizerFast, RobertaModel\n",
    "from transformers import DebertaModel, DebertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to create process using 'C:\\Users\\Kartheek Kotha\\.conda\\envs\\nlp\\python.exe \"C:\\Users\\Kartheek Kotha\\.conda\\envs\\nlp\\Scripts\\pip-script.py\" install nltk'\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary modules for NLP\n",
    "! pip install nltk\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = {\n",
    "    \"Base_data_path\": \"../../../data/nbme-score-clinical-patient-notes\",\n",
    "    \"max_length\": 416,\n",
    "    \"padding\": \"max_length\",\n",
    "    \"return_offsets_mapping\": True,\n",
    "    \"truncation\": \"only_second\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"lr\": 1e-5,\n",
    "    \"test_size\": 0.2,\n",
    "    \"seed\": 1268,\n",
    "    \"batch_size\": 8,\n",
    "    \"model_name\": \"microsoft/deberta-base\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prepare_data():\n",
    "    def __init__(self , config):\n",
    "        self.config = config\n",
    "    def merge_data(self):\n",
    "        features = pd.read_csv(f\"{self.config['Base_data_path']}/features.csv\")\n",
    "        patient_notes = pd.read_csv(f\"{self.config['Base_data_path']}/patient_notes.csv\")\n",
    "        train_df = pd.read_csv(f\"{self.config['Base_data_path']}/train.csv\")\n",
    "        train_df['annotation_list'] = [literal_eval(x) for x in train_df['annotation']]\n",
    "        train_df['location_list'] = [literal_eval(x) for x in train_df['location']]\n",
    "        \n",
    "        merged = train_df.merge(patient_notes, how='left')\n",
    "        merged = merged.merge(features, how='left')\n",
    "\n",
    "        merged['pn_history'] = merged['pn_history'].apply(lambda x: x.lower())\n",
    "        merged['feature_text'] = merged['feature_text'].apply(lambda x: x.lower())\n",
    "        merged['feature_text'] = merged['feature_text'].apply(lambda x: x.replace('-', ' ').replace('-OR-', \";-\"))\n",
    "        return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To pre-process data - Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pre_process_data():\n",
    "    def __init__(self , config):\n",
    "        self.config = config\n",
    "    def split_loc(self , loc_list):\n",
    "        final_loc = []\n",
    "        for loc in loc_list:\n",
    "            locations = loc.split(';')\n",
    "            for location in locations:\n",
    "                start , end = location.split()\n",
    "                final_loc.append((int(start) , int(end)))\n",
    "        return final_loc\n",
    "    # so basically tokenizer divides the text so to assign whether the label belong to this or not we assign values accordingly\n",
    "    def tokenize_and_addLabels(self , data , tokenizer , config):\n",
    "        tokenized = tokenizer(\n",
    "            data['feature_text'],\n",
    "            data['pn_history'],\n",
    "            truncation= config['truncation'],\n",
    "            max_length= config['max_length'],\n",
    "            padding= config['padding'],\n",
    "            return_offsets_mapping=config['return_offsets_mapping']\n",
    "        )\n",
    "        labels = [0.0] * len(tokenized['input_ids'])\n",
    "        tokenized['location_int'] = self.split_loc(data['location_list'])\n",
    "        tokenized['sequence_ids'] = tokenized.sequence_ids()\n",
    "\n",
    "        for idx, (seq_id, offsets) in enumerate(zip(tokenized[\"sequence_ids\"], tokenized[\"offset_mapping\"])):\n",
    "            if not seq_id or seq_id == 0:\n",
    "                labels[idx] = -1\n",
    "                continue\n",
    "\n",
    "            token_start , token_end = offsets\n",
    "            for feature_start , feature_end in tokenized['location_int']:\n",
    "                if token_start >= feature_start and token_end <= feature_end:\n",
    "                    labels[idx] = 1.0\n",
    "                    break\n",
    "        \n",
    "        tokenized['labels'] = labels\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Score and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class score_class():\n",
    "    def __init__(self , config):\n",
    "        self.config = config\n",
    "    def get_location_predictions(self , preds , offset_mapping , sequence_ids , test = False):\n",
    "        all_predictions = []\n",
    "        for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n",
    "            pred = 1 / (1+ np.exp(-pred))\n",
    "            start_idx = None\n",
    "            end_idx = None\n",
    "            current_preds = []\n",
    "            for pred , offset , seq_id in zip(pred , offsets , seq_ids):\n",
    "                if seq_id is None or seq_id == 0:\n",
    "                    continue\n",
    "                if pred >0.5:\n",
    "                    if start_idx is None:\n",
    "                        start_idx = offset[0]\n",
    "                    end_idx = offset[1]\n",
    "                elif start_idx is not None:\n",
    "                    if test:\n",
    "                        current_preds.append(f\"{start_idx} {end_idx}\")\n",
    "                    else:\n",
    "                        current_preds.append((start_idx, end_idx))\n",
    "                    start_idx = None\n",
    "            if test:\n",
    "                all_predictions.append(\"; \".join(current_preds))\n",
    "            else:\n",
    "                all_predictions.append(current_preds)\n",
    "            \n",
    "        return all_predictions\n",
    "    def calculate_char_cv(self , predictions, offset_mapping, sequence_ids, labels):\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        for preds, offsets, seq_ids, labels in zip(predictions, offset_mapping, sequence_ids, labels):\n",
    "\n",
    "            num_chars = max(list(chain(*offsets)))\n",
    "            char_labels = np.zeros(num_chars)\n",
    "\n",
    "            for o, s_id, label in zip(offsets, seq_ids, labels):\n",
    "                if s_id is None or s_id == 0:\n",
    "                    continue\n",
    "                if int(label) == 1:\n",
    "                    char_labels[o[0]:o[1]] = 1\n",
    "\n",
    "            char_preds = np.zeros(num_chars)\n",
    "\n",
    "            for start_idx, end_idx in preds:\n",
    "                char_preds[start_idx:end_idx] = 1\n",
    "\n",
    "            all_labels.extend(char_labels)\n",
    "            all_preds.extend(char_preds)\n",
    "\n",
    "        results = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", labels=np.unique(all_preds))\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "\n",
    "        return {\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"precision\": results[0],\n",
    "            \"recall\": results[1],\n",
    "            \"f1\": results[2]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, config):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data.iloc[idx]\n",
    "        tokenizerObj = pre_process_data(self.config)\n",
    "        tokens = tokenizerObj.tokenize_and_addLabels( data, self.tokenizer, self.config)\n",
    "\n",
    "        input_ids = np.array(tokens[\"input_ids\"])\n",
    "        attention_mask = np.array(tokens[\"attention_mask\"])\n",
    "#         token_type_ids = __getitem__np.array(tokens[\"token_type_ids\"])\n",
    "\n",
    "        labels = np.array(tokens[\"labels\"])\n",
    "        offset_mapping = np.array(tokens['offset_mapping'])\n",
    "        sequence_ids = np.array(tokens['sequence_ids']).astype(\"float16\")\n",
    "        \n",
    "        return input_ids, attention_mask, labels, offset_mapping, sequence_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.deberta = DebertaModel.from_pretrained(config['model_name'])  # DeBERTa model\n",
    "        self.lstm = nn.LSTM(input_size=768, hidden_size=256, num_layers=1, batch_first=True , bidirectional=True)  # LSTM layer\n",
    "        self.dropout = nn.Dropout(p=config['dropout'])\n",
    "        self.config = config\n",
    "        self.fc1 = nn.Linear(512, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_outputs = outputs.last_hidden_state\n",
    "        outputs , _ = self.lstm(sequence_outputs)\n",
    "        logits = self.fc1(outputs)\n",
    "        logits = self.fc2(self.dropout(logits))\n",
    "        logits = self.fc3(self.dropout(logits)).squeeze(-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = prepare_data(base_config)\n",
    "train_df = obj.merge_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "tokenizer = DebertaTokenizerFast.from_pretrained(base_config['model_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_folds = 3\n",
    "kf = KFold(n_splits=k_folds, shuffle=True, random_state=base_config['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The device is cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'The device is {DEVICE}')\n",
    "model = CustomModel(base_config).to(DEVICE)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "optimizer = optim.AdamW(model.parameters(), lr=base_config['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model , dataloader , optimizer , criterion):\n",
    "    model.train()\n",
    "    train_loss = []\n",
    "    count = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        # print(f'Batch {count}')\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[0].to(DEVICE)\n",
    "        attention_mask = batch[1].to(DEVICE)\n",
    "        labels = batch[2].to(DEVICE)\n",
    "\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss = torch.masked_select(loss, labels > -1.0).mean()\n",
    "        train_loss.append(loss.item() * input_ids.size(0))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        logger.info(f\"Training : batch {count} Loss: {loss.item()}\")\n",
    "        count+=1\n",
    "    return sum(train_loss) / len(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader, criterion):\n",
    "        model.eval()\n",
    "        valid_loss = []\n",
    "        preds = []\n",
    "        offsets = []\n",
    "        seq_ids = []\n",
    "        valid_labels = []\n",
    "        count = 0 \n",
    "        for batch in tqdm(dataloader):\n",
    "            # print(f'batch {count}')\n",
    "            input_ids = batch[0].to(DEVICE)\n",
    "            attention_mask = batch[1].to(DEVICE)\n",
    "#             token_type_ids = batch[2].to(DEVICE)\n",
    "            labels = batch[2].to(DEVICE)\n",
    "            offset_mapping = batch[3]\n",
    "            sequence_ids = batch[4]\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss = torch.masked_select(loss, labels > -1.0).mean()\n",
    "            valid_loss.append(loss.item() * input_ids.size(0))\n",
    "\n",
    "            preds.append(logits.detach().cpu().numpy())\n",
    "            offsets.append(offset_mapping.numpy())\n",
    "            seq_ids.append(sequence_ids.numpy())\n",
    "            valid_labels.append(labels.detach().cpu().numpy())\n",
    "            logger.info(f\"Eval Epoch : batch {count} Loss: {loss.item()}\")\n",
    "            count+=1\n",
    "\n",
    "        preds = np.concatenate(preds, axis=0)\n",
    "        offsets = np.concatenate(offsets, axis=0)\n",
    "        seq_ids = np.concatenate(seq_ids, axis=0)\n",
    "        valid_labels = np.concatenate(valid_labels, axis=0)\n",
    "        score_obj = score_class(base_config)\n",
    "        location_preds = score_obj.get_location_predictions(preds, offsets, seq_ids, test=False)\n",
    "        score = score_obj.calculate_char_cv(location_preds, offsets, seq_ids, valid_labels)\n",
    "        logger.info(f\"The score of the eval model is {score}\")\n",
    "        return sum(valid_loss)/len(valid_loss), score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "train_loss_data, valid_loss_data = [], []\n",
    "score_data_list = []\n",
    "valid_loss_min = np.Inf\n",
    "since = time.time()\n",
    "epochs = 3\n",
    "fold_train_loss = []\n",
    "fold_valid_loss = []\n",
    "fold_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/3\n",
      "[2024-04-20 14:47:20,448: INFO: 625199155: Epoch: 1/5]\n",
      "Epoch: 1/5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b958ce4166640b7b0205378e67c398e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-20 14:48:08,467: INFO: 3013262975: Training : batch 0 Loss: 0.7031705568487301]\n",
      "[2024-04-20 14:48:43,800: INFO: 3013262975: Training : batch 1 Loss: 0.6886493501266345]\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(train_df)):\n",
    "    print(f\"Fold {fold + 1}/{k_folds}\")\n",
    "    X_train, X_test = train_df.iloc[train_index], train_df.iloc[test_index]\n",
    "\n",
    "    training_data = CustomDataset(X_train, tokenizer, base_config)\n",
    "    train_dataloader = DataLoader(training_data, batch_size=base_config['batch_size'], shuffle=True)\n",
    "\n",
    "    testing_data = CustomDataset(X_test, tokenizer, base_config)\n",
    "    test_dataloader = DataLoader(testing_data, batch_size=base_config['batch_size'], shuffle=False)\n",
    "\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        logger.info(f\"Epoch: {epoch + 1}/{epochs}\")\n",
    "        print(\"Epoch: {}/{}\".format(epoch + 1, epochs))\n",
    "        # Train model\n",
    "        train_loss = train_model(model, train_dataloader, optimizer, criterion)\n",
    "        train_loss_data.append(train_loss)\n",
    "        print(f\"Train loss: {train_loss}\")\n",
    "        fold_train_loss.append(train_loss)\n",
    "        # Evaluate model\n",
    "        valid_loss, score = eval_model(model, test_dataloader, criterion)\n",
    "        valid_loss_data.append(valid_loss)\n",
    "        fold_valid_loss.append(valid_loss)\n",
    "        score_data_list.append(score)\n",
    "        print(f\"Valid loss: {valid_loss}\")\n",
    "        print(f\"Valid score: {score}\")\n",
    "\n",
    "        if valid_loss < best_loss:\n",
    "            best_loss = valid_loss\n",
    "            torch.save(model.state_dict(), f\"nbme_bert_fold{fold}_v2.pth\")\n",
    "        with open(f\"fold_{fold}_data.txt\", \"w\") as file:\n",
    "            file.write(f\"Train Loss: {fold_train_loss}\\n\")\n",
    "            file.write(f\"Valid Loss: {fold_valid_loss}\\n\")\n",
    "            file.write(f\"Scores: {fold_scores}\\n\")\n",
    "\n",
    "        # Clear fold data lists for next iteration\n",
    "        fold_train_loss.clear()\n",
    "        fold_valid_loss.clear()\n",
    "        fold_scores.clear()\n",
    "\n",
    "# After all folds are done, calculate average scores if needed\n",
    "avg_train_loss = sum(train_loss_data) / len(train_loss_data)\n",
    "avg_valid_loss = sum(valid_loss_data) / len(valid_loss_data)\n",
    "print(f\"Average Train Loss: {avg_train_loss}\")\n",
    "print(f\"Average Valid Loss: {avg_valid_loss}\")\n",
    "\n",
    "# Plotting\n",
    "pd.to_pickle(train_loss_data, \"train_loss_data.pkl\")\n",
    "pd.to_pickle(valid_loss_data, \"valid_loss_data.pkl\")\n",
    "plt.plot(train_loss_data, label=\"Training loss\")\n",
    "plt.plot(valid_loss_data, label=\"Validation loss\")\n",
    "plt.legend(frameon=False)\n",
    "plt.show()\n",
    "\n",
    "# Convert score data to dataframe if needed\n",
    "score_df = pd.DataFrame.from_dict(score_data_list)\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(train_loss_data, \"train_loss_data.pkl\")\n",
    "pd.to_pickle(valid_loss_data, \"valid_loss_data.pkl\")\n",
    "plt.plot(train_loss_data, label=\"Training loss\")\n",
    "plt.plot(valid_loss_data, label=\"validation loss\")\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame.from_dict(score_data_list)\n",
    "score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing purpose below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14300 entries, 0 to 14299\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           14300 non-null  object\n",
      " 1   case_num     14300 non-null  int64 \n",
      " 2   pn_num       14300 non-null  int64 \n",
      " 3   feature_num  14300 non-null  int64 \n",
      " 4   annotation   14300 non-null  object\n",
      " 5   location     14300 non-null  object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 670.4+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num                        annotation  \\\n",
       "0  00016_000         0      16            0  ['dad with recent heart attcak']   \n",
       "1  00016_001         0      16            1     ['mom with \"thyroid disease']   \n",
       "\n",
       "      location  \n",
       "0  ['696 724']  \n",
       "1  ['668 693']  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read data\n",
    "df = pd.read_csv(f\"{base_config['Base_data_path']}/train.csv\")\n",
    "print(df.info())\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14300 entries, 0 to 14299\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               14300 non-null  object\n",
      " 1   case_num         14300 non-null  int64 \n",
      " 2   pn_num           14300 non-null  int64 \n",
      " 3   feature_num      14300 non-null  int64 \n",
      " 4   annotation       14300 non-null  object\n",
      " 5   location         14300 non-null  object\n",
      " 6   annotation_list  14300 non-null  object\n",
      " 7   location_list    14300 non-null  object\n",
      " 8   pn_history       14300 non-null  object\n",
      " 9   feature_text     14300 non-null  object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>case_num</th>\n",
       "      <th>pn_num</th>\n",
       "      <th>feature_num</th>\n",
       "      <th>annotation</th>\n",
       "      <th>location</th>\n",
       "      <th>annotation_list</th>\n",
       "      <th>location_list</th>\n",
       "      <th>pn_history</th>\n",
       "      <th>feature_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00016_000</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>['dad with recent heart attcak']</td>\n",
       "      <td>['696 724']</td>\n",
       "      <td>[dad with recent heart attcak]</td>\n",
       "      <td>[696 724]</td>\n",
       "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
       "      <td>family history of mi or family history of myoc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00016_001</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>['mom with \"thyroid disease']</td>\n",
       "      <td>['668 693']</td>\n",
       "      <td>[mom with \"thyroid disease]</td>\n",
       "      <td>[668 693]</td>\n",
       "      <td>hpi: 17yo m presents with palpitations. patien...</td>\n",
       "      <td>family history of thyroid disorder</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  case_num  pn_num  feature_num                        annotation  \\\n",
       "0  00016_000         0      16            0  ['dad with recent heart attcak']   \n",
       "1  00016_001         0      16            1     ['mom with \"thyroid disease']   \n",
       "\n",
       "      location                 annotation_list location_list  \\\n",
       "0  ['696 724']  [dad with recent heart attcak]     [696 724]   \n",
       "1  ['668 693']     [mom with \"thyroid disease]     [668 693]   \n",
       "\n",
       "                                          pn_history  \\\n",
       "0  hpi: 17yo m presents with palpitations. patien...   \n",
       "1  hpi: 17yo m presents with palpitations. patien...   \n",
       "\n",
       "                                        feature_text  \n",
       "0  family history of mi or family history of myoc...  \n",
       "1                 family history of thyroid disorder  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = prepare_data(base_config)\n",
    "train_df = obj.merge_data()\n",
    "print(train_df.info())\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_pre_process = pre_process_data(base_config)\n",
    "# train_df['location_list'] = obj_pre_process.split_loc(train_df['location_list'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (4.39.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kartheek kotha\\.conda\\envs\\nlp\\lib\\site-packages (from requests->transformers) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14300 entries, 0 to 14299\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               14300 non-null  object\n",
      " 1   case_num         14300 non-null  int64 \n",
      " 2   pn_num           14300 non-null  int64 \n",
      " 3   feature_num      14300 non-null  int64 \n",
      " 4   annotation       14300 non-null  object\n",
      " 5   location         14300 non-null  object\n",
      " 6   annotation_list  14300 non-null  object\n",
      " 7   location_list    14300 non-null  object\n",
      " 8   pn_history       14300 non-null  object\n",
      " 9   feature_text     14300 non-null  object\n",
      "dtypes: int64(3), object(7)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9064, 6406, 162, 30, 143, 2788, 47, 1017, 101, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Kartheek Kotha\\.conda\\envs\\nlp\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded_input)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(obj_pre_process\u001b[38;5;241m.\u001b[39mtokenize_and_addLabels(\u001b[43mtrain_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m , tokenizer , base_config))\n",
      "File \u001b[1;32mc:\\Users\\Kartheek Kotha\\.conda\\envs\\nlp\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Kartheek Kotha\\.conda\\envs\\nlp\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "print(encoded_input)\n",
    "print(obj_pre_process.tokenize_and_addLabels(train_df[0] , tokenizer , base_config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

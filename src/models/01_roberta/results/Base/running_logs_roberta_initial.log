[2024-04-17 11:59:15,859: INFO: utils: NumExpr defaulting to 2 threads.]
[2024-04-17 11:59:38,875: INFO: main: Epoch: 1/5]
[2024-04-17 11:59:41,334: INFO: main: Training : batch 0 Loss: 0.7598593879433161]
[2024-04-17 11:59:41,902: INFO: main: Training : batch 1 Loss: 0.7314884732555818]
[2024-04-17 11:59:42,486: INFO: main: Training : batch 2 Loss: 0.7030551998889325]
[2024-04-17 11:59:43,061: INFO: main: Training : batch 3 Loss: 0.6769025003802829]
[2024-04-17 11:59:43,639: INFO: main: Training : batch 4 Loss: 0.6456232489470023]
[2024-04-17 11:59:44,220: INFO: main: Training : batch 5 Loss: 0.6151325557913853]
[2024-04-17 11:59:44,798: INFO: main: Training : batch 6 Loss: 0.5813169219936759]
[2024-04-17 11:59:45,372: INFO: main: Training : batch 7 Loss: 0.5511448343180041]
[2024-04-17 11:59:45,944: INFO: main: Training : batch 8 Loss: 0.5055589489344763]
[2024-04-17 11:59:46,517: INFO: main: Training : batch 9 Loss: 0.4638655583471154]
[2024-04-17 11:59:47,094: INFO: main: Training : batch 10 Loss: 0.4120408280191058]
[2024-04-17 11:59:47,664: INFO: main: Training : batch 11 Loss: 0.3523013595405338]
[2024-04-17 11:59:48,239: INFO: main: Training : batch 12 Loss: 0.30366338732317394]
[2024-04-17 11:59:48,814: INFO: main: Training : batch 13 Loss: 0.26451144841544755]
[2024-04-17 11:59:49,388: INFO: main: Training : batch 14 Loss: 0.2229294394112345]
[2024-04-17 11:59:49,964: INFO: main: Training : batch 15 Loss: 0.19109396405059695]
[2024-04-17 11:59:50,541: INFO: main: Training : batch 16 Loss: 0.1783733377560383]
[2024-04-17 11:59:51,118: INFO: main: Training : batch 17 Loss: 0.1531570597319198]
[2024-04-17 11:59:51,694: INFO: main: Training : batch 18 Loss: 0.18521125646441786]
[2024-04-17 11:59:52,272: INFO: main: Training : batch 19 Loss: 0.1374482899588391]
[2024-04-17 11:59:52,848: INFO: main: Training : batch 20 Loss: 0.11286212167454186]
[2024-04-17 11:59:53,425: INFO: main: Training : batch 21 Loss: 0.11403380840278093]
[2024-04-17 11:59:54,001: INFO: main: Training : batch 22 Loss: 0.10125092752430616]
[2024-04-17 11:59:54,585: INFO: main: Training : batch 23 Loss: 0.12618468585244036]
[2024-04-17 11:59:55,161: INFO: main: Training : batch 24 Loss: 0.09441669840265768]
[2024-04-17 11:59:55,748: INFO: main: Training : batch 25 Loss: 0.07601517582962276]
[2024-04-17 11:59:56,333: INFO: main: Training : batch 26 Loss: 0.10857055296980958]
[2024-04-17 11:59:56,916: INFO: main: Training : batch 27 Loss: 0.07717947587131305]
[2024-04-17 11:59:57,508: INFO: main: Training : batch 28 Loss: 0.06085107872362979]
[2024-04-17 11:59:58,098: INFO: main: Training : batch 29 Loss: 0.07706536918912288]
[2024-04-17 11:59:58,679: INFO: main: Training : batch 30 Loss: 0.06914672222175149]
[2024-04-17 11:59:59,255: INFO: main: Training : batch 31 Loss: 0.05366390970912118]
[2024-04-17 11:59:59,840: INFO: main: Training : batch 32 Loss: 0.10056401818628029]
[2024-04-17 12:00:00,424: INFO: main: Training : batch 33 Loss: 0.07281998809335619]
[2024-04-17 12:00:01,008: INFO: main: Training : batch 34 Loss: 0.06299986499496753]
[2024-04-17 12:00:01,593: INFO: main: Training : batch 35 Loss: 0.08964883583385376]
[2024-04-17 12:00:02,182: INFO: main: Training : batch 36 Loss: 0.06384930874567368]
[2024-04-17 12:00:02,766: INFO: main: Training : batch 37 Loss: 0.08609875331329901]
[2024-04-17 12:00:03,352: INFO: main: Training : batch 38 Loss: 0.08711088643051859]
[2024-04-17 12:00:03,938: INFO: main: Training : batch 39 Loss: 0.12049428887559839]
[2024-04-17 12:00:04,528: INFO: main: Training : batch 40 Loss: 0.0799959374233549]
[2024-04-17 12:00:05,116: INFO: main: Training : batch 41 Loss: 0.07470269974533665]
[2024-04-17 12:00:05,704: INFO: main: Training : batch 42 Loss: 0.10598598428752264]
[2024-04-17 12:00:06,294: INFO: main: Training : batch 43 Loss: 0.0951536057476048]
[2024-04-17 12:00:06,878: INFO: main: Training : batch 44 Loss: 0.03794736935858174]
[2024-04-17 12:00:07,470: INFO: main: Training : batch 45 Loss: 0.08883000329150595]
[2024-04-17 12:00:08,058: INFO: main: Training : batch 46 Loss: 0.07904086486318372]
[2024-04-17 12:00:08,653: INFO: main: Training : batch 47 Loss: 0.12626629360817188]
[2024-04-17 12:00:09,249: INFO: main: Training : batch 48 Loss: 0.049480332956254224]
[2024-04-17 12:00:09,846: INFO: main: Training : batch 49 Loss: 0.04088007905221931]
[2024-04-17 12:00:10,443: INFO: main: Training : batch 50 Loss: 0.04914825844240483]
[2024-04-17 12:00:11,039: INFO: main: Training : batch 51 Loss: 0.038792286640802864]
[2024-04-17 12:00:11,631: INFO: main: Training : batch 52 Loss: 0.05652412370193218]
[2024-04-17 12:00:12,230: INFO: main: Training : batch 53 Loss: 0.1340546801854637]
[2024-04-17 12:00:12,822: INFO: main: Training : batch 54 Loss: 0.07789201582949705]
[2024-04-17 12:00:13,423: INFO: main: Training : batch 55 Loss: 0.07149073096002385]
[2024-04-17 12:00:14,010: INFO: main: Training : batch 56 Loss: 0.06570977640696614]
[2024-04-17 12:00:14,608: INFO: main: Training : batch 57 Loss: 0.09058135042621943]
[2024-04-17 12:00:15,197: INFO: main: Training : batch 58 Loss: 0.07283262805845055]
[2024-04-17 12:00:15,794: INFO: main: Training : batch 59 Loss: 0.045660494153324925]
[2024-04-17 12:00:16,383: INFO: main: Training : batch 60 Loss: 0.07301762509232855]
[2024-04-17 12:00:16,983: INFO: main: Training : batch 61 Loss: 0.02881912990004977]
[2024-04-17 12:00:17,572: INFO: main: Training : batch 62 Loss: 0.08195353790206499]
[2024-04-17 12:00:18,169: INFO: main: Training : batch 63 Loss: 0.0650228528869675]
[2024-04-17 12:00:18,760: INFO: main: Training : batch 64 Loss: 0.07992181733125184]
[2024-04-17 12:00:19,359: INFO: main: Training : batch 65 Loss: 0.08942485186182862]
[2024-04-17 12:00:19,948: INFO: main: Training : batch 66 Loss: 0.10515712819538815]
[2024-04-17 12:00:20,549: INFO: main: Training : batch 67 Loss: 0.059517862065764315]
[2024-04-17 12:00:21,142: INFO: main: Training : batch 68 Loss: 0.07558702428856917]
[2024-04-17 12:00:21,748: INFO: main: Training : batch 69 Loss: 0.051948183313444994]
[2024-04-17 12:00:22,351: INFO: main: Training : batch 70 Loss: 0.1301743713789552]
[2024-04-17 12:00:22,952: INFO: main: Training : batch 71 Loss: 0.11894273716573438]
[2024-04-17 12:00:23,564: INFO: main: Training : batch 72 Loss: 0.06607340234451871]
[2024-04-17 12:00:24,178: INFO: main: Training : batch 73 Loss: 0.0830038504806852]
[2024-04-17 12:00:24,778: INFO: main: Training : batch 74 Loss: 0.06815234477380439]
[2024-04-17 12:00:25,377: INFO: main: Training : batch 75 Loss: 0.09687003237238616]
[2024-04-17 12:00:25,985: INFO: main: Training : batch 76 Loss: 0.06146856685524828]
[2024-04-17 12:00:26,589: INFO: main: Training : batch 77 Loss: 0.1747901242141264]
[2024-04-17 12:00:27,192: INFO: main: Training : batch 78 Loss: 0.0759510615926273]
[2024-04-17 12:00:27,794: INFO: main: Training : batch 79 Loss: 0.0452541934844629]
[2024-04-17 12:00:28,398: INFO: main: Training : batch 80 Loss: 0.04044182155159625]
[2024-04-17 12:00:29,005: INFO: main: Training : batch 81 Loss: 0.07652220210574691]
[2024-04-17 12:00:29,613: INFO: main: Training : batch 82 Loss: 0.0725439274567456]
[2024-04-17 12:00:30,215: INFO: main: Training : batch 83 Loss: 0.08949921646385571]
[2024-04-17 12:00:30,819: INFO: main: Training : batch 84 Loss: 0.0694371121755145]
[2024-04-17 12:00:31,421: INFO: main: Training : batch 85 Loss: 0.060300958618767435]
[2024-04-17 12:00:32,026: INFO: main: Training : batch 86 Loss: 0.0761281809281123]
[2024-04-17 12:00:32,634: INFO: main: Training : batch 87 Loss: 0.08526149216395305]
[2024-04-17 12:00:33,240: INFO: main: Training : batch 88 Loss: 0.07074447137139363]
[2024-04-17 12:00:33,850: INFO: main: Training : batch 89 Loss: 0.09985190317613594]
[2024-04-17 12:00:34,458: INFO: main: Training : batch 90 Loss: 0.056632624500626345]
[2024-04-17 12:00:35,072: INFO: main: Training : batch 91 Loss: 0.05362681188577608]
[2024-04-17 12:00:35,690: INFO: main: Training : batch 92 Loss: 0.037722719079339495]
[2024-04-17 12:00:36,300: INFO: main: Training : batch 93 Loss: 0.058444407067382464]
[2024-04-17 12:00:36,914: INFO: main: Training : batch 94 Loss: 0.08850919641771086]
[2024-04-17 12:00:37,530: INFO: main: Training : batch 95 Loss: 0.0967923450408305]
[2024-04-17 12:00:38,145: INFO: main: Training : batch 96 Loss: 0.04519576986330722]
[2024-04-17 12:00:38,754: INFO: main: Training : batch 97 Loss: 0.16308666144588704]
[2024-04-17 12:00:39,375: INFO: main: Training : batch 98 Loss: 0.04247211120561455]
[2024-04-17 12:00:39,990: INFO: main: Training : batch 99 Loss: 0.047289404649732275]
[2024-04-17 12:00:40,601: INFO: main: Training : batch 100 Loss: 0.053407674256256435]
[2024-04-17 12:00:41,219: INFO: main: Training : batch 101 Loss: 0.04006426830742619]
[2024-04-17 12:00:41,829: INFO: main: Training : batch 102 Loss: 0.04740806985954267]
[2024-04-17 12:00:42,455: INFO: main: Training : batch 103 Loss: 0.1650792502596035]
[2024-04-17 12:00:43,070: INFO: main: Training : batch 104 Loss: 0.07266601435926363]
[2024-04-17 12:00:43,685: INFO: main: Training : batch 105 Loss: 0.07609225540879395]
[2024-04-17 12:00:44,304: INFO: main: Training : batch 106 Loss: 0.08045634724775]
[2024-04-17 12:00:44,926: INFO: main: Training : batch 107 Loss: 0.06550027171316453]
[2024-04-17 12:00:45,539: INFO: main: Training : batch 108 Loss: 0.06778158800736989]
[2024-04-17 12:00:46,156: INFO: main: Training : batch 109 Loss: 0.07913479491034074]
[2024-04-17 12:00:46,781: INFO: main: Training : batch 110 Loss: 0.05435348202842309]
[2024-04-17 12:00:47,397: INFO: main: Training : batch 111 Loss: 0.09632908726983894]
[2024-04-17 12:00:48,031: INFO: main: Training : batch 112 Loss: 0.054272646262631896]
[2024-04-17 12:00:48,660: INFO: main: Training : batch 113 Loss: 0.06938473464547369]
[2024-04-17 12:00:49,288: INFO: main: Training : batch 114 Loss: 0.04363547336381029]
[2024-04-17 12:00:49,916: INFO: main: Training : batch 115 Loss: 0.04634795059871191]
[2024-04-17 12:00:50,548: INFO: main: Training : batch 116 Loss: 0.09176345767932433]
[2024-04-17 12:00:51,176: INFO: main: Training : batch 117 Loss: 0.17553564186494708]
[2024-04-17 12:00:51,806: INFO: main: Training : batch 118 Loss: 0.05481881707157225]
[2024-04-17 12:00:52,432: INFO: main: Training : batch 119 Loss: 0.08745760660924098]
[2024-04-17 12:00:53,060: INFO: main: Training : batch 120 Loss: 0.0586222033160627]
[2024-04-17 12:00:53,689: INFO: main: Training : batch 121 Loss: 0.0669232913856363]
[2024-04-17 12:00:54,315: INFO: main: Training : batch 122 Loss: 0.08059903056903811]
[2024-04-17 12:00:54,943: INFO: main: Training : batch 123 Loss: 0.06960535057597943]
[2024-04-17 12:00:55,574: INFO: main: Training : batch 124 Loss: 0.11538004375662948]
[2024-04-17 12:00:56,202: INFO: main: Training : batch 125 Loss: 0.05856164096299384]
[2024-04-17 12:00:56,836: INFO: main: Training : batch 126 Loss: 0.06335255911533769]
[2024-04-17 12:00:57,467: INFO: main: Training : batch 127 Loss: 0.1025650085815037]
[2024-04-17 12:00:58,096: INFO: main: Training : batch 128 Loss: 0.039829598064464865]
[2024-04-17 12:00:58,731: INFO: main: Training : batch 129 Loss: 0.040242414429330596]
[2024-04-17 12:00:59,364: INFO: main: Training : batch 130 Loss: 0.06452768168924887]
[2024-04-17 12:01:00,001: INFO: main: Training : batch 131 Loss: 0.061008741215030894]
[2024-04-17 12:01:00,640: INFO: main: Training : batch 132 Loss: 0.03515439714154062]
[2024-04-17 12:01:01,285: INFO: main: Training : batch 133 Loss: 0.07368304587815973]
[2024-04-17 12:01:01,930: INFO: main: Training : batch 134 Loss: 0.10058207001616047]
[2024-04-17 12:01:02,568: INFO: main: Training : batch 135 Loss: 0.05845947916009529]
[2024-04-17 12:01:03,218: INFO: main: Training : batch 136 Loss: 0.0463851286999608]
[2024-04-17 12:01:03,865: INFO: main: Training : batch 137 Loss: 0.08089360455807798]
[2024-04-17 12:01:04,512: INFO: main: Training : batch 138 Loss: 0.042705274690789254]
[2024-04-17 12:01:05,153: INFO: main: Training : batch 139 Loss: 0.06880949358159531]
[2024-04-17 12:01:05,795: INFO: main: Training : batch 140 Loss: 0.10567203164425337]
[2024-04-17 12:01:06,433: INFO: main: Training : batch 141 Loss: 0.07571995447545966]
[2024-04-17 12:01:07,075: INFO: main: Training : batch 142 Loss: 0.03503840194760505]
[2024-04-17 12:01:07,723: INFO: main: Training : batch 143 Loss: 0.07579371394389507]
[2024-04-17 12:01:08,369: INFO: main: Training : batch 144 Loss: 0.1069195117278232]
[2024-04-17 12:01:09,019: INFO: main: Training : batch 145 Loss: 0.08266327261110706]
[2024-04-17 12:01:09,662: INFO: main: Training : batch 146 Loss: 0.07490626149858856]
[2024-04-17 12:01:10,303: INFO: main: Training : batch 147 Loss: 0.0758076801869468]
[2024-04-17 12:01:10,948: INFO: main: Training : batch 148 Loss: 0.07509372832667877]
[2024-04-17 12:01:11,596: INFO: main: Training : batch 149 Loss: 0.0963142000253779]
[2024-04-17 12:01:12,244: INFO: main: Training : batch 150 Loss: 0.04821980016767918]
[2024-04-17 12:01:12,889: INFO: main: Training : batch 151 Loss: 0.041289395316268136]
[2024-04-17 12:01:13,541: INFO: main: Training : batch 152 Loss: 0.06021925593383386]
[2024-04-17 12:01:14,191: INFO: main: Training : batch 153 Loss: 0.07679152703916509]
[2024-04-17 12:01:14,836: INFO: main: Training : batch 154 Loss: 0.10951943420696744]
[2024-04-17 12:01:15,498: INFO: main: Training : batch 155 Loss: 0.07579588745047765]
[2024-04-17 12:01:16,159: INFO: main: Training : batch 156 Loss: 0.06601634114547028]
[2024-04-17 12:01:16,811: INFO: main: Training : batch 157 Loss: 0.07815187138020327]
[2024-04-17 12:01:17,468: INFO: main: Training : batch 158 Loss: 0.03167269168931105]
[2024-04-17 12:01:18,120: INFO: main: Training : batch 159 Loss: 0.0562362519480689]
[2024-04-17 12:01:18,767: INFO: main: Training : batch 160 Loss: 0.05327779600075909]
[2024-04-17 12:01:19,417: INFO: main: Training : batch 161 Loss: 0.07507753344070932]
[2024-04-17 12:01:20,065: INFO: main: Training : batch 162 Loss: 0.05041105666376855]
[2024-04-17 12:01:20,713: INFO: main: Training : batch 163 Loss: 0.08387700707595427]
[2024-04-17 12:01:21,361: INFO: main: Training : batch 164 Loss: 0.0720761275361719]
[2024-04-17 12:01:22,005: INFO: main: Training : batch 165 Loss: 0.05765811412480965]
[2024-04-17 12:01:22,645: INFO: main: Training : batch 166 Loss: 0.06291814656314289]
[2024-04-17 12:01:23,285: INFO: main: Training : batch 167 Loss: 0.09359387564619288]
[2024-04-17 12:01:23,929: INFO: main: Training : batch 168 Loss: 0.0708423198231336]
[2024-04-17 12:01:24,573: INFO: main: Training : batch 169 Loss: 0.06626824707778471]
[2024-04-17 12:01:25,217: INFO: main: Training : batch 170 Loss: 0.07726294763720935]
[2024-04-17 12:01:25,863: INFO: main: Training : batch 171 Loss: 0.18553578397808954]
[2024-04-17 12:01:26,499: INFO: main: Training : batch 172 Loss: 0.08723872035857191]
[2024-04-17 12:01:27,138: INFO: main: Training : batch 173 Loss: 0.037084639354236865]
[2024-04-17 12:01:27,780: INFO: main: Training : batch 174 Loss: 0.11943028585946114]
[2024-04-17 12:01:28,426: INFO: main: Training : batch 175 Loss: 0.054217665436001324]
[2024-04-17 12:01:29,069: INFO: main: Training : batch 176 Loss: 0.04881188806550983]
[2024-04-17 12:01:29,716: INFO: main: Training : batch 177 Loss: 0.11898216355275337]
[2024-04-17 12:01:30,365: INFO: main: Training : batch 178 Loss: 0.04274998118949305]
[2024-04-17 12:01:30,999: INFO: main: Training : batch 179 Loss: 0.06515089653360516]
[2024-04-17 12:01:31,639: INFO: main: Training : batch 180 Loss: 0.12771457215920298]
[2024-04-17 12:01:32,278: INFO: main: Training : batch 181 Loss: 0.05921640006004837]
[2024-04-17 12:01:32,918: INFO: main: Training : batch 182 Loss: 0.10662335320513482]
[2024-04-17 12:01:33,554: INFO: main: Training : batch 183 Loss: 0.09954911142272084]
[2024-04-17 12:01:34,189: INFO: main: Training : batch 184 Loss: 0.059385402600891944]
[2024-04-17 12:01:34,820: INFO: main: Training : batch 185 Loss: 0.12005477900154453]
[2024-04-17 12:01:35,454: INFO: main: Training : batch 186 Loss: 0.14847263348884543]
[2024-04-17 12:01:36,085: INFO: main: Training : batch 187 Loss: 0.10754441503997583]
[2024-04-17 12:01:36,720: INFO: main: Training : batch 188 Loss: 0.07282112570771203]
[2024-04-17 12:01:37,347: INFO: main: Training : batch 189 Loss: 0.08087615286689494]
[2024-04-17 12:01:37,976: INFO: main: Training : batch 190 Loss: 0.0735462209236897]
[2024-04-17 12:01:38,606: INFO: main: Training : batch 191 Loss: 0.06665050844831012]
[2024-04-17 12:01:39,237: INFO: main: Training : batch 192 Loss: 0.059809816646252255]
[2024-04-17 12:01:39,867: INFO: main: Training : batch 193 Loss: 0.043468314311336326]
[2024-04-17 12:01:40,501: INFO: main: Training : batch 194 Loss: 0.055471432909321085]
[2024-04-17 12:01:41,135: INFO: main: Training : batch 195 Loss: 0.055528613614836915]
[2024-04-17 12:01:41,772: INFO: main: Training : batch 196 Loss: 0.08557940680872311]
[2024-04-17 12:01:42,399: INFO: main: Training : batch 197 Loss: 0.0762524536353994]
[2024-04-17 12:01:43,035: INFO: main: Training : batch 198 Loss: 0.15796304536446132]
[2024-04-17 12:01:43,665: INFO: main: Training : batch 199 Loss: 0.0904395518021556]
[2024-04-17 12:01:44,302: INFO: main: Training : batch 200 Loss: 0.07538682262272667]
[2024-04-17 12:01:44,931: INFO: main: Training : batch 201 Loss: 0.07182964899824397]
[2024-04-17 12:01:45,554: INFO: main: Training : batch 202 Loss: 0.048158447134926896]
[2024-04-17 12:01:46,184: INFO: main: Training : batch 203 Loss: 0.07629659495657008]
[2024-04-17 12:01:46,810: INFO: main: Training : batch 204 Loss: 0.06270960188052215]
[2024-04-17 12:01:47,440: INFO: main: Training : batch 205 Loss: 0.07592793657993521]
[2024-04-17 12:01:48,062: INFO: main: Training : batch 206 Loss: 0.05282939154785138]
[2024-04-17 12:01:48,684: INFO: main: Training : batch 207 Loss: 0.15093578530645746]
[2024-04-17 12:01:49,310: INFO: main: Training : batch 208 Loss: 0.06576857078085149]
[2024-04-17 12:01:49,932: INFO: main: Training : batch 209 Loss: 0.06817924871336897]
[2024-04-17 12:01:50,562: INFO: main: Training : batch 210 Loss: 0.08491235948540991]
[2024-04-17 12:01:51,182: INFO: main: Training : batch 211 Loss: 0.05542406472991142]
[2024-04-17 12:01:51,810: INFO: main: Training : batch 212 Loss: 0.07360122170411298]
[2024-04-17 12:01:52,435: INFO: main: Training : batch 213 Loss: 0.034493686869792047]
[2024-04-17 12:01:53,057: INFO: main: Training : batch 214 Loss: 0.10199740146403273]
[2024-04-17 12:01:53,688: INFO: main: Training : batch 215 Loss: 0.05167266430232737]
[2024-04-17 12:01:54,317: INFO: main: Training : batch 216 Loss: 0.07151443846304627]
[2024-04-17 12:01:54,944: INFO: main: Training : batch 217 Loss: 0.08991898653094055]
[2024-04-17 12:01:55,574: INFO: main: Training : batch 218 Loss: 0.10691118356355997]
[2024-04-17 12:01:56,205: INFO: main: Training : batch 219 Loss: 0.0739344809187347]
[2024-04-17 12:01:56,837: INFO: main: Training : batch 220 Loss: 0.05251682206540562]
[2024-04-17 12:01:57,459: INFO: main: Training : batch 221 Loss: 0.09443540703884237]
[2024-04-17 12:01:58,083: INFO: main: Training : batch 222 Loss: 0.05100974180472635]
[2024-04-17 12:01:58,717: INFO: main: Training : batch 223 Loss: 0.03126781424576173]
[2024-04-17 12:01:59,342: INFO: main: Training : batch 224 Loss: 0.07776121158376585]
[2024-04-17 12:01:59,973: INFO: main: Training : batch 225 Loss: 0.06174229468366874]
[2024-04-17 12:02:00,599: INFO: main: Training : batch 226 Loss: 0.058198816276544114]
[2024-04-17 12:02:01,221: INFO: main: Training : batch 227 Loss: 0.059882866088623116]
[2024-04-17 12:02:01,845: INFO: main: Training : batch 228 Loss: 0.07369020308117168]
[2024-04-17 12:02:02,473: INFO: main: Training : batch 229 Loss: 0.07958323063081288]
[2024-04-17 12:02:03,093: INFO: main: Training : batch 230 Loss: 0.09204248923115312]
[2024-04-17 12:02:03,720: INFO: main: Training : batch 231 Loss: 0.09246878923635733]
[2024-04-17 12:02:04,343: INFO: main: Training : batch 232 Loss: 0.029855122752525828]
[2024-04-17 12:02:04,966: INFO: main: Training : batch 233 Loss: 0.06746830887619373]
[2024-04-17 12:02:05,586: INFO: main: Training : batch 234 Loss: 0.11994836926327714]
[2024-04-17 12:02:06,213: INFO: main: Training : batch 235 Loss: 0.09311925466863313]
[2024-04-17 12:02:06,838: INFO: main: Training : batch 236 Loss: 0.08564303826108693]
[2024-04-17 12:02:07,468: INFO: main: Training : batch 237 Loss: 0.08643732676993054]
[2024-04-17 12:02:08,102: INFO: main: Training : batch 238 Loss: 0.05382984720582635]
[2024-04-17 12:02:08,723: INFO: main: Training : batch 239 Loss: 0.05560892122037293]
[2024-04-17 12:02:09,355: INFO: main: Training : batch 240 Loss: 0.09749076617224954]
[2024-04-17 12:02:09,982: INFO: main: Training : batch 241 Loss: 0.042516521174662654]
[2024-04-17 12:02:10,608: INFO: main: Training : batch 242 Loss: 0.06333914723251728]
[2024-04-17 12:02:11,233: INFO: main: Training : batch 243 Loss: 0.08288430448170311]
[2024-04-17 12:02:11,862: INFO: main: Training : batch 244 Loss: 0.07199882290438119]
[2024-04-17 12:02:12,485: INFO: main: Training : batch 245 Loss: 0.038121708883938954]
[2024-04-17 12:02:13,114: INFO: main: Training : batch 246 Loss: 0.07622923870864015]
[2024-04-17 12:02:13,738: INFO: main: Training : batch 247 Loss: 0.10584999736075179]
[2024-04-17 12:02:14,365: INFO: main: Training : batch 248 Loss: 0.10237537619135534]
[2024-04-17 12:02:14,992: INFO: main: Training : batch 249 Loss: 0.12925755670765654]
[2024-04-17 12:02:15,618: INFO: main: Training : batch 250 Loss: 0.09328509741393265]
[2024-04-17 12:02:16,245: INFO: main: Training : batch 251 Loss: 0.054123795694210726]
[2024-04-17 12:02:16,872: INFO: main: Training : batch 252 Loss: 0.09668519024754647]
[2024-04-17 12:02:17,499: INFO: main: Training : batch 253 Loss: 0.057237363514774625]
[2024-04-17 12:02:18,130: INFO: main: Training : batch 254 Loss: 0.04630191410775426]
[2024-04-17 12:02:18,758: INFO: main: Training : batch 255 Loss: 0.06747396408474811]
[2024-04-17 12:02:19,386: INFO: main: Training : batch 256 Loss: 0.051633009700700304]
[2024-04-17 12:02:20,013: INFO: main: Training : batch 257 Loss: 0.13235300268689149]
[2024-04-17 12:02:20,648: INFO: main: Training : batch 258 Loss: 0.11287460615398189]
[2024-04-17 12:02:21,281: INFO: main: Training : batch 259 Loss: 0.08882087270476627]
[2024-04-17 12:02:21,916: INFO: main: Training : batch 260 Loss: 0.10046520970105656]
[2024-04-17 12:02:22,554: INFO: main: Training : batch 261 Loss: 0.07773596528423445]
[2024-04-17 12:02:23,195: INFO: main: Training : batch 262 Loss: 0.09066438099055099]
[2024-04-17 12:02:23,826: INFO: main: Training : batch 263 Loss: 0.05405976509537873]
[2024-04-17 12:02:24,467: INFO: main: Training : batch 264 Loss: 0.04204251761680511]
[2024-04-17 12:02:25,102: INFO: main: Training : batch 265 Loss: 0.0679918764663846]
[2024-04-17 12:02:25,736: INFO: main: Training : batch 266 Loss: 0.0908181508647741]
[2024-04-17 12:02:26,362: INFO: main: Training : batch 267 Loss: 0.07787010274514987]
[2024-04-17 12:02:26,996: INFO: main: Training : batch 268 Loss: 0.05792322079569553]
[2024-04-17 12:02:27,621: INFO: main: Training : batch 269 Loss: 0.08301204524638285]
[2024-04-17 12:02:28,255: INFO: main: Training : batch 270 Loss: 0.08168016313138998]
[2024-04-17 12:02:28,889: INFO: main: Training : batch 271 Loss: 0.1290397875042688]
[2024-04-17 12:02:29,522: INFO: main: Training : batch 272 Loss: 0.0447936419714994]
[2024-04-17 12:02:30,152: INFO: main: Training : batch 273 Loss: 0.043684460728530654]
[2024-04-17 12:02:30,784: INFO: main: Training : batch 274 Loss: 0.07982920641136415]
[2024-04-17 12:02:31,417: INFO: main: Training : batch 275 Loss: 0.07947575761003174]
[2024-04-17 12:02:32,052: INFO: main: Training : batch 276 Loss: 0.07765911282056508]
[2024-04-17 12:02:32,683: INFO: main: Training : batch 277 Loss: 0.056025787537396815]
[2024-04-17 12:02:33,322: INFO: main: Training : batch 278 Loss: 0.05899992445230291]
[2024-04-17 12:02:33,971: INFO: main: Training : batch 279 Loss: 0.0677192890602565]
[2024-04-17 12:02:34,612: INFO: main: Training : batch 280 Loss: 0.0775565193028895]
[2024-04-17 12:02:35,254: INFO: main: Training : batch 281 Loss: 0.13121448332613647]
[2024-04-17 12:02:35,890: INFO: main: Training : batch 282 Loss: 0.042825112453050464]
[2024-04-17 12:02:36,526: INFO: main: Training : batch 283 Loss: 0.06813205592724053]
[2024-04-17 12:02:37,158: INFO: main: Training : batch 284 Loss: 0.0710672990167777]
[2024-04-17 12:02:37,793: INFO: main: Training : batch 285 Loss: 0.07185446161144335]
[2024-04-17 12:02:38,423: INFO: main: Training : batch 286 Loss: 0.08589185910751419]
[2024-04-17 12:02:39,055: INFO: main: Training : batch 287 Loss: 0.09783880749600173]
[2024-04-17 12:02:39,687: INFO: main: Training : batch 288 Loss: 0.029358674724016428]
[2024-04-17 12:02:40,320: INFO: main: Training : batch 289 Loss: 0.05311719895823181]
[2024-04-17 12:02:40,948: INFO: main: Training : batch 290 Loss: 0.08963078498437634]
[2024-04-17 12:02:41,585: INFO: main: Training : batch 291 Loss: 0.07474914890459662]
[2024-04-17 12:02:42,224: INFO: main: Training : batch 292 Loss: 0.049873816285025865]
[2024-04-17 12:02:42,858: INFO: main: Training : batch 293 Loss: 0.06436742652289422]
[2024-04-17 12:02:43,490: INFO: main: Training : batch 294 Loss: 0.04432601050888764]
[2024-04-17 12:02:44,133: INFO: main: Training : batch 295 Loss: 0.06886541658279194]
[2024-04-17 12:02:44,770: INFO: main: Training : batch 296 Loss: 0.04614888832057542]
[2024-04-17 12:02:45,403: INFO: main: Training : batch 297 Loss: 0.06202134476964372]
[2024-04-17 12:02:46,037: INFO: main: Training : batch 298 Loss: 0.03207190596454859]
[2024-04-17 12:02:46,672: INFO: main: Training : batch 299 Loss: 0.0581036316738755]
[2024-04-17 12:02:47,315: INFO: main: Training : batch 300 Loss: 0.09450911093599851]
[2024-04-17 12:02:47,953: INFO: main: Training : batch 301 Loss: 0.04220576454913096]
[2024-04-17 12:02:48,596: INFO: main: Training : batch 302 Loss: 0.05725724395795628]
[2024-04-17 12:02:49,240: INFO: main: Training : batch 303 Loss: 0.05708832672486702]
[2024-04-17 12:02:49,876: INFO: main: Training : batch 304 Loss: 0.05668902457528141]
[2024-04-17 12:02:50,513: INFO: main: Training : batch 305 Loss: 0.036012289823528276]
[2024-04-17 12:02:51,148: INFO: main: Training : batch 306 Loss: 0.03344317801663612]
[2024-04-17 12:02:51,784: INFO: main: Training : batch 307 Loss: 0.03981554278173623]
[2024-04-17 12:02:52,418: INFO: main: Training : batch 308 Loss: 0.06401173646288875]
[2024-04-17 12:02:53,050: INFO: main: Training : batch 309 Loss: 0.02799275262009944]
[2024-04-17 12:02:53,685: INFO: main: Training : batch 310 Loss: 0.02293664884074811]
[2024-04-17 12:02:54,323: INFO: main: Training : batch 311 Loss: 0.047978819199727164]
[2024-04-17 12:02:54,954: INFO: main: Training : batch 312 Loss: 0.03430631066903562]
[2024-04-17 12:02:55,587: INFO: main: Training : batch 313 Loss: 0.04022013926668814]
[2024-04-17 12:02:56,225: INFO: main: Training : batch 314 Loss: 0.04363386997248341]
[2024-04-17 12:02:56,858: INFO: main: Training : batch 315 Loss: 0.0570152122765544]
[2024-04-17 12:02:57,496: INFO: main: Training : batch 316 Loss: 0.10821872641587901]
[2024-04-17 12:02:58,128: INFO: main: Training : batch 317 Loss: 0.019480867031505707]
[2024-04-17 12:02:58,759: INFO: main: Training : batch 318 Loss: 0.03336448379045732]
[2024-04-17 12:02:59,389: INFO: main: Training : batch 319 Loss: 0.05823125114297439]
[2024-04-17 12:03:00,026: INFO: main: Training : batch 320 Loss: 0.01126678563794973]
[2024-04-17 12:03:00,662: INFO: main: Training : batch 321 Loss: 0.01412661521794184]
[2024-04-17 12:03:01,304: INFO: main: Training : batch 322 Loss: 0.08192221076816268]
[2024-04-17 12:03:01,943: INFO: main: Training : batch 323 Loss: 0.03730655366230572]
[2024-04-17 12:03:02,574: INFO: main: Training : batch 324 Loss: 0.03546496879043191]
[2024-04-17 12:03:03,209: INFO: main: Training : batch 325 Loss: 0.05867781700787648]
[2024-04-17 12:03:03,841: INFO: main: Training : batch 326 Loss: 0.03891013050845485]
[2024-04-17 12:03:04,474: INFO: main: Training : batch 327 Loss: 0.024239225527096057]
[2024-04-17 12:03:05,107: INFO: main: Training : batch 328 Loss: 0.0454173876968618]
[2024-04-17 12:03:05,739: INFO: main: Training : batch 329 Loss: 0.0402188892267006]
[2024-04-17 12:03:06,374: INFO: main: Training : batch 330 Loss: 0.036732899014045]
[2024-04-17 12:03:07,006: INFO: main: Training : batch 331 Loss: 0.03252238267178818]
[2024-04-17 12:03:07,638: INFO: main: Training : batch 332 Loss: 0.039840368356055075]
[2024-04-17 12:03:08,266: INFO: main: Training : batch 333 Loss: 0.018833187232021844]
[2024-04-17 12:03:08,904: INFO: main: Training : batch 334 Loss: 0.04650252032548511]
[2024-04-17 12:03:09,539: INFO: main: Training : batch 335 Loss: 0.013864022350031315]
[2024-04-17 12:03:10,173: INFO: main: Training : batch 336 Loss: 0.03364190353248578]
[2024-04-17 12:03:10,802: INFO: main: Training : batch 337 Loss: 0.06094786622423467]
[2024-04-17 12:03:11,434: INFO: main: Training : batch 338 Loss: 0.060645061003110776]
[2024-04-17 12:03:12,064: INFO: main: Training : batch 339 Loss: 0.02001367573991317]
[2024-04-17 12:03:12,705: INFO: main: Training : batch 340 Loss: 0.03127345179309154]
[2024-04-17 12:03:13,342: INFO: main: Training : batch 341 Loss: 0.04826227852021465]
[2024-04-17 12:03:13,977: INFO: main: Training : batch 342 Loss: 0.03231740305843998]
[2024-04-17 12:03:14,616: INFO: main: Training : batch 343 Loss: 0.027502348843225503]
[2024-04-17 12:03:15,259: INFO: main: Training : batch 344 Loss: 0.12765307129227882]
[2024-04-17 12:03:15,892: INFO: main: Training : batch 345 Loss: 0.030740304321760063]
[2024-04-17 12:03:16,524: INFO: main: Training : batch 346 Loss: 0.018295582373162894]
[2024-04-17 12:03:17,156: INFO: main: Training : batch 347 Loss: 0.047095351375766634]
[2024-04-17 12:03:17,788: INFO: main: Training : batch 348 Loss: 0.10550058861476679]
[2024-04-17 12:03:18,418: INFO: main: Training : batch 349 Loss: 0.021146462285437605]
[2024-04-17 12:03:19,047: INFO: main: Training : batch 350 Loss: 0.02952170376237914]
[2024-04-17 12:03:19,677: INFO: main: Training : batch 351 Loss: 0.03008582946509852]
[2024-04-17 12:03:20,306: INFO: main: Training : batch 352 Loss: 0.014696552585043387]
[2024-04-17 12:03:20,931: INFO: main: Training : batch 353 Loss: 0.07028760585085096]
[2024-04-17 12:03:21,563: INFO: main: Training : batch 354 Loss: 0.02391614914295127]
[2024-04-17 12:03:22,196: INFO: main: Training : batch 355 Loss: 0.025643868592329793]
[2024-04-17 12:03:22,828: INFO: main: Training : batch 356 Loss: 0.05055776588114452]
[2024-04-17 12:03:23,458: INFO: main: Training : batch 357 Loss: 0.02417554983605022]
[2024-04-17 12:03:24,088: INFO: main: Training : batch 358 Loss: 0.04810492249808067]
[2024-04-17 12:03:24,720: INFO: main: Training : batch 359 Loss: 0.035633365870323626]
[2024-04-17 12:03:25,349: INFO: main: Training : batch 360 Loss: 0.03340270816580211]
[2024-04-17 12:03:25,984: INFO: main: Training : batch 361 Loss: 0.06779437326759268]
[2024-04-17 12:03:26,619: INFO: main: Training : batch 362 Loss: 0.043544848361272356]
[2024-04-17 12:03:27,255: INFO: main: Training : batch 363 Loss: 0.0366973007125434]
[2024-04-17 12:03:27,896: INFO: main: Training : batch 364 Loss: 0.019948458745211735]
[2024-04-17 12:03:28,537: INFO: main: Training : batch 365 Loss: 0.029897361919854596]
[2024-04-17 12:03:29,170: INFO: main: Training : batch 366 Loss: 0.029949907501904318]
[2024-04-17 12:03:29,798: INFO: main: Training : batch 367 Loss: 0.03657589379387597]
[2024-04-17 12:03:30,427: INFO: main: Training : batch 368 Loss: 0.04139413410425507]
[2024-04-17 12:03:31,054: INFO: main: Training : batch 369 Loss: 0.0169282277923696]
[2024-04-17 12:03:31,684: INFO: main: Training : batch 370 Loss: 0.03749309592395567]
[2024-04-17 12:03:32,314: INFO: main: Training : batch 371 Loss: 0.06006592693720203]
[2024-04-17 12:03:32,946: INFO: main: Training : batch 372 Loss: 0.01181298094939558]
[2024-04-17 12:03:33,575: INFO: main: Training : batch 373 Loss: 0.03280282394868702]
[2024-04-17 12:03:34,204: INFO: main: Training : batch 374 Loss: 0.03699715668416811]
[2024-04-17 12:03:34,836: INFO: main: Training : batch 375 Loss: 0.016338655537273834]
[2024-04-17 12:03:35,462: INFO: main: Training : batch 376 Loss: 0.04440590436501583]
[2024-04-17 12:03:36,091: INFO: main: Training : batch 377 Loss: 0.020608320961595494]
[2024-04-17 12:03:36,719: INFO: main: Training : batch 378 Loss: 0.05888702016074381]
[2024-04-17 12:03:37,352: INFO: main: Training : batch 379 Loss: 0.04772186519675112]
[2024-04-17 12:03:37,980: INFO: main: Training : batch 380 Loss: 0.027570720626068176]
[2024-04-17 12:03:38,617: INFO: main: Training : batch 381 Loss: 0.030660737765539754]
[2024-04-17 12:03:39,257: INFO: main: Training : batch 382 Loss: 0.050090151214175684]
[2024-04-17 12:03:39,890: INFO: main: Training : batch 383 Loss: 0.013282147004070132]
[2024-04-17 12:03:40,528: INFO: main: Training : batch 384 Loss: 0.019034977719533663]
[2024-04-17 12:03:41,167: INFO: main: Training : batch 385 Loss: 0.019282926922881128]
[2024-04-17 12:03:41,805: INFO: main: Training : batch 386 Loss: 0.0872025965644623]
[2024-04-17 12:03:42,433: INFO: main: Training : batch 387 Loss: 0.09399356397456793]
[2024-04-17 12:03:43,066: INFO: main: Training : batch 388 Loss: 0.016036486979130543]
[2024-04-17 12:03:43,698: INFO: main: Training : batch 389 Loss: 0.03625147416884777]
[2024-04-17 12:03:44,324: INFO: main: Training : batch 390 Loss: 0.029435368261936573]
[2024-04-17 12:03:44,951: INFO: main: Training : batch 391 Loss: 0.03465061634451582]
[2024-04-17 12:03:45,579: INFO: main: Training : batch 392 Loss: 0.0214582515041019]
[2024-04-17 12:03:46,207: INFO: main: Training : batch 393 Loss: 0.01837964784800609]
[2024-04-17 12:03:46,840: INFO: main: Training : batch 394 Loss: 0.02238598169571129]
[2024-04-17 12:03:47,464: INFO: main: Training : batch 395 Loss: 0.023996849398699612]
[2024-04-17 12:03:48,096: INFO: main: Training : batch 396 Loss: 0.06432256234797906]
[2024-04-17 12:03:48,725: INFO: main: Training : batch 397 Loss: 0.02084674289013686]
[2024-04-17 12:03:49,359: INFO: main: Training : batch 398 Loss: 0.019655458789565077]
[2024-04-17 12:03:49,987: INFO: main: Training : batch 399 Loss: 0.02437152372189428]
[2024-04-17 12:03:50,619: INFO: main: Training : batch 400 Loss: 0.01191888124573346]
[2024-04-17 12:03:51,251: INFO: main: Training : batch 401 Loss: 0.06025586264045217]
[2024-04-17 12:03:51,877: INFO: main: Training : batch 402 Loss: 0.01851793749303772]
[2024-04-17 12:03:52,523: INFO: main: Training : batch 403 Loss: 0.023496637995246213]
[2024-04-17 12:03:53,163: INFO: main: Training : batch 404 Loss: 0.06834321191720442]
[2024-04-17 12:03:53,798: INFO: main: Training : batch 405 Loss: 0.015531033587711214]
[2024-04-17 12:03:54,442: INFO: main: Training : batch 406 Loss: 0.11857784121681075]
[2024-04-17 12:03:55,073: INFO: main: Training : batch 407 Loss: 0.08225986088286726]
[2024-04-17 12:03:55,703: INFO: main: Training : batch 408 Loss: 0.014433895742079313]
[2024-04-17 12:03:56,340: INFO: main: Training : batch 409 Loss: 0.05126162988313818]
[2024-04-17 12:03:56,973: INFO: main: Training : batch 410 Loss: 0.0449637615167638]
[2024-04-17 12:03:57,600: INFO: main: Training : batch 411 Loss: 0.03348687603603422]
[2024-04-17 12:03:58,231: INFO: main: Training : batch 412 Loss: 0.035000727745603144]
[2024-04-17 12:03:58,863: INFO: main: Training : batch 413 Loss: 0.03953921258924476]
[2024-04-17 12:03:59,493: INFO: main: Training : batch 414 Loss: 0.02235682591836013]
[2024-04-17 12:04:00,128: INFO: main: Training : batch 415 Loss: 0.01699852926561034]
[2024-04-17 12:04:00,757: INFO: main: Training : batch 416 Loss: 0.01829098055490282]
[2024-04-17 12:04:01,389: INFO: main: Training : batch 417 Loss: 0.033480917032262275]
[2024-04-17 12:04:02,020: INFO: main: Training : batch 418 Loss: 0.029269605071021942]
[2024-04-17 12:04:02,653: INFO: main: Training : batch 419 Loss: 0.0380495392575476]
[2024-04-17 12:04:03,278: INFO: main: Training : batch 420 Loss: 0.009505676886199037]
[2024-04-17 12:04:03,907: INFO: main: Training : batch 421 Loss: 0.0386328241723847]
[2024-04-17 12:04:04,536: INFO: main: Training : batch 422 Loss: 0.04078375348064264]
[2024-04-17 12:04:05,171: INFO: main: Training : batch 423 Loss: 0.01604203070368453]
[2024-04-17 12:04:05,812: INFO: main: Training : batch 424 Loss: 0.017248899321499863]
[2024-04-17 12:04:06,445: INFO: main: Training : batch 425 Loss: 0.05903749494789978]
[2024-04-17 12:04:07,089: INFO: main: Training : batch 426 Loss: 0.0696281679633466]
[2024-04-17 12:04:07,725: INFO: main: Training : batch 427 Loss: 0.04763739094736341]
[2024-04-17 12:04:08,354: INFO: main: Training : batch 428 Loss: 0.03755862273956324]
[2024-04-17 12:04:08,986: INFO: main: Training : batch 429 Loss: 0.02259768066494305]
[2024-04-17 12:04:09,611: INFO: main: Training : batch 430 Loss: 0.10337400203089278]
[2024-04-17 12:04:10,238: INFO: main: Training : batch 431 Loss: 0.062373790270322964]
[2024-04-17 12:04:10,867: INFO: main: Training : batch 432 Loss: 0.01393968897104275]
[2024-04-17 12:04:11,497: INFO: main: Training : batch 433 Loss: 0.02998834116872958]
[2024-04-17 12:04:12,128: INFO: main: Training : batch 434 Loss: 0.02059634682163405]
[2024-04-17 12:04:12,759: INFO: main: Training : batch 435 Loss: 0.03457044713886062]
[2024-04-17 12:04:13,388: INFO: main: Training : batch 436 Loss: 0.06233984528019128]
[2024-04-17 12:04:14,017: INFO: main: Training : batch 437 Loss: 0.061353031827061984]
[2024-04-17 12:04:14,650: INFO: main: Training : batch 438 Loss: 0.03037315900545584]
[2024-04-17 12:04:15,282: INFO: main: Training : batch 439 Loss: 0.012727530368672782]
[2024-04-17 12:04:15,917: INFO: main: Training : batch 440 Loss: 0.04063362910759036]
[2024-04-17 12:04:16,550: INFO: main: Training : batch 441 Loss: 0.04827751522870203]
[2024-04-17 12:04:17,183: INFO: main: Training : batch 442 Loss: 0.03276025477332198]
[2024-04-17 12:04:17,812: INFO: main: Training : batch 443 Loss: 0.008018532983422418]
[2024-04-17 12:04:18,452: INFO: main: Training : batch 444 Loss: 0.030756922881205982]
[2024-04-17 12:04:19,087: INFO: main: Training : batch 445 Loss: 0.021101409876572656]
[2024-04-17 12:04:19,720: INFO: main: Training : batch 446 Loss: 0.02861289242080918]
[2024-04-17 12:04:20,360: INFO: main: Training : batch 447 Loss: 0.057396661243796374]
[2024-04-17 12:04:20,995: INFO: main: Training : batch 448 Loss: 0.057440501414663794]
[2024-04-17 12:04:21,627: INFO: main: Training : batch 449 Loss: 0.014965758418597189]
[2024-04-17 12:04:22,259: INFO: main: Training : batch 450 Loss: 0.030376618728810915]
[2024-04-17 12:04:22,889: INFO: main: Training : batch 451 Loss: 0.02792459292666647]
[2024-04-17 12:04:23,527: INFO: main: Training : batch 452 Loss: 0.03765115301173099]
[2024-04-17 12:04:24,157: INFO: main: Training : batch 453 Loss: 0.0395477500860218]
[2024-04-17 12:04:24,790: INFO: main: Training : batch 454 Loss: 0.01459944333188511]
[2024-04-17 12:04:25,422: INFO: main: Training : batch 455 Loss: 0.06771695959539484]
[2024-04-17 12:04:26,054: INFO: main: Training : batch 456 Loss: 0.02954573241067228]
[2024-04-17 12:04:26,684: INFO: main: Training : batch 457 Loss: 0.07836716963892855]
[2024-04-17 12:04:27,317: INFO: main: Training : batch 458 Loss: 0.06763041130936404]
[2024-04-17 12:04:27,950: INFO: main: Training : batch 459 Loss: 0.039493523879697984]
[2024-04-17 12:04:28,581: INFO: main: Training : batch 460 Loss: 0.053665938289896215]
[2024-04-17 12:04:29,212: INFO: main: Training : batch 461 Loss: 0.020028833804237152]
[2024-04-17 12:04:29,844: INFO: main: Training : batch 462 Loss: 0.04095025857584863]
[2024-04-17 12:04:30,472: INFO: main: Training : batch 463 Loss: 0.05400724377228265]
[2024-04-17 12:04:31,104: INFO: main: Training : batch 464 Loss: 0.013453008896780784]
[2024-04-17 12:04:31,744: INFO: main: Training : batch 465 Loss: 0.04395491067759369]
[2024-04-17 12:04:32,382: INFO: main: Training : batch 466 Loss: 0.014432894051415582]
[2024-04-17 12:04:33,023: INFO: main: Training : batch 467 Loss: 0.009331515125813894]
[2024-04-17 12:04:33,656: INFO: main: Training : batch 468 Loss: 0.023736859331407383]
[2024-04-17 12:04:34,299: INFO: main: Training : batch 469 Loss: 0.055992876542332586]
[2024-04-17 12:04:34,934: INFO: main: Training : batch 470 Loss: 0.023645349191206304]
[2024-04-17 12:04:35,564: INFO: main: Training : batch 471 Loss: 0.02069768530270436]
[2024-04-17 12:04:36,196: INFO: main: Training : batch 472 Loss: 0.022549667663582174]
[2024-04-17 12:04:36,827: INFO: main: Training : batch 473 Loss: 0.037668114733916715]
[2024-04-17 12:04:37,459: INFO: main: Training : batch 474 Loss: 0.03990932049599439]
[2024-04-17 12:04:38,090: INFO: main: Training : batch 475 Loss: 0.005733861573424586]
[2024-04-17 12:04:38,720: INFO: main: Training : batch 476 Loss: 0.01627032175083041]
[2024-04-17 12:04:39,354: INFO: main: Training : batch 477 Loss: 0.024550314596112385]
[2024-04-17 12:04:39,984: INFO: main: Training : batch 478 Loss: 0.03702981207212807]
[2024-04-17 12:04:40,616: INFO: main: Training : batch 479 Loss: 0.02278232872324964]
[2024-04-17 12:04:41,249: INFO: main: Training : batch 480 Loss: 0.02086272036143308]
[2024-04-17 12:04:41,880: INFO: main: Training : batch 481 Loss: 0.057373410909236866]
[2024-04-17 12:04:42,515: INFO: main: Training : batch 482 Loss: 0.013213116122917258]
[2024-04-17 12:04:43,149: INFO: main: Training : batch 483 Loss: 0.021281910336395114]
[2024-04-17 12:04:43,781: INFO: main: Training : batch 484 Loss: 0.018468993405834425]
[2024-04-17 12:04:44,415: INFO: main: Training : batch 485 Loss: 0.037375342127152614]
[2024-04-17 12:04:45,048: INFO: main: Training : batch 486 Loss: 0.01836120865839063]
[2024-04-17 12:04:45,683: INFO: main: Training : batch 487 Loss: 0.02154093633932797]
[2024-04-17 12:04:46,318: INFO: main: Training : batch 488 Loss: 0.009863709133330309]
[2024-04-17 12:04:46,956: INFO: main: Training : batch 489 Loss: 0.020589399186391206]
[2024-04-17 12:04:47,590: INFO: main: Training : batch 490 Loss: 0.025862037949886986]
[2024-04-17 12:04:48,223: INFO: main: Training : batch 491 Loss: 0.016959899523687087]
[2024-04-17 12:04:48,857: INFO: main: Training : batch 492 Loss: 0.01837395038088219]
[2024-04-17 12:04:49,496: INFO: main: Training : batch 493 Loss: 0.019144831800264585]
[2024-04-17 12:04:50,129: INFO: main: Training : batch 494 Loss: 0.02609189596492797]
[2024-04-17 12:04:50,760: INFO: main: Training : batch 495 Loss: 0.03438204328757242]
[2024-04-17 12:04:51,392: INFO: main: Training : batch 496 Loss: 0.02339761292835489]
[2024-04-17 12:04:52,022: INFO: main: Training : batch 497 Loss: 0.01926887466299911]
[2024-04-17 12:04:52,656: INFO: main: Training : batch 498 Loss: 0.008146036393838783]
[2024-04-17 12:04:53,290: INFO: main: Training : batch 499 Loss: 0.0438973141399984]
[2024-04-17 12:04:53,918: INFO: main: Training : batch 500 Loss: 0.061109778835471654]
[2024-04-17 12:04:54,548: INFO: main: Training : batch 501 Loss: 0.04872909653084832]
[2024-04-17 12:04:55,178: INFO: main: Training : batch 502 Loss: 0.03583693104326327]
[2024-04-17 12:04:55,814: INFO: main: Training : batch 503 Loss: 0.051608584421113914]
[2024-04-17 12:04:56,444: INFO: main: Training : batch 504 Loss: 0.02487647433614723]
[2024-04-17 12:04:57,077: INFO: main: Training : batch 505 Loss: 0.012845173212827656]
[2024-04-17 12:04:57,710: INFO: main: Training : batch 506 Loss: 0.015687991131633907]
[2024-04-17 12:04:58,354: INFO: main: Training : batch 507 Loss: 0.01738967682228333]
[2024-04-17 12:04:58,993: INFO: main: Training : batch 508 Loss: 0.029134866468511045]
[2024-04-17 12:04:59,635: INFO: main: Training : batch 509 Loss: 0.01309615750659584]
[2024-04-17 12:05:00,283: INFO: main: Training : batch 510 Loss: 0.02241271780272502]
[2024-04-17 12:05:00,918: INFO: main: Training : batch 511 Loss: 0.015407381446618261]
[2024-04-17 12:05:01,548: INFO: main: Training : batch 512 Loss: 0.042990544370925136]
[2024-04-17 12:05:02,181: INFO: main: Training : batch 513 Loss: 0.025967859879370368]
[2024-04-17 12:05:02,810: INFO: main: Training : batch 514 Loss: 0.02598544153643788]
[2024-04-17 12:05:03,437: INFO: main: Training : batch 515 Loss: 0.011778501508113581]
[2024-04-17 12:05:04,070: INFO: main: Training : batch 516 Loss: 0.018177739321017496]
[2024-04-17 12:05:04,701: INFO: main: Training : batch 517 Loss: 0.04386839954421536]
[2024-04-17 12:05:05,333: INFO: main: Training : batch 518 Loss: 0.0049924370953993505]
[2024-04-17 12:05:05,967: INFO: main: Training : batch 519 Loss: 0.01597679392486065]
[2024-04-17 12:05:06,602: INFO: main: Training : batch 520 Loss: 0.029584846783258525]
[2024-04-17 12:05:07,234: INFO: main: Training : batch 521 Loss: 0.03907063026696774]
[2024-04-17 12:05:07,864: INFO: main: Training : batch 522 Loss: 0.024436329209524814]
[2024-04-17 12:05:08,494: INFO: main: Training : batch 523 Loss: 0.019491542442326013]
[2024-04-17 12:05:09,125: INFO: main: Training : batch 524 Loss: 0.03840957726684784]
[2024-04-17 12:05:09,756: INFO: main: Training : batch 525 Loss: 0.042978093273884244]
[2024-04-17 12:05:10,390: INFO: main: Training : batch 526 Loss: 0.010152736608340854]
[2024-04-17 12:05:11,022: INFO: main: Training : batch 527 Loss: 0.022658329551171443]
[2024-04-17 12:05:11,660: INFO: main: Training : batch 528 Loss: 0.033439048032693264]
[2024-04-17 12:05:12,300: INFO: main: Training : batch 529 Loss: 0.07113765628266969]
[2024-04-17 12:05:12,936: INFO: main: Training : batch 530 Loss: 0.020797587084334424]
[2024-04-17 12:05:13,575: INFO: main: Training : batch 531 Loss: 0.032725517417525954]
[2024-04-17 12:05:14,210: INFO: main: Training : batch 532 Loss: 0.010362269492893572]
[2024-04-17 12:05:14,838: INFO: main: Training : batch 533 Loss: 0.021075018498679134]
[2024-04-17 12:05:15,468: INFO: main: Training : batch 534 Loss: 0.009010338357589638]
[2024-04-17 12:05:16,099: INFO: main: Training : batch 535 Loss: 0.033947641977385784]
[2024-04-17 12:05:16,729: INFO: main: Training : batch 536 Loss: 0.017549220544852407]
[2024-04-17 12:05:17,360: INFO: main: Training : batch 537 Loss: 0.04651801787623852]
[2024-04-17 12:05:17,989: INFO: main: Training : batch 538 Loss: 0.03899225199753742]
[2024-04-17 12:05:18,615: INFO: main: Training : batch 539 Loss: 0.04180739612168686]
[2024-04-17 12:05:19,247: INFO: main: Training : batch 540 Loss: 0.042399576693132664]
[2024-04-17 12:05:19,876: INFO: main: Training : batch 541 Loss: 0.04260951944574318]
[2024-04-17 12:05:20,505: INFO: main: Training : batch 542 Loss: 0.01764346956913618]
[2024-04-17 12:05:21,140: INFO: main: Training : batch 543 Loss: 0.015642003240525334]
[2024-04-17 12:05:21,774: INFO: main: Training : batch 544 Loss: 0.01330942056168568]
[2024-04-17 12:05:22,404: INFO: main: Training : batch 545 Loss: 0.015733963291472374]
[2024-04-17 12:05:23,034: INFO: main: Training : batch 546 Loss: 0.020760544062404036]
[2024-04-17 12:05:23,666: INFO: main: Training : batch 547 Loss: 0.022652147638645464]
[2024-04-17 12:05:24,301: INFO: main: Training : batch 548 Loss: 0.04549234606831379]
[2024-04-17 12:05:24,939: INFO: main: Training : batch 549 Loss: 0.039603744171472506]
[2024-04-17 12:05:25,575: INFO: main: Training : batch 550 Loss: 0.04630017302653433]
[2024-04-17 12:05:26,211: INFO: main: Training : batch 551 Loss: 0.032986389029282366]
[2024-04-17 12:05:26,847: INFO: main: Training : batch 552 Loss: 0.027522769752262023]
[2024-04-17 12:05:27,479: INFO: main: Training : batch 553 Loss: 0.018959156316286172]
[2024-04-17 12:05:28,111: INFO: main: Training : batch 554 Loss: 0.021300962723819487]
[2024-04-17 12:05:28,745: INFO: main: Training : batch 555 Loss: 0.02371288737755024]
[2024-04-17 12:05:29,377: INFO: main: Training : batch 556 Loss: 0.022538665488334872]
[2024-04-17 12:05:30,007: INFO: main: Training : batch 557 Loss: 0.07789682030640738]
[2024-04-17 12:05:30,638: INFO: main: Training : batch 558 Loss: 0.01787581529848963]
[2024-04-17 12:05:31,267: INFO: main: Training : batch 559 Loss: 0.02431910968289257]
[2024-04-17 12:05:31,901: INFO: main: Training : batch 560 Loss: 0.034283795280801145]
[2024-04-17 12:05:32,536: INFO: main: Training : batch 561 Loss: 0.030825477616087275]
[2024-04-17 12:05:33,166: INFO: main: Training : batch 562 Loss: 0.03323073782828934]
[2024-04-17 12:05:33,798: INFO: main: Training : batch 563 Loss: 0.013133821061199908]
[2024-04-17 12:05:34,425: INFO: main: Training : batch 564 Loss: 0.020290119925521932]
[2024-04-17 12:05:35,056: INFO: main: Training : batch 565 Loss: 0.025444588268641172]
[2024-04-17 12:05:35,685: INFO: main: Training : batch 566 Loss: 0.011367671753017632]
[2024-04-17 12:05:36,317: INFO: main: Training : batch 567 Loss: 0.03746342864515866]
[2024-04-17 12:05:36,966: INFO: main: Training : batch 568 Loss: 0.01781474875773848]
[2024-04-17 12:05:37,607: INFO: main: Training : batch 569 Loss: 0.0057681692117946114]
[2024-04-17 12:05:38,246: INFO: main: Training : batch 570 Loss: 0.026696877497135615]
[2024-04-17 12:05:38,885: INFO: main: Training : batch 571 Loss: 0.03216549264621706]
[2024-04-17 12:05:39,521: INFO: main: Training : batch 572 Loss: 0.028448220622317957]
[2024-04-17 12:05:40,155: INFO: main: Training : batch 573 Loss: 0.05457809304636274]
[2024-04-17 12:05:40,782: INFO: main: Training : batch 574 Loss: 0.017371226547342283]
[2024-04-17 12:05:41,417: INFO: main: Training : batch 575 Loss: 0.021883700903673028]
[2024-04-17 12:05:42,047: INFO: main: Training : batch 576 Loss: 0.0426978177854607]
[2024-04-17 12:05:42,679: INFO: main: Training : batch 577 Loss: 0.01356551482388367]
[2024-04-17 12:05:43,313: INFO: main: Training : batch 578 Loss: 0.01064402063880836]
[2024-04-17 12:05:43,942: INFO: main: Training : batch 579 Loss: 0.02217404126845808]
[2024-04-17 12:05:44,577: INFO: main: Training : batch 580 Loss: 0.029718862917779684]
[2024-04-17 12:05:45,208: INFO: main: Training : batch 581 Loss: 0.05581579963149728]
[2024-04-17 12:05:45,841: INFO: main: Training : batch 582 Loss: 0.028678118768741385]
[2024-04-17 12:05:46,472: INFO: main: Training : batch 583 Loss: 0.022530818275941264]
[2024-04-17 12:05:47,103: INFO: main: Training : batch 584 Loss: 0.05312893863446843]
[2024-04-17 12:05:47,738: INFO: main: Training : batch 585 Loss: 0.023590768882631483]
[2024-04-17 12:05:48,371: INFO: main: Training : batch 586 Loss: 0.009477813310527248]
[2024-04-17 12:05:49,004: INFO: main: Training : batch 587 Loss: 0.009948677147576876]
[2024-04-17 12:05:49,635: INFO: main: Training : batch 588 Loss: 0.03229948911634543]
[2024-04-17 12:05:50,275: INFO: main: Training : batch 589 Loss: 0.021760173810047072]
[2024-04-17 12:05:50,911: INFO: main: Training : batch 590 Loss: 0.04549091653434384]
[2024-04-17 12:05:51,550: INFO: main: Training : batch 591 Loss: 0.01197068436330183]
[2024-04-17 12:05:52,185: INFO: main: Training : batch 592 Loss: 0.025680796186777943]
[2024-04-17 12:05:52,824: INFO: main: Training : batch 593 Loss: 0.012221120295001713]
[2024-04-17 12:05:53,455: INFO: main: Training : batch 594 Loss: 0.013706129256863209]
[2024-04-17 12:05:54,089: INFO: main: Training : batch 595 Loss: 0.039222334118184814]
[2024-04-17 12:05:54,719: INFO: main: Training : batch 596 Loss: 0.02745405525611605]
[2024-04-17 12:05:55,354: INFO: main: Training : batch 597 Loss: 0.02910577777771352]
[2024-04-17 12:05:55,986: INFO: main: Training : batch 598 Loss: 0.0092396384006143]
[2024-04-17 12:05:56,619: INFO: main: Training : batch 599 Loss: 0.03764416918443312]
[2024-04-17 12:05:57,249: INFO: main: Training : batch 600 Loss: 0.03955713330582424]
[2024-04-17 12:05:57,881: INFO: main: Training : batch 601 Loss: 0.01667811383739369]
[2024-04-17 12:05:58,510: INFO: main: Training : batch 602 Loss: 0.02029544570476372]
[2024-04-17 12:05:59,140: INFO: main: Training : batch 603 Loss: 0.01102323190907799]
[2024-04-17 12:05:59,773: INFO: main: Training : batch 604 Loss: 0.019449585533788432]
[2024-04-17 12:06:00,404: INFO: main: Training : batch 605 Loss: 0.00994912703411506]
[2024-04-17 12:06:01,034: INFO: main: Training : batch 606 Loss: 0.04580842433308775]
[2024-04-17 12:06:01,666: INFO: main: Training : batch 607 Loss: 0.05952834147441995]
[2024-04-17 12:06:02,299: INFO: main: Training : batch 608 Loss: 0.02243697444683779]
[2024-04-17 12:06:02,930: INFO: main: Training : batch 609 Loss: 0.04323657182358233]
[2024-04-17 12:06:03,564: INFO: main: Training : batch 610 Loss: 0.02097092084462855]
[2024-04-17 12:06:04,202: INFO: main: Training : batch 611 Loss: 0.012724418058292745]
[2024-04-17 12:06:04,844: INFO: main: Training : batch 612 Loss: 0.014885509696301967]
[2024-04-17 12:06:05,482: INFO: main: Training : batch 613 Loss: 0.01249703892043786]
[2024-04-17 12:06:06,111: INFO: main: Training : batch 614 Loss: 0.01028109213858058]
[2024-04-17 12:06:06,741: INFO: main: Training : batch 615 Loss: 0.012105912541178543]
[2024-04-17 12:06:07,373: INFO: main: Training : batch 616 Loss: 0.02478057601388493]
[2024-04-17 12:06:08,009: INFO: main: Training : batch 617 Loss: 0.007149506800760244]
[2024-04-17 12:06:08,640: INFO: main: Training : batch 618 Loss: 0.041307961859853096]
[2024-04-17 12:06:09,269: INFO: main: Training : batch 619 Loss: 0.019854895174523154]
[2024-04-17 12:06:09,900: INFO: main: Training : batch 620 Loss: 0.00733806482482514]
[2024-04-17 12:06:10,531: INFO: main: Training : batch 621 Loss: 0.015488994602932304]
[2024-04-17 12:06:11,164: INFO: main: Training : batch 622 Loss: 0.030292959436092833]
[2024-04-17 12:06:11,792: INFO: main: Training : batch 623 Loss: 0.050532695880059854]
[2024-04-17 12:06:12,427: INFO: main: Training : batch 624 Loss: 0.006783843508035642]
[2024-04-17 12:06:13,056: INFO: main: Training : batch 625 Loss: 0.04735396576156796]
[2024-04-17 12:06:13,685: INFO: main: Training : batch 626 Loss: 0.06282917869981434]
[2024-04-17 12:06:14,321: INFO: main: Training : batch 627 Loss: 0.04285814304862384]
[2024-04-17 12:06:14,952: INFO: main: Training : batch 628 Loss: 0.02428733297621243]
[2024-04-17 12:06:15,579: INFO: main: Training : batch 629 Loss: 0.022067722573172306]
[2024-04-17 12:06:16,220: INFO: main: Training : batch 630 Loss: 0.046799642638563656]
[2024-04-17 12:06:16,864: INFO: main: Training : batch 631 Loss: 0.028392933676612078]
[2024-04-17 12:06:17,501: INFO: main: Training : batch 632 Loss: 0.049145783535875336]
[2024-04-17 12:06:18,144: INFO: main: Training : batch 633 Loss: 0.026573205384183184]
[2024-04-17 12:06:18,779: INFO: main: Training : batch 634 Loss: 0.038557452370283005]
[2024-04-17 12:06:19,413: INFO: main: Training : batch 635 Loss: 0.023556851857852616]
[2024-04-17 12:06:20,045: INFO: main: Training : batch 636 Loss: 0.023591693326565695]
[2024-04-17 12:06:20,673: INFO: main: Training : batch 637 Loss: 0.013863599941090642]
[2024-04-17 12:06:21,306: INFO: main: Training : batch 638 Loss: 0.05694601120510654]
[2024-04-17 12:06:21,934: INFO: main: Training : batch 639 Loss: 0.025360811782417246]
[2024-04-17 12:06:22,565: INFO: main: Training : batch 640 Loss: 0.018194126464908658]
[2024-04-17 12:06:23,196: INFO: main: Training : batch 641 Loss: 0.012149407272196633]
[2024-04-17 12:06:23,830: INFO: main: Training : batch 642 Loss: 0.03947873177029384]
[2024-04-17 12:06:24,466: INFO: main: Training : batch 643 Loss: 0.05435733721940848]
[2024-04-17 12:06:25,097: INFO: main: Training : batch 644 Loss: 0.04584429024948979]
[2024-04-17 12:06:25,729: INFO: main: Training : batch 645 Loss: 0.05796139334925712]
[2024-04-17 12:06:26,359: INFO: main: Training : batch 646 Loss: 0.02327100270620744]
[2024-04-17 12:06:26,991: INFO: main: Training : batch 647 Loss: 0.039855620795928644]
[2024-04-17 12:06:27,623: INFO: main: Training : batch 648 Loss: 0.04837438679751241]
[2024-04-17 12:06:28,254: INFO: main: Training : batch 649 Loss: 0.01829673504307614]
[2024-04-17 12:06:28,885: INFO: main: Training : batch 650 Loss: 0.031973397483430664]
[2024-04-17 12:06:29,522: INFO: main: Training : batch 651 Loss: 0.02345594285548881]
[2024-04-17 12:06:30,161: INFO: main: Training : batch 652 Loss: 0.044226079311016644]
[2024-04-17 12:06:30,803: INFO: main: Training : batch 653 Loss: 0.015468399235747153]
[2024-04-17 12:06:31,440: INFO: main: Training : batch 654 Loss: 0.023806002357606618]
[2024-04-17 12:06:32,078: INFO: main: Training : batch 655 Loss: 0.01510659446601846]
[2024-04-17 12:06:32,713: INFO: main: Training : batch 656 Loss: 0.051726470513007945]
[2024-04-17 12:06:33,341: INFO: main: Training : batch 657 Loss: 0.05075112540240977]
[2024-04-17 12:06:33,970: INFO: main: Training : batch 658 Loss: 0.016322482343451475]
[2024-04-17 12:06:34,597: INFO: main: Training : batch 659 Loss: 0.012787169495552645]
[2024-04-17 12:06:35,228: INFO: main: Training : batch 660 Loss: 0.006753061983078316]
[2024-04-17 12:06:35,854: INFO: main: Training : batch 661 Loss: 0.031460641554604284]
[2024-04-17 12:06:36,484: INFO: main: Training : batch 662 Loss: 0.041607129227443786]
[2024-04-17 12:06:37,114: INFO: main: Training : batch 663 Loss: 0.01838977135054134]
[2024-04-17 12:06:37,742: INFO: main: Training : batch 664 Loss: 0.012602439425204403]
[2024-04-17 12:06:38,373: INFO: main: Training : batch 665 Loss: 0.04021338085004138]
[2024-04-17 12:06:39,006: INFO: main: Training : batch 666 Loss: 0.018069882731043773]
[2024-04-17 12:06:39,634: INFO: main: Training : batch 667 Loss: 0.012726192242939072]
[2024-04-17 12:06:40,266: INFO: main: Training : batch 668 Loss: 0.011280673096251737]
[2024-04-17 12:06:40,902: INFO: main: Training : batch 669 Loss: 0.040013196963166714]
[2024-04-17 12:06:41,531: INFO: main: Training : batch 670 Loss: 0.015423807628341201]
[2024-04-17 12:06:42,164: INFO: main: Training : batch 671 Loss: 0.02267853296713984]
[2024-04-17 12:06:42,798: INFO: main: Training : batch 672 Loss: 0.03096450333417966]
[2024-04-17 12:06:43,438: INFO: main: Training : batch 673 Loss: 0.02464473076616886]
[2024-04-17 12:06:44,071: INFO: main: Training : batch 674 Loss: 0.013742606932487666]
[2024-04-17 12:06:44,712: INFO: main: Training : batch 675 Loss: 0.013878423656268367]
[2024-04-17 12:06:45,346: INFO: main: Training : batch 676 Loss: 0.005508806417749546]
[2024-04-17 12:06:45,980: INFO: main: Training : batch 677 Loss: 0.01356908060642194]
[2024-04-17 12:06:46,613: INFO: main: Training : batch 678 Loss: 0.012719263060354724]
[2024-04-17 12:06:47,245: INFO: main: Training : batch 679 Loss: 0.026875114350571935]
[2024-04-17 12:06:47,876: INFO: main: Training : batch 680 Loss: 0.017286631848655837]
[2024-04-17 12:06:48,506: INFO: main: Training : batch 681 Loss: 0.02427077159830203]
[2024-04-17 12:06:49,139: INFO: main: Training : batch 682 Loss: 0.03628414202338341]
[2024-04-17 12:06:49,771: INFO: main: Training : batch 683 Loss: 0.011104726114328235]
[2024-04-17 12:06:50,400: INFO: main: Training : batch 684 Loss: 0.010548573290443411]
[2024-04-17 12:06:51,032: INFO: main: Training : batch 685 Loss: 0.0560529304050311]
[2024-04-17 12:06:51,663: INFO: main: Training : batch 686 Loss: 0.0487346365231907]
[2024-04-17 12:06:52,301: INFO: main: Training : batch 687 Loss: 0.04265124702909638]
[2024-04-17 12:06:52,935: INFO: main: Training : batch 688 Loss: 0.041852884909600784]
[2024-04-17 12:06:53,561: INFO: main: Training : batch 689 Loss: 0.016879566672939075]
[2024-04-17 12:06:54,190: INFO: main: Training : batch 690 Loss: 0.03834347890312668]
[2024-04-17 12:06:54,823: INFO: main: Training : batch 691 Loss: 0.030876539045289815]
[2024-04-17 12:06:55,455: INFO: main: Training : batch 692 Loss: 0.013208611444569793]
[2024-04-17 12:06:56,090: INFO: main: Training : batch 693 Loss: 0.028898651704222827]
[2024-04-17 12:06:56,725: INFO: main: Training : batch 694 Loss: 0.025877365098553526]
[2024-04-17 12:06:57,357: INFO: main: Training : batch 695 Loss: 0.03391695745534275]
[2024-04-17 12:06:57,990: INFO: main: Training : batch 696 Loss: 0.01885407252036025]
[2024-04-17 12:06:58,637: INFO: main: Training : batch 697 Loss: 0.0312966426831336]
[2024-04-17 12:06:59,270: INFO: main: Training : batch 698 Loss: 0.020189633982654245]
[2024-04-17 12:06:59,900: INFO: main: Training : batch 699 Loss: 0.008270319253074481]
[2024-04-17 12:07:00,540: INFO: main: Training : batch 700 Loss: 0.0274630363040512]
[2024-04-17 12:07:01,174: INFO: main: Training : batch 701 Loss: 0.009465500769362641]
[2024-04-17 12:07:01,804: INFO: main: Training : batch 702 Loss: 0.009608474191282423]
[2024-04-17 12:07:02,427: INFO: main: Training : batch 703 Loss: 0.018431810882442178]
[2024-04-17 12:07:03,056: INFO: main: Training : batch 704 Loss: 0.03045387152294076]
[2024-04-17 12:07:03,689: INFO: main: Training : batch 705 Loss: 0.033024618477525115]
[2024-04-17 12:07:04,321: INFO: main: Training : batch 706 Loss: 0.03757354246341615]
[2024-04-17 12:07:04,951: INFO: main: Training : batch 707 Loss: 0.03064099317888951]
[2024-04-17 12:07:05,582: INFO: main: Training : batch 708 Loss: 0.01848525823825302]
[2024-04-17 12:07:06,211: INFO: main: Training : batch 709 Loss: 0.03420087890373117]
[2024-04-17 12:07:06,835: INFO: main: Training : batch 710 Loss: 0.007681659339543348]
[2024-04-17 12:07:07,463: INFO: main: Training : batch 711 Loss: 0.018344315864446074]
[2024-04-17 12:07:08,097: INFO: main: Training : batch 712 Loss: 0.043129778270821185]
[2024-04-17 12:07:08,743: INFO: main: Training : batch 713 Loss: 0.010429451109681331]
[2024-04-17 12:07:09,378: INFO: main: Training : batch 714 Loss: 0.027138600315603462]
[2024-04-17 12:07:10,017: INFO: main: Training : batch 715 Loss: 0.045349923498414685]
[2024-04-17 12:07:10,654: INFO: main: Training : batch 716 Loss: 0.01825227869341554]
[2024-04-17 12:07:11,301: INFO: main: Training : batch 717 Loss: 0.010235030336771507]
[2024-04-17 12:07:11,932: INFO: main: Training : batch 718 Loss: 0.012187499451740233]
[2024-04-17 12:07:12,564: INFO: main: Training : batch 719 Loss: 0.020407327227618142]
[2024-04-17 12:07:13,196: INFO: main: Training : batch 720 Loss: 0.04734764559473746]
[2024-04-17 12:07:13,824: INFO: main: Training : batch 721 Loss: 0.06839790903640583]
[2024-04-17 12:07:14,454: INFO: main: Training : batch 722 Loss: 0.030160626090830193]
[2024-04-17 12:07:15,090: INFO: main: Training : batch 723 Loss: 0.011163192958782875]
[2024-04-17 12:07:15,719: INFO: main: Training : batch 724 Loss: 0.012860243728998778]
[2024-04-17 12:07:16,349: INFO: main: Training : batch 725 Loss: 0.03699668625507011]
[2024-04-17 12:07:16,978: INFO: main: Training : batch 726 Loss: 0.023869029710325323]
[2024-04-17 12:07:17,611: INFO: main: Training : batch 727 Loss: 0.06204399251606808]
[2024-04-17 12:07:18,242: INFO: main: Training : batch 728 Loss: 0.04361492672113271]
[2024-04-17 12:07:18,873: INFO: main: Training : batch 729 Loss: 0.03798152131147095]
[2024-04-17 12:07:19,507: INFO: main: Training : batch 730 Loss: 0.028294538070200367]
[2024-04-17 12:07:20,138: INFO: main: Training : batch 731 Loss: 0.05132749862086375]
[2024-04-17 12:07:20,767: INFO: main: Training : batch 732 Loss: 0.015456321022949512]
[2024-04-17 12:07:21,401: INFO: main: Training : batch 733 Loss: 0.024131541527714004]
[2024-04-17 12:07:22,035: INFO: main: Training : batch 734 Loss: 0.01596032473289524]
[2024-04-17 12:07:22,668: INFO: main: Training : batch 735 Loss: 0.02021036386685329]
[2024-04-17 12:07:23,301: INFO: main: Training : batch 736 Loss: 0.02564202682603828]
[2024-04-17 12:07:23,939: INFO: main: Training : batch 737 Loss: 0.024780195157563117]
[2024-04-17 12:07:24,576: INFO: main: Training : batch 738 Loss: 0.009780233892933136]
[2024-04-17 12:07:25,208: INFO: main: Training : batch 739 Loss: 0.013110635387061934]
[2024-04-17 12:07:25,839: INFO: main: Training : batch 740 Loss: 0.03503685926334028]
[2024-04-17 12:07:26,464: INFO: main: Training : batch 741 Loss: 0.014232235135880261]
[2024-04-17 12:07:27,092: INFO: main: Training : batch 742 Loss: 0.02386521108956668]
[2024-04-17 12:07:27,725: INFO: main: Training : batch 743 Loss: 0.018802294160958265]
[2024-04-17 12:07:28,356: INFO: main: Training : batch 744 Loss: 0.029976537004003066]
[2024-04-17 12:07:28,986: INFO: main: Training : batch 745 Loss: 0.005513121660974436]
[2024-04-17 12:07:29,616: INFO: main: Training : batch 746 Loss: 0.03810226967512799]
[2024-04-17 12:07:30,250: INFO: main: Training : batch 747 Loss: 0.016080575627966155]
[2024-04-17 12:07:30,881: INFO: main: Training : batch 748 Loss: 0.021191062154778836]
[2024-04-17 12:07:31,514: INFO: main: Training : batch 749 Loss: 0.01528524103564797]
[2024-04-17 12:07:32,146: INFO: main: Training : batch 750 Loss: 0.022078408000050977]
[2024-04-17 12:07:32,779: INFO: main: Training : batch 751 Loss: 0.026202881103814902]
[2024-04-17 12:07:33,411: INFO: main: Training : batch 752 Loss: 0.01364744525871383]
[2024-04-17 12:07:34,042: INFO: main: Training : batch 753 Loss: 0.047539049346878054]
[2024-04-17 12:07:34,675: INFO: main: Training : batch 754 Loss: 0.021366852359666687]
[2024-04-17 12:07:35,312: INFO: main: Training : batch 755 Loss: 0.022556630117873167]
[2024-04-17 12:07:35,949: INFO: main: Training : batch 756 Loss: 0.007157428943253728]
[2024-04-17 12:07:36,582: INFO: main: Training : batch 757 Loss: 0.0034325337943437017]
[2024-04-17 12:07:37,225: INFO: main: Training : batch 758 Loss: 0.0209629307194752]
[2024-04-17 12:07:37,864: INFO: main: Training : batch 759 Loss: 0.019249887352963912]
[2024-04-17 12:07:38,496: INFO: main: Training : batch 760 Loss: 0.03647865410944192]
[2024-04-17 12:07:39,123: INFO: main: Training : batch 761 Loss: 0.007246160733345618]
[2024-04-17 12:07:39,754: INFO: main: Training : batch 762 Loss: 0.020319278481343045]
[2024-04-17 12:07:40,386: INFO: main: Training : batch 763 Loss: 0.03946965287146692]
[2024-04-17 12:07:41,014: INFO: main: Training : batch 764 Loss: 0.012500418754569248]
[2024-04-17 12:07:41,644: INFO: main: Training : batch 765 Loss: 0.019421443169993852]
[2024-04-17 12:07:42,273: INFO: main: Training : batch 766 Loss: 0.001965262040821753]
[2024-04-17 12:07:42,903: INFO: main: Training : batch 767 Loss: 0.024110047605172415]
[2024-04-17 12:07:43,534: INFO: main: Training : batch 768 Loss: 0.008368806018405114]
[2024-04-17 12:07:44,165: INFO: main: Training : batch 769 Loss: 0.03734563331444463]
[2024-04-17 12:07:44,796: INFO: main: Training : batch 770 Loss: 0.1263052594709137]
[2024-04-17 12:07:45,425: INFO: main: Training : batch 771 Loss: 0.04480586796915898]
[2024-04-17 12:07:46,052: INFO: main: Training : batch 772 Loss: 0.009313626940656563]
[2024-04-17 12:07:46,688: INFO: main: Training : batch 773 Loss: 0.015447494070965063]
[2024-04-17 12:07:47,318: INFO: main: Training : batch 774 Loss: 0.015735958214423692]
[2024-04-17 12:07:47,948: INFO: main: Training : batch 775 Loss: 0.03675326622423729]
[2024-04-17 12:07:48,589: INFO: main: Training : batch 776 Loss: 0.01775440844969477]
[2024-04-17 12:07:49,238: INFO: main: Training : batch 777 Loss: 0.026115843585440897]
[2024-04-17 12:07:49,878: INFO: main: Training : batch 778 Loss: 0.01366536396479292]
[2024-04-17 12:07:50,515: INFO: main: Training : batch 779 Loss: 0.0374066696769261]
[2024-04-17 12:07:51,155: INFO: main: Training : batch 780 Loss: 0.014860445846484459]
[2024-04-17 12:07:51,784: INFO: main: Training : batch 781 Loss: 0.023072774374546513]
[2024-04-17 12:07:52,413: INFO: main: Training : batch 782 Loss: 0.01921093802074889]
[2024-04-17 12:07:53,041: INFO: main: Training : batch 783 Loss: 0.007980649571407794]
[2024-04-17 12:07:53,668: INFO: main: Training : batch 784 Loss: 0.01568733036596384]
[2024-04-17 12:07:54,301: INFO: main: Training : batch 785 Loss: 0.034505840842599105]
[2024-04-17 12:07:54,927: INFO: main: Training : batch 786 Loss: 0.0062427317707045215]
[2024-04-17 12:07:55,559: INFO: main: Training : batch 787 Loss: 0.006301628108872066]
[2024-04-17 12:07:56,190: INFO: main: Training : batch 788 Loss: 0.016232145073473824]
[2024-04-17 12:07:56,822: INFO: main: Training : batch 789 Loss: 0.01323971831947422]
[2024-04-17 12:07:57,456: INFO: main: Training : batch 790 Loss: 0.036992485814945304]
[2024-04-17 12:07:58,086: INFO: main: Training : batch 791 Loss: 0.013529592147362744]
[2024-04-17 12:07:58,711: INFO: main: Training : batch 792 Loss: 0.035830720842938264]
[2024-04-17 12:07:59,341: INFO: main: Training : batch 793 Loss: 0.018376512830944925]
[2024-04-17 12:07:59,973: INFO: main: Training : batch 794 Loss: 0.012967841250447256]
[2024-04-17 12:08:00,607: INFO: main: Training : batch 795 Loss: 0.020276346161871673]
[2024-04-17 12:08:01,237: INFO: main: Training : batch 796 Loss: 0.015076820490041156]
[2024-04-17 12:08:01,882: INFO: main: Training : batch 797 Loss: 0.07119876467398632]
[2024-04-17 12:08:02,523: INFO: main: Training : batch 798 Loss: 0.027570486820507438]
[2024-04-17 12:08:03,160: INFO: main: Training : batch 799 Loss: 0.026467633524042168]
[2024-04-17 12:08:03,808: INFO: main: Training : batch 800 Loss: 0.014450365712430358]
[2024-04-17 12:08:04,452: INFO: main: Training : batch 801 Loss: 0.01066453096593306]
[2024-04-17 12:08:05,081: INFO: main: Training : batch 802 Loss: 0.028728289662649682]
[2024-04-17 12:08:05,716: INFO: main: Training : batch 803 Loss: 0.02718047622909244]
[2024-04-17 12:08:06,349: INFO: main: Training : batch 804 Loss: 0.007541691339997655]
[2024-04-17 12:08:06,978: INFO: main: Training : batch 805 Loss: 0.013671198173491255]
[2024-04-17 12:08:07,611: INFO: main: Training : batch 806 Loss: 0.019762639191843533]
[2024-04-17 12:08:08,243: INFO: main: Training : batch 807 Loss: 0.01592722477208424]
[2024-04-17 12:08:08,878: INFO: main: Training : batch 808 Loss: 0.008651407238675798]
[2024-04-17 12:08:09,507: INFO: main: Training : batch 809 Loss: 0.008516760211679915]
[2024-04-17 12:08:10,140: INFO: main: Training : batch 810 Loss: 0.04184903361834424]
[2024-04-17 12:08:10,772: INFO: main: Training : batch 811 Loss: 0.006995120868735395]
[2024-04-17 12:08:11,401: INFO: main: Training : batch 812 Loss: 0.023871010355498958]
[2024-04-17 12:08:12,034: INFO: main: Training : batch 813 Loss: 0.031451494784661166]
[2024-04-17 12:08:12,665: INFO: main: Training : batch 814 Loss: 0.03883806278942102]
[2024-04-17 12:08:13,294: INFO: main: Training : batch 815 Loss: 0.016260933578618145]
[2024-04-17 12:08:13,923: INFO: main: Training : batch 816 Loss: 0.025858930324308173]
[2024-04-17 12:08:14,556: INFO: main: Training : batch 817 Loss: 0.023742324970800116]
[2024-04-17 12:08:15,192: INFO: main: Training : batch 818 Loss: 0.026683012455599116]
[2024-04-17 12:08:15,834: INFO: main: Training : batch 819 Loss: 0.011137628765361035]
[2024-04-17 12:08:16,473: INFO: main: Training : batch 820 Loss: 0.02541814869371584]
[2024-04-17 12:08:17,105: INFO: main: Training : batch 821 Loss: 0.014950138791129792]
[2024-04-17 12:08:17,746: INFO: main: Training : batch 822 Loss: 0.019305462469007828]
[2024-04-17 12:08:18,375: INFO: main: Training : batch 823 Loss: 0.0147734051516442]
[2024-04-17 12:08:19,006: INFO: main: Training : batch 824 Loss: 0.016383329028226732]
[2024-04-17 12:08:19,638: INFO: main: Training : batch 825 Loss: 0.01063497753338651]
[2024-04-17 12:08:20,265: INFO: main: Training : batch 826 Loss: 0.015567413059783907]
[2024-04-17 12:08:20,891: INFO: main: Training : batch 827 Loss: 0.03461744763430127]
[2024-04-17 12:08:21,516: INFO: main: Training : batch 828 Loss: 0.016829872390355466]
[2024-04-17 12:08:22,147: INFO: main: Training : batch 829 Loss: 0.048500534844409846]
[2024-04-17 12:08:22,776: INFO: main: Training : batch 830 Loss: 0.07039867651685074]
[2024-04-17 12:08:23,405: INFO: main: Training : batch 831 Loss: 0.02782076331360503]
[2024-04-17 12:08:24,037: INFO: main: Training : batch 832 Loss: 0.003074436656526404]
[2024-04-17 12:08:24,665: INFO: main: Training : batch 833 Loss: 0.02471906557099958]
[2024-04-17 12:08:25,294: INFO: main: Training : batch 834 Loss: 0.024074698870268623]
[2024-04-17 12:08:25,926: INFO: main: Training : batch 835 Loss: 0.009576769708883706]
[2024-04-17 12:08:26,557: INFO: main: Training : batch 836 Loss: 0.01947761876034161]
[2024-04-17 12:08:27,186: INFO: main: Training : batch 837 Loss: 0.037820842614694684]
[2024-04-17 12:08:27,815: INFO: main: Training : batch 838 Loss: 0.019163013816408017]
[2024-04-17 12:08:28,446: INFO: main: Training : batch 839 Loss: 0.014797630161023052]
[2024-04-17 12:08:29,088: INFO: main: Training : batch 840 Loss: 0.012786248187809367]
[2024-04-17 12:08:29,725: INFO: main: Training : batch 841 Loss: 0.018104838954156162]
[2024-04-17 12:08:30,366: INFO: main: Training : batch 842 Loss: 0.036713322585080456]
[2024-04-17 12:08:31,011: INFO: main: Training : batch 843 Loss: 0.03994788181926811]
[2024-04-17 12:08:31,642: INFO: main: Training : batch 844 Loss: 0.01338622158832908]
[2024-04-17 12:08:32,273: INFO: main: Training : batch 845 Loss: 0.011579433252176252]
[2024-04-17 12:08:32,901: INFO: main: Training : batch 846 Loss: 0.04520594851893098]
[2024-04-17 12:08:33,529: INFO: main: Training : batch 847 Loss: 0.017927047640840108]
[2024-04-17 12:08:34,158: INFO: main: Training : batch 848 Loss: 0.023118325373069182]
[2024-04-17 12:08:34,790: INFO: main: Training : batch 849 Loss: 0.010741967625674073]
[2024-04-17 12:08:35,420: INFO: main: Training : batch 850 Loss: 0.01840344655585588]
[2024-04-17 12:08:36,052: INFO: main: Training : batch 851 Loss: 0.014485891322445425]
[2024-04-17 12:08:36,677: INFO: main: Training : batch 852 Loss: 0.017282925432213082]
[2024-04-17 12:08:37,311: INFO: main: Training : batch 853 Loss: 0.007774518670160734]
[2024-04-17 12:08:37,943: INFO: main: Training : batch 854 Loss: 0.02428293732464335]
[2024-04-17 12:08:38,574: INFO: main: Training : batch 855 Loss: 0.051391186145714626]
[2024-04-17 12:08:39,206: INFO: main: Training : batch 856 Loss: 0.010409629343125108]
[2024-04-17 12:08:39,838: INFO: main: Training : batch 857 Loss: 0.010791116452053025]
[2024-04-17 12:08:40,464: INFO: main: Training : batch 858 Loss: 0.03597692445945143]
[2024-04-17 12:08:41,096: INFO: main: Training : batch 859 Loss: 0.04417138974704368]
[2024-04-17 12:08:41,727: INFO: main: Training : batch 860 Loss: 0.007738317478274973]
[2024-04-17 12:08:42,363: INFO: main: Training : batch 861 Loss: 0.010859311631634939]
[2024-04-17 12:08:43,002: INFO: main: Training : batch 862 Loss: 0.01022425607728724]
[2024-04-17 12:08:43,641: INFO: main: Training : batch 863 Loss: 0.010965875207181768]
[2024-04-17 12:08:44,282: INFO: main: Training : batch 864 Loss: 0.02888387314458686]
[2024-04-17 12:08:44,912: INFO: main: Training : batch 865 Loss: 0.018482910470173877]
[2024-04-17 12:08:45,545: INFO: main: Training : batch 866 Loss: 0.005551890865752261]
[2024-04-17 12:08:46,174: INFO: main: Training : batch 867 Loss: 0.06339283618490997]
[2024-04-17 12:08:46,803: INFO: main: Training : batch 868 Loss: 0.01737013644531012]
[2024-04-17 12:08:47,431: INFO: main: Training : batch 869 Loss: 0.043918535628072214]
[2024-04-17 12:08:48,063: INFO: main: Training : batch 870 Loss: 0.0454482147974814]
[2024-04-17 12:08:48,696: INFO: main: Training : batch 871 Loss: 0.024238252656109502]
[2024-04-17 12:08:49,326: INFO: main: Training : batch 872 Loss: 0.0069343274866903734]
[2024-04-17 12:08:49,955: INFO: main: Training : batch 873 Loss: 0.023583747119612575]
[2024-04-17 12:08:50,586: INFO: main: Training : batch 874 Loss: 0.013788088497376194]
[2024-04-17 12:08:51,213: INFO: main: Training : batch 875 Loss: 0.009076531882024272]
[2024-04-17 12:08:51,845: INFO: main: Training : batch 876 Loss: 0.01753776267770899]
[2024-04-17 12:08:52,469: INFO: main: Training : batch 877 Loss: 0.008300255680198567]
[2024-04-17 12:08:53,100: INFO: main: Training : batch 878 Loss: 0.007017927116559301]
[2024-04-17 12:08:53,732: INFO: main: Training : batch 879 Loss: 0.0027355253628890998]
[2024-04-17 12:08:54,366: INFO: main: Training : batch 880 Loss: 0.028838924767948176]
[2024-04-17 12:08:55,005: INFO: main: Training : batch 881 Loss: 0.04729367590053053]
[2024-04-17 12:08:55,642: INFO: main: Training : batch 882 Loss: 0.02310936514530741]
[2024-04-17 12:08:56,278: INFO: main: Training : batch 883 Loss: 0.025173944464113713]
[2024-04-17 12:08:56,914: INFO: main: Training : batch 884 Loss: 0.03266356411142864]
[2024-04-17 12:08:57,549: INFO: main: Training : batch 885 Loss: 0.023913130656856393]
[2024-04-17 12:08:58,179: INFO: main: Training : batch 886 Loss: 0.01772327206830629]
[2024-04-17 12:08:58,812: INFO: main: Training : batch 887 Loss: 0.023608624046555785]
[2024-04-17 12:08:59,443: INFO: main: Training : batch 888 Loss: 0.025258281657240665]
[2024-04-17 12:09:00,073: INFO: main: Training : batch 889 Loss: 0.01925914823339842]
[2024-04-17 12:09:00,707: INFO: main: Training : batch 890 Loss: 0.012662291853249836]
[2024-04-17 12:09:01,335: INFO: main: Training : batch 891 Loss: 0.01926799346411006]
[2024-04-17 12:09:01,968: INFO: main: Training : batch 892 Loss: 0.020570447381074202]
[2024-04-17 12:09:02,602: INFO: main: Training : batch 893 Loss: 0.02938768698781348]
[2024-04-17 12:09:03,238: INFO: main: Training : batch 894 Loss: 0.01804108260552987]
[2024-04-17 12:09:03,873: INFO: main: Training : batch 895 Loss: 0.0038562152588527076]
[2024-04-17 12:09:04,504: INFO: main: Training : batch 896 Loss: 0.04395250313291671]
[2024-04-17 12:09:05,137: INFO: main: Training : batch 897 Loss: 0.012486868218399528]
[2024-04-17 12:09:05,765: INFO: main: Training : batch 898 Loss: 0.03135991459945446]
[2024-04-17 12:09:06,396: INFO: main: Training : batch 899 Loss: 0.015386169998453636]
[2024-04-17 12:09:07,027: INFO: main: Training : batch 900 Loss: 0.01955975738324407]
[2024-04-17 12:09:07,658: INFO: main: Training : batch 901 Loss: 0.029090278668517103]
[2024-04-17 12:09:08,292: INFO: main: Training : batch 902 Loss: 0.029036930864010973]
[2024-04-17 12:09:08,927: INFO: main: Training : batch 903 Loss: 0.009887801688062326]
[2024-04-17 12:09:09,564: INFO: main: Training : batch 904 Loss: 0.014032143421970528]
[2024-04-17 12:09:10,201: INFO: main: Training : batch 905 Loss: 0.02382505802682286]
[2024-04-17 12:09:10,845: INFO: main: Training : batch 906 Loss: 0.018447172718313257]
[2024-04-17 12:09:11,478: INFO: main: Training : batch 907 Loss: 0.033175903080006626]
[2024-04-17 12:09:12,103: INFO: main: Training : batch 908 Loss: 0.004152612371470096]
[2024-04-17 12:09:12,734: INFO: main: Training : batch 909 Loss: 0.008410301584348663]
[2024-04-17 12:09:13,360: INFO: main: Training : batch 910 Loss: 0.014127179516321269]
[2024-04-17 12:09:13,991: INFO: main: Training : batch 911 Loss: 0.04644739066591432]
[2024-04-17 12:09:14,623: INFO: main: Training : batch 912 Loss: 0.005047175301214671]
[2024-04-17 12:09:15,252: INFO: main: Training : batch 913 Loss: 0.008784143279947915]
[2024-04-17 12:09:15,877: INFO: main: Training : batch 914 Loss: 0.044076845407337154]
[2024-04-17 12:09:16,510: INFO: main: Training : batch 915 Loss: 0.03578177148996938]
[2024-04-17 12:09:17,139: INFO: main: Training : batch 916 Loss: 0.024595624964803053]
[2024-04-17 12:09:17,766: INFO: main: Training : batch 917 Loss: 0.022426261598643464]
[2024-04-17 12:09:18,398: INFO: main: Training : batch 918 Loss: 0.05204310817391299]
[2024-04-17 12:09:19,028: INFO: main: Training : batch 919 Loss: 0.003513059197052419]
[2024-04-17 12:09:19,660: INFO: main: Training : batch 920 Loss: 0.022800225045246578]
[2024-04-17 12:09:20,291: INFO: main: Training : batch 921 Loss: 0.018426774753448612]
[2024-04-17 12:09:20,929: INFO: main: Training : batch 922 Loss: 0.016846789303555945]
[2024-04-17 12:09:21,570: INFO: main: Training : batch 923 Loss: 0.011821377647718505]
[2024-04-17 12:09:22,212: INFO: main: Training : batch 924 Loss: 0.008642836151429844]
[2024-04-17 12:09:22,853: INFO: main: Training : batch 925 Loss: 0.012930575994868236]
[2024-04-17 12:09:23,488: INFO: main: Training : batch 926 Loss: 0.007907947982132275]
[2024-04-17 12:09:24,120: INFO: main: Training : batch 927 Loss: 0.030197484111643393]
[2024-04-17 12:09:24,749: INFO: main: Training : batch 928 Loss: 0.005259834404388652]
[2024-04-17 12:09:25,383: INFO: main: Training : batch 929 Loss: 0.03991309394723926]
[2024-04-17 12:09:26,019: INFO: main: Training : batch 930 Loss: 0.02521492003271045]
[2024-04-17 12:09:26,649: INFO: main: Training : batch 931 Loss: 0.019367338602763923]
[2024-04-17 12:09:27,282: INFO: main: Training : batch 932 Loss: 0.034142961176065446]
[2024-04-17 12:09:27,912: INFO: main: Training : batch 933 Loss: 0.04144677206759148]
[2024-04-17 12:09:28,544: INFO: main: Training : batch 934 Loss: 0.02726115413140831]
[2024-04-17 12:09:29,174: INFO: main: Training : batch 935 Loss: 0.0232670090534363]
[2024-04-17 12:09:29,804: INFO: main: Training : batch 936 Loss: 0.02019496769485597]
[2024-04-17 12:09:30,432: INFO: main: Training : batch 937 Loss: 0.04774349570095413]
[2024-04-17 12:09:31,062: INFO: main: Training : batch 938 Loss: 0.04688900444507046]
[2024-04-17 12:09:31,690: INFO: main: Training : batch 939 Loss: 0.019877996363609478]
[2024-04-17 12:09:32,328: INFO: main: Training : batch 940 Loss: 0.04533854276733896]
[2024-04-17 12:09:32,957: INFO: main: Training : batch 941 Loss: 0.029268015289561838]
[2024-04-17 12:09:33,591: INFO: main: Training : batch 942 Loss: 0.006315093610836534]
[2024-04-17 12:09:34,225: INFO: main: Training : batch 943 Loss: 0.04571330867183112]
[2024-04-17 12:09:34,864: INFO: main: Training : batch 944 Loss: 0.013908437775084194]
[2024-04-17 12:09:35,501: INFO: main: Training : batch 945 Loss: 0.04106253848570741]
[2024-04-17 12:09:36,142: INFO: main: Training : batch 946 Loss: 0.009604192515028122]
[2024-04-17 12:09:36,785: INFO: main: Training : batch 947 Loss: 0.010503189415801606]
[2024-04-17 12:09:37,414: INFO: main: Training : batch 948 Loss: 0.04221640704116057]
[2024-04-17 12:09:38,049: INFO: main: Training : batch 949 Loss: 0.018199128362441454]
[2024-04-17 12:09:38,685: INFO: main: Training : batch 950 Loss: 0.04199621595418172]
[2024-04-17 12:09:39,316: INFO: main: Training : batch 951 Loss: 0.008389679379842538]
[2024-04-17 12:09:39,947: INFO: main: Training : batch 952 Loss: 0.025953332191176553]
[2024-04-17 12:09:40,580: INFO: main: Training : batch 953 Loss: 0.05189531521685363]
[2024-04-17 12:09:41,210: INFO: main: Training : batch 954 Loss: 0.019951062291720577]
[2024-04-17 12:09:41,840: INFO: main: Training : batch 955 Loss: 0.03720983355331628]
[2024-04-17 12:09:42,473: INFO: main: Training : batch 956 Loss: 0.004841053763519378]
[2024-04-17 12:09:43,102: INFO: main: Training : batch 957 Loss: 0.012642247692938481]
[2024-04-17 12:09:43,739: INFO: main: Training : batch 958 Loss: 0.03282576489066124]
[2024-04-17 12:09:44,373: INFO: main: Training : batch 959 Loss: 0.023041096383655747]
[2024-04-17 12:09:45,007: INFO: main: Training : batch 960 Loss: 0.019443750393061385]
[2024-04-17 12:09:45,637: INFO: main: Training : batch 961 Loss: 0.011035988634640807]
[2024-04-17 12:09:46,264: INFO: main: Training : batch 962 Loss: 0.025953131408786134]
[2024-04-17 12:09:46,893: INFO: main: Training : batch 963 Loss: 0.01542146513967852]
[2024-04-17 12:09:47,530: INFO: main: Training : batch 964 Loss: 0.026142667913692273]
[2024-04-17 12:09:48,174: INFO: main: Training : batch 965 Loss: 0.008012766557260466]
[2024-04-17 12:09:48,809: INFO: main: Training : batch 966 Loss: 0.03566733608253307]
[2024-04-17 12:09:49,447: INFO: main: Training : batch 967 Loss: 0.00991874967276781]
[2024-04-17 12:09:50,081: INFO: main: Training : batch 968 Loss: 0.014449672255087703]
[2024-04-17 12:09:50,713: INFO: main: Training : batch 969 Loss: 0.007943389284446094]
[2024-04-17 12:09:51,342: INFO: main: Training : batch 970 Loss: 0.004445030554989506]
[2024-04-17 12:09:51,969: INFO: main: Training : batch 971 Loss: 0.015161133519349316]
[2024-04-17 12:09:52,600: INFO: main: Training : batch 972 Loss: 0.024389235828547868]
[2024-04-17 12:09:53,235: INFO: main: Training : batch 973 Loss: 0.025453659472357525]
[2024-04-17 12:09:53,872: INFO: main: Training : batch 974 Loss: 0.03350150604254846]
[2024-04-17 12:09:54,501: INFO: main: Training : batch 975 Loss: 0.02142048691031618]
[2024-04-17 12:09:55,135: INFO: main: Training : batch 976 Loss: 0.02065174022107193]
[2024-04-17 12:09:55,770: INFO: main: Training : batch 977 Loss: 0.027541397271040647]
[2024-04-17 12:09:56,405: INFO: main: Training : batch 978 Loss: 0.005647520982892761]
[2024-04-17 12:09:57,037: INFO: main: Training : batch 979 Loss: 0.024574541641740177]
[2024-04-17 12:09:57,670: INFO: main: Training : batch 980 Loss: 0.015845069741563085]
[2024-04-17 12:09:58,304: INFO: main: Training : batch 981 Loss: 0.03648119489601578]
[2024-04-17 12:09:58,935: INFO: main: Training : batch 982 Loss: 0.04378550627007795]
[2024-04-17 12:09:59,562: INFO: main: Training : batch 983 Loss: 0.013682445594401998]
[2024-04-17 12:10:00,196: INFO: main: Training : batch 984 Loss: 0.00994198147283734]
[2024-04-17 12:10:00,832: INFO: main: Training : batch 985 Loss: 0.030676342646831407]
[2024-04-17 12:10:01,473: INFO: main: Training : batch 986 Loss: 0.008235435777339161]
[2024-04-17 12:10:02,111: INFO: main: Training : batch 987 Loss: 0.0045973047446723845]
[2024-04-17 12:10:02,748: INFO: main: Training : batch 988 Loss: 0.009674963088361943]
[2024-04-17 12:10:03,398: INFO: main: Training : batch 989 Loss: 0.007423537897005189]
[2024-04-17 12:10:04,032: INFO: main: Training : batch 990 Loss: 0.02258166792898574]
[2024-04-17 12:10:04,666: INFO: main: Training : batch 991 Loss: 0.01179146763990565]
[2024-04-17 12:10:05,296: INFO: main: Training : batch 992 Loss: 0.0497747524928669]
[2024-04-17 12:10:05,931: INFO: main: Training : batch 993 Loss: 0.029146462644940994]
[2024-04-17 12:10:06,562: INFO: main: Training : batch 994 Loss: 0.0036723454335379395]
[2024-04-17 12:10:07,193: INFO: main: Training : batch 995 Loss: 0.03354818557826536]
[2024-04-17 12:10:07,821: INFO: main: Training : batch 996 Loss: 0.017377141806621192]
[2024-04-17 12:10:08,454: INFO: main: Training : batch 997 Loss: 0.014739705585225315]
[2024-04-17 12:10:09,088: INFO: main: Training : batch 998 Loss: 0.016916760777798696]
[2024-04-17 12:10:09,718: INFO: main: Training : batch 999 Loss: 0.023439407081690154]
[2024-04-17 12:10:10,349: INFO: main: Training : batch 1000 Loss: 0.013024508826680216]
[2024-04-17 12:10:10,981: INFO: main: Training : batch 1001 Loss: 0.02729943161068571]
[2024-04-17 12:10:11,612: INFO: main: Training : batch 1002 Loss: 0.007884187811725313]
[2024-04-17 12:10:12,244: INFO: main: Training : batch 1003 Loss: 0.01679446170869357]
[2024-04-17 12:10:12,877: INFO: main: Training : batch 1004 Loss: 0.011118451095421682]
[2024-04-17 12:10:13,509: INFO: main: Training : batch 1005 Loss: 0.008305643889415513]
[2024-04-17 12:10:14,147: INFO: main: Training : batch 1006 Loss: 0.018468947157000745]
[2024-04-17 12:10:14,780: INFO: main: Training : batch 1007 Loss: 0.013427221009818024]
[2024-04-17 12:10:15,420: INFO: main: Training : batch 1008 Loss: 0.015212930382344442]
[2024-04-17 12:10:16,061: INFO: main: Training : batch 1009 Loss: 0.01667334892421183]
[2024-04-17 12:10:16,707: INFO: main: Training : batch 1010 Loss: 0.018489660341662943]
[2024-04-17 12:10:17,340: INFO: main: Training : batch 1011 Loss: 0.020416191938957485]
[2024-04-17 12:10:17,966: INFO: main: Training : batch 1012 Loss: 0.008866491035299452]
[2024-04-17 12:10:18,596: INFO: main: Training : batch 1013 Loss: 0.015354758796771244]
[2024-04-17 12:10:19,224: INFO: main: Training : batch 1014 Loss: 0.00265494048503353]
[2024-04-17 12:10:19,857: INFO: main: Training : batch 1015 Loss: 0.015730107260865964]
[2024-04-17 12:10:20,485: INFO: main: Training : batch 1016 Loss: 0.013270639870972531]
[2024-04-17 12:10:21,117: INFO: main: Training : batch 1017 Loss: 0.006234812714528621]
[2024-04-17 12:10:21,744: INFO: main: Training : batch 1018 Loss: 0.03423365231326567]
[2024-04-17 12:10:22,376: INFO: main: Training : batch 1019 Loss: 0.004920609289235849]
[2024-04-17 12:10:23,010: INFO: main: Training : batch 1020 Loss: 0.025900981886302438]
[2024-04-17 12:10:23,639: INFO: main: Training : batch 1021 Loss: 0.0240013947012598]
[2024-04-17 12:10:24,268: INFO: main: Training : batch 1022 Loss: 0.017343814304746154]
[2024-04-17 12:10:24,901: INFO: main: Training : batch 1023 Loss: 0.02531946432479861]
[2024-04-17 12:10:25,529: INFO: main: Training : batch 1024 Loss: 0.02472848830582692]
[2024-04-17 12:10:26,164: INFO: main: Training : batch 1025 Loss: 0.01415486701217846]
[2024-04-17 12:10:26,799: INFO: main: Training : batch 1026 Loss: 0.02661490914321153]
[2024-04-17 12:10:27,435: INFO: main: Training : batch 1027 Loss: 0.011707929106025947]
[2024-04-17 12:10:28,073: INFO: main: Training : batch 1028 Loss: 0.06595100826373125]
[2024-04-17 12:10:28,714: INFO: main: Training : batch 1029 Loss: 0.02037928899369355]
[2024-04-17 12:10:29,354: INFO: main: Training : batch 1030 Loss: 0.004199958990931726]
[2024-04-17 12:10:29,985: INFO: main: Training : batch 1031 Loss: 0.003877027920922503]
[2024-04-17 12:10:30,616: INFO: main: Training : batch 1032 Loss: 0.014439645529956564]
[2024-04-17 12:10:31,247: INFO: main: Training : batch 1033 Loss: 0.011044823968455198]
[2024-04-17 12:10:31,877: INFO: main: Training : batch 1034 Loss: 0.04873191316180616]
[2024-04-17 12:10:32,508: INFO: main: Training : batch 1035 Loss: 0.054362418070873886]
[2024-04-17 12:10:33,140: INFO: main: Training : batch 1036 Loss: 0.012071924329788764]
[2024-04-17 12:10:33,768: INFO: main: Training : batch 1037 Loss: 0.023376509500434097]
[2024-04-17 12:10:34,393: INFO: main: Training : batch 1038 Loss: 0.012964851148860859]
[2024-04-17 12:10:35,026: INFO: main: Training : batch 1039 Loss: 0.06916810831753087]
[2024-04-17 12:10:35,656: INFO: main: Training : batch 1040 Loss: 0.02854845145155066]
[2024-04-17 12:10:36,284: INFO: main: Training : batch 1041 Loss: 0.015343142420215509]
[2024-04-17 12:10:36,914: INFO: main: Training : batch 1042 Loss: 0.018677059867912675]
[2024-04-17 12:10:37,550: INFO: main: Training : batch 1043 Loss: 0.01422090862496012]
[2024-04-17 12:10:38,182: INFO: main: Training : batch 1044 Loss: 0.020880805722538007]
[2024-04-17 12:10:38,813: INFO: main: Training : batch 1045 Loss: 0.024024477728593317]
[2024-04-17 12:10:39,444: INFO: main: Training : batch 1046 Loss: 0.005970692973153252]
[2024-04-17 12:10:40,081: INFO: main: Training : batch 1047 Loss: 0.0314236739104719]
[2024-04-17 12:10:40,720: INFO: main: Training : batch 1048 Loss: 0.025717315226780044]
[2024-04-17 12:10:41,361: INFO: main: Training : batch 1049 Loss: 0.025049629106653353]
[2024-04-17 12:10:42,004: INFO: main: Training : batch 1050 Loss: 0.031004870609075152]
[2024-04-17 12:10:42,640: INFO: main: Training : batch 1051 Loss: 0.023149406850533967]
[2024-04-17 12:10:43,270: INFO: main: Training : batch 1052 Loss: 0.015401353975514155]
[2024-04-17 12:10:43,899: INFO: main: Training : batch 1053 Loss: 0.009122453577493382]
[2024-04-17 12:10:44,533: INFO: main: Training : batch 1054 Loss: 0.012078733764498351]
[2024-04-17 12:10:45,165: INFO: main: Training : batch 1055 Loss: 0.00567914666889166]
[2024-04-17 12:10:45,797: INFO: main: Training : batch 1056 Loss: 0.015527194675431058]
[2024-04-17 12:10:46,428: INFO: main: Training : batch 1057 Loss: 0.009525241971417221]
[2024-04-17 12:10:47,054: INFO: main: Training : batch 1058 Loss: 0.015868913623099373]
[2024-04-17 12:10:47,686: INFO: main: Training : batch 1059 Loss: 0.02068110388056532]
[2024-04-17 12:10:48,315: INFO: main: Training : batch 1060 Loss: 0.011733053921084341]
[2024-04-17 12:10:48,949: INFO: main: Training : batch 1061 Loss: 0.011846658129399272]
[2024-04-17 12:10:49,581: INFO: main: Training : batch 1062 Loss: 0.04211627060944093]
[2024-04-17 12:10:50,215: INFO: main: Training : batch 1063 Loss: 0.007348823254363031]
[2024-04-17 12:10:50,841: INFO: main: Training : batch 1064 Loss: 0.007972218122152572]
[2024-04-17 12:10:51,469: INFO: main: Training : batch 1065 Loss: 0.01089925423080255]
[2024-04-17 12:10:52,101: INFO: main: Training : batch 1066 Loss: 0.01355544815313751]
[2024-04-17 12:10:52,733: INFO: main: Training : batch 1067 Loss: 0.011416560295681485]
[2024-04-17 12:10:53,365: INFO: main: Training : batch 1068 Loss: 0.020475036235366176]
[2024-04-17 12:10:54,008: INFO: main: Training : batch 1069 Loss: 0.043223488299577756]
[2024-04-17 12:10:54,643: INFO: main: Training : batch 1070 Loss: 0.010457313538385674]
[2024-04-17 12:10:55,284: INFO: main: Training : batch 1071 Loss: 0.006100235565957052]
[2024-04-17 12:10:55,914: INFO: main: Training : batch 1072 Loss: 0.021288532654835535]
[2024-04-17 12:10:56,547: INFO: main: Training : batch 1073 Loss: 0.01598010655617808]
[2024-04-17 12:10:57,174: INFO: main: Training : batch 1074 Loss: 0.010283984630344632]
[2024-04-17 12:10:57,803: INFO: main: Training : batch 1075 Loss: 0.03436527587241516]
[2024-04-17 12:10:58,434: INFO: main: Training : batch 1076 Loss: 0.023771388075677153]
[2024-04-17 12:10:59,061: INFO: main: Training : batch 1077 Loss: 0.04913471063099444]
[2024-04-17 12:10:59,688: INFO: main: Training : batch 1078 Loss: 0.035769813184833216]
[2024-04-17 12:11:00,312: INFO: main: Training : batch 1079 Loss: 0.004074074697227516]
[2024-04-17 12:11:00,943: INFO: main: Training : batch 1080 Loss: 0.011441440557068468]
[2024-04-17 12:11:01,567: INFO: main: Training : batch 1081 Loss: 0.027182010351808977]
[2024-04-17 12:11:02,201: INFO: main: Training : batch 1082 Loss: 0.007761852780413072]
[2024-04-17 12:11:02,832: INFO: main: Training : batch 1083 Loss: 0.00815436652374834]
[2024-04-17 12:11:03,464: INFO: main: Training : batch 1084 Loss: 0.06285800122704087]
[2024-04-17 12:11:04,099: INFO: main: Training : batch 1085 Loss: 0.018207769001211693]
[2024-04-17 12:11:04,729: INFO: main: Training : batch 1086 Loss: 0.006435427006933505]
[2024-04-17 12:11:05,358: INFO: main: Training : batch 1087 Loss: 0.009601520297240757]
[2024-04-17 12:11:05,990: INFO: main: Training : batch 1088 Loss: 0.014556462539123391]
[2024-04-17 12:11:06,631: INFO: main: Training : batch 1089 Loss: 0.007920009940501281]
[2024-04-17 12:11:07,267: INFO: main: Training : batch 1090 Loss: 0.012577972090526924]
[2024-04-17 12:11:07,906: INFO: main: Training : batch 1091 Loss: 0.01370151208202194]
[2024-04-17 12:11:08,539: INFO: main: Training : batch 1092 Loss: 0.013712475551528481]
[2024-04-17 12:11:09,175: INFO: main: Training : batch 1093 Loss: 0.004718752139495756]
[2024-04-17 12:11:09,813: INFO: main: Training : batch 1094 Loss: 0.034279829922975645]
[2024-04-17 12:11:10,440: INFO: main: Training : batch 1095 Loss: 0.004632528595051754]
[2024-04-17 12:11:11,072: INFO: main: Training : batch 1096 Loss: 0.015173843931426133]
[2024-04-17 12:11:11,701: INFO: main: Training : batch 1097 Loss: 0.016240863767722685]
[2024-04-17 12:11:12,335: INFO: main: Training : batch 1098 Loss: 0.015004209250705077]
[2024-04-17 12:11:12,968: INFO: main: Training : batch 1099 Loss: 0.008580569421151343]
[2024-04-17 12:11:13,598: INFO: main: Training : batch 1100 Loss: 0.008427225971445863]
[2024-04-17 12:11:14,229: INFO: main: Training : batch 1101 Loss: 0.05572790128884911]
[2024-04-17 12:11:14,863: INFO: main: Training : batch 1102 Loss: 0.03891301965422451]
[2024-04-17 12:11:15,492: INFO: main: Training : batch 1103 Loss: 0.020342806763797308]
[2024-04-17 12:11:16,117: INFO: main: Training : batch 1104 Loss: 0.013297418961117512]
[2024-04-17 12:11:16,746: INFO: main: Training : batch 1105 Loss: 0.007349375005952263]
[2024-04-17 12:11:17,374: INFO: main: Training : batch 1106 Loss: 0.01238373518212198]
[2024-04-17 12:11:18,008: INFO: main: Training : batch 1107 Loss: 0.022528578087973827]
[2024-04-17 12:11:18,640: INFO: main: Training : batch 1108 Loss: 0.020888935551489178]
[2024-04-17 12:11:19,268: INFO: main: Training : batch 1109 Loss: 0.008118770349653389]
[2024-04-17 12:11:19,902: INFO: main: Training : batch 1110 Loss: 0.0152023428619196]
[2024-04-17 12:11:20,534: INFO: main: Training : batch 1111 Loss: 0.014284526748470137]
[2024-04-17 12:11:21,171: INFO: main: Training : batch 1112 Loss: 0.004999106579863196]
[2024-04-17 12:11:21,806: INFO: main: Training : batch 1113 Loss: 0.01835244189502462]
[2024-04-17 12:11:22,437: INFO: main: Training : batch 1114 Loss: 0.04972770479795271]
[2024-04-17 12:11:23,066: INFO: main: Training : batch 1115 Loss: 0.021383669544675202]
[2024-04-17 12:11:23,700: INFO: main: Training : batch 1116 Loss: 0.026363576302316873]
[2024-04-17 12:11:24,331: INFO: main: Training : batch 1117 Loss: 0.025708632923635236]
[2024-04-17 12:11:24,962: INFO: main: Training : batch 1118 Loss: 0.022074844518456784]
[2024-04-17 12:11:25,594: INFO: main: Training : batch 1119 Loss: 0.027374365780395066]
[2024-04-17 12:11:26,222: INFO: main: Training : batch 1120 Loss: 0.015966601012447495]
[2024-04-17 12:11:26,852: INFO: main: Training : batch 1121 Loss: 0.018712157397695686]
[2024-04-17 12:11:27,482: INFO: main: Training : batch 1122 Loss: 0.01831579665250392]
[2024-04-17 12:11:28,112: INFO: main: Training : batch 1123 Loss: 0.016815304380567147]
[2024-04-17 12:11:28,742: INFO: main: Training : batch 1124 Loss: 0.01821914175776548]
[2024-04-17 12:11:29,371: INFO: main: Training : batch 1125 Loss: 0.006991576653482555]
[2024-04-17 12:11:29,998: INFO: main: Training : batch 1126 Loss: 0.029659786098027358]
[2024-04-17 12:11:30,630: INFO: main: Training : batch 1127 Loss: 0.004342844868853638]
[2024-04-17 12:11:31,256: INFO: main: Training : batch 1128 Loss: 0.011617776048185863]
[2024-04-17 12:11:31,889: INFO: main: Training : batch 1129 Loss: 0.011993473738513595]
[2024-04-17 12:11:32,517: INFO: main: Training : batch 1130 Loss: 0.029786633158130106]
[2024-04-17 12:11:33,151: INFO: main: Training : batch 1131 Loss: 0.026011950957612]
[2024-04-17 12:11:33,796: INFO: main: Training : batch 1132 Loss: 0.014282833833154077]
[2024-04-17 12:11:34,434: INFO: main: Training : batch 1133 Loss: 0.014575298944232115]
[2024-04-17 12:11:35,073: INFO: main: Training : batch 1134 Loss: 0.01806952466404307]
[2024-04-17 12:11:35,715: INFO: main: Training : batch 1135 Loss: 0.015240454235570525]
[2024-04-17 12:11:36,349: INFO: main: Training : batch 1136 Loss: 0.01706677614246454]
[2024-04-17 12:11:36,983: INFO: main: Training : batch 1137 Loss: 0.025609217834172825]
[2024-04-17 12:11:37,615: INFO: main: Training : batch 1138 Loss: 0.03352042431915095]
[2024-04-17 12:11:38,249: INFO: main: Training : batch 1139 Loss: 0.03273189220071675]
[2024-04-17 12:11:38,879: INFO: main: Training : batch 1140 Loss: 0.016442345642947415]
[2024-04-17 12:11:39,510: INFO: main: Training : batch 1141 Loss: 0.014096935474437078]
[2024-04-17 12:11:40,142: INFO: main: Training : batch 1142 Loss: 0.036526896001520204]
[2024-04-17 12:11:40,769: INFO: main: Training : batch 1143 Loss: 0.009206607858187313]
[2024-04-17 12:11:41,401: INFO: main: Training : batch 1144 Loss: 0.01310936709869752]
[2024-04-17 12:11:42,031: INFO: main: Training : batch 1145 Loss: 0.007280790763723538]
[2024-04-17 12:11:42,659: INFO: main: Training : batch 1146 Loss: 0.015049747695783202]
[2024-04-17 12:11:43,285: INFO: main: Training : batch 1147 Loss: 0.018263289351992146]
[2024-04-17 12:11:43,914: INFO: main: Training : batch 1148 Loss: 0.007133967834479397]
[2024-04-17 12:11:44,546: INFO: main: Training : batch 1149 Loss: 0.03963947903903761]
[2024-04-17 12:11:45,180: INFO: main: Training : batch 1150 Loss: 0.008184474571806573]
[2024-04-17 12:11:45,811: INFO: main: Training : batch 1151 Loss: 0.04638801020395598]
[2024-04-17 12:11:46,445: INFO: main: Training : batch 1152 Loss: 0.015881543539693344]
[2024-04-17 12:11:47,077: INFO: main: Training : batch 1153 Loss: 0.012961510791664785]
[2024-04-17 12:11:47,714: INFO: main: Training : batch 1154 Loss: 0.012410460911898053]
[2024-04-17 12:11:48,356: INFO: main: Training : batch 1155 Loss: 0.004698836737042063]
[2024-04-17 12:11:48,998: INFO: main: Training : batch 1156 Loss: 0.00636560201998014]
[2024-04-17 12:11:49,632: INFO: main: Training : batch 1157 Loss: 0.010988476858585923]
[2024-04-17 12:11:50,264: INFO: main: Training : batch 1158 Loss: 0.00787509593028496]
[2024-04-17 12:11:50,894: INFO: main: Training : batch 1159 Loss: 0.018063849539558748]
[2024-04-17 12:11:51,520: INFO: main: Training : batch 1160 Loss: 0.016281249262694572]
[2024-04-17 12:11:52,150: INFO: main: Training : batch 1161 Loss: 0.021624887929780928]
[2024-04-17 12:11:52,777: INFO: main: Training : batch 1162 Loss: 0.008929388741726293]
[2024-04-17 12:11:53,404: INFO: main: Training : batch 1163 Loss: 0.03171697774570531]
[2024-04-17 12:11:54,031: INFO: main: Training : batch 1164 Loss: 0.012274319584276406]
[2024-04-17 12:11:54,660: INFO: main: Training : batch 1165 Loss: 0.011894792293393172]
[2024-04-17 12:11:55,292: INFO: main: Training : batch 1166 Loss: 0.015217478340856327]
[2024-04-17 12:11:55,924: INFO: main: Training : batch 1167 Loss: 0.009693607196179939]
[2024-04-17 12:11:56,553: INFO: main: Training : batch 1168 Loss: 0.010742209925234569]
[2024-04-17 12:11:57,186: INFO: main: Training : batch 1169 Loss: 0.009687050996480446]
[2024-04-17 12:11:57,816: INFO: main: Training : batch 1170 Loss: 0.06904853401896148]
[2024-04-17 12:11:58,449: INFO: main: Training : batch 1171 Loss: 0.055315011322359615]
[2024-04-17 12:11:59,087: INFO: main: Training : batch 1172 Loss: 0.019668314954564843]
[2024-04-17 12:11:59,726: INFO: main: Training : batch 1173 Loss: 0.0018035716392498585]
[2024-04-17 12:12:00,367: INFO: main: Training : batch 1174 Loss: 0.03507498220346943]
[2024-04-17 12:12:01,003: INFO: main: Training : batch 1175 Loss: 0.008520157475636262]
[2024-04-17 12:12:01,644: INFO: main: Training : batch 1176 Loss: 0.013400715603515436]
[2024-04-17 12:12:02,276: INFO: main: Training : batch 1177 Loss: 0.009621111944217147]
[2024-04-17 12:12:02,908: INFO: main: Training : batch 1178 Loss: 0.012079249286343926]
[2024-04-17 12:12:03,539: INFO: main: Training : batch 1179 Loss: 0.006859172335953841]
[2024-04-17 12:12:04,168: INFO: main: Training : batch 1180 Loss: 0.017137937390303244]
[2024-04-17 12:12:04,800: INFO: main: Training : batch 1181 Loss: 0.005206120113564344]
[2024-04-17 12:12:05,431: INFO: main: Training : batch 1182 Loss: 0.012249823257372965]
[2024-04-17 12:12:06,067: INFO: main: Training : batch 1183 Loss: 0.008269334035211554]
[2024-04-17 12:12:06,696: INFO: main: Training : batch 1184 Loss: 0.022237890910489323]
[2024-04-17 12:12:07,322: INFO: main: Training : batch 1185 Loss: 0.010310684435484177]
[2024-04-17 12:12:07,958: INFO: main: Training : batch 1186 Loss: 0.018213158543566366]
[2024-04-17 12:12:08,592: INFO: main: Training : batch 1187 Loss: 0.03686804510670932]
[2024-04-17 12:12:09,220: INFO: main: Training : batch 1188 Loss: 0.011612393235273645]
[2024-04-17 12:12:09,848: INFO: main: Training : batch 1189 Loss: 0.007561935493170992]
[2024-04-17 12:12:10,482: INFO: main: Training : batch 1190 Loss: 0.012838571405582018]
[2024-04-17 12:12:11,112: INFO: main: Training : batch 1191 Loss: 0.007040601371324939]
[2024-04-17 12:12:11,746: INFO: main: Training : batch 1192 Loss: 0.004189147642603008]
[2024-04-17 12:12:12,382: INFO: main: Training : batch 1193 Loss: 0.015352923662738584]
[2024-04-17 12:12:13,017: INFO: main: Training : batch 1194 Loss: 0.017864299091075994]
[2024-04-17 12:12:13,659: INFO: main: Training : batch 1195 Loss: 0.018964464699136204]
[2024-04-17 12:12:14,295: INFO: main: Training : batch 1196 Loss: 0.01752574398242112]
[2024-04-17 12:12:14,937: INFO: main: Training : batch 1197 Loss: 0.010540055322440263]
[2024-04-17 12:12:15,571: INFO: main: Training : batch 1198 Loss: 0.017474693975343358]
[2024-04-17 12:12:16,198: INFO: main: Training : batch 1199 Loss: 0.024255697481028064]
[2024-04-17 12:12:16,824: INFO: main: Training : batch 1200 Loss: 0.01138056956157167]
[2024-04-17 12:12:17,455: INFO: main: Training : batch 1201 Loss: 0.04424797120350009]
[2024-04-17 12:12:18,085: INFO: main: Training : batch 1202 Loss: 0.024082151326159658]
[2024-04-17 12:12:18,718: INFO: main: Training : batch 1203 Loss: 0.010925403170380148]
[2024-04-17 12:12:19,348: INFO: main: Training : batch 1204 Loss: 0.016539554929200805]
[2024-04-17 12:12:19,983: INFO: main: Training : batch 1205 Loss: 0.02712170312793845]
[2024-04-17 12:12:20,614: INFO: main: Training : batch 1206 Loss: 0.012306328054710512]
[2024-04-17 12:12:21,244: INFO: main: Training : batch 1207 Loss: 0.02042214807524693]
[2024-04-17 12:12:21,878: INFO: main: Training : batch 1208 Loss: 0.01929488141352232]
[2024-04-17 12:12:22,511: INFO: main: Training : batch 1209 Loss: 0.05566909283682292]
[2024-04-17 12:12:23,145: INFO: main: Training : batch 1210 Loss: 0.011456261370883613]
[2024-04-17 12:12:23,775: INFO: main: Training : batch 1211 Loss: 0.011154469203666363]
[2024-04-17 12:12:24,404: INFO: main: Training : batch 1212 Loss: 0.011253202643798858]
[2024-04-17 12:12:25,038: INFO: main: Training : batch 1213 Loss: 0.012670916053679258]
[2024-04-17 12:12:25,678: INFO: main: Training : batch 1214 Loss: 0.009781320824311409]
[2024-04-17 12:12:26,316: INFO: main: Training : batch 1215 Loss: 0.013215625058970153]
[2024-04-17 12:12:26,954: INFO: main: Training : batch 1216 Loss: 0.022720816093562708]
[2024-04-17 12:12:27,587: INFO: main: Training : batch 1217 Loss: 0.027220883598378293]
[2024-04-17 12:12:28,231: INFO: main: Training : batch 1218 Loss: 0.05176897357716412]
[2024-04-17 12:12:28,867: INFO: main: Training : batch 1219 Loss: 0.009782002265789005]
[2024-04-17 12:12:29,495: INFO: main: Training : batch 1220 Loss: 0.01286728033664084]
[2024-04-17 12:12:30,123: INFO: main: Training : batch 1221 Loss: 0.010663630608325944]
[2024-04-17 12:12:30,757: INFO: main: Training : batch 1222 Loss: 0.006014547416020779]
[2024-04-17 12:12:31,387: INFO: main: Training : batch 1223 Loss: 0.016226948104524175]
[2024-04-17 12:12:32,017: INFO: main: Training : batch 1224 Loss: 0.02019055953267148]
[2024-04-17 12:12:32,647: INFO: main: Training : batch 1225 Loss: 0.016612761224619466]
[2024-04-17 12:12:33,283: INFO: main: Training : batch 1226 Loss: 0.02835133576720624]
[2024-04-17 12:12:33,911: INFO: main: Training : batch 1227 Loss: 0.021776653387299425]
[2024-04-17 12:12:34,547: INFO: main: Training : batch 1228 Loss: 0.016830199057917765]
[2024-04-17 12:12:35,182: INFO: main: Training : batch 1229 Loss: 0.07355943002920402]
[2024-04-17 12:12:35,813: INFO: main: Training : batch 1230 Loss: 0.008937779424495204]
[2024-04-17 12:12:36,441: INFO: main: Training : batch 1231 Loss: 0.007597254730643795]
[2024-04-17 12:12:37,072: INFO: main: Training : batch 1232 Loss: 0.015471714282870783]
[2024-04-17 12:12:37,702: INFO: main: Training : batch 1233 Loss: 0.008981942624885225]
[2024-04-17 12:12:38,332: INFO: main: Training : batch 1234 Loss: 0.02328990740259366]
[2024-04-17 12:12:38,970: INFO: main: Training : batch 1235 Loss: 0.04157234609450318]
[2024-04-17 12:12:39,612: INFO: main: Training : batch 1236 Loss: 0.0249761914195865]
[2024-04-17 12:12:40,255: INFO: main: Training : batch 1237 Loss: 0.018557194305356583]
[2024-04-17 12:12:40,891: INFO: main: Training : batch 1238 Loss: 0.01310447481328679]
[2024-04-17 12:12:41,528: INFO: main: Training : batch 1239 Loss: 0.02857335974541069]
[2024-04-17 12:12:42,157: INFO: main: Training : batch 1240 Loss: 0.018946472219597058]
[2024-04-17 12:12:42,788: INFO: main: Training : batch 1241 Loss: 0.002631552780808026]
[2024-04-17 12:12:43,422: INFO: main: Training : batch 1242 Loss: 0.07008963630336809]
[2024-04-17 12:12:44,052: INFO: main: Training : batch 1243 Loss: 0.03481626248398236]
[2024-04-17 12:12:44,684: INFO: main: Training : batch 1244 Loss: 0.02366273486862253]
[2024-04-17 12:12:45,314: INFO: main: Training : batch 1245 Loss: 0.0072830893758737145]
[2024-04-17 12:12:45,942: INFO: main: Training : batch 1246 Loss: 0.017259942855019653]
[2024-04-17 12:12:46,575: INFO: main: Training : batch 1247 Loss: 0.00912393464999514]
[2024-04-17 12:12:47,205: INFO: main: Training : batch 1248 Loss: 0.016422314589612075]
[2024-04-17 12:12:47,839: INFO: main: Training : batch 1249 Loss: 0.049744291140296486]
[2024-04-17 12:12:48,472: INFO: main: Training : batch 1250 Loss: 0.02158493022203697]
[2024-04-17 12:12:49,103: INFO: main: Training : batch 1251 Loss: 0.0051434425474421695]
[2024-04-17 12:12:49,736: INFO: main: Training : batch 1252 Loss: 0.012676042798258711]
[2024-04-17 12:12:50,365: INFO: main: Training : batch 1253 Loss: 0.019252457068242662]
[2024-04-17 12:12:51,000: INFO: main: Training : batch 1254 Loss: 0.014128665853614837]
[2024-04-17 12:12:51,638: INFO: main: Training : batch 1255 Loss: 0.009788474108140831]
[2024-04-17 12:12:52,278: INFO: main: Training : batch 1256 Loss: 0.018461544455231755]
[2024-04-17 12:12:52,917: INFO: main: Training : batch 1257 Loss: 0.008504473883542717]
[2024-04-17 12:12:53,556: INFO: main: Training : batch 1258 Loss: 0.007866284260652558]
[2024-04-17 12:12:54,196: INFO: main: Training : batch 1259 Loss: 0.02433584605724168]
[2024-04-17 12:12:54,838: INFO: main: Training : batch 1260 Loss: 0.008993961450349983]
[2024-04-17 12:12:55,470: INFO: main: Training : batch 1261 Loss: 0.006613574282907655]
[2024-04-17 12:12:56,100: INFO: main: Training : batch 1262 Loss: 0.004608259180802377]
[2024-04-17 12:12:56,731: INFO: main: Training : batch 1263 Loss: 0.009348012876962618]
[2024-04-17 12:12:57,359: INFO: main: Training : batch 1264 Loss: 0.007424959462246971]
[2024-04-17 12:12:57,992: INFO: main: Training : batch 1265 Loss: 0.014754714083032913]
[2024-04-17 12:12:58,622: INFO: main: Training : batch 1266 Loss: 0.01017329444790983]
[2024-04-17 12:12:59,256: INFO: main: Training : batch 1267 Loss: 0.022418908995080398]
[2024-04-17 12:12:59,883: INFO: main: Training : batch 1268 Loss: 0.009078676451008448]
[2024-04-17 12:13:00,519: INFO: main: Training : batch 1269 Loss: 0.030055432948231577]
[2024-04-17 12:13:01,152: INFO: main: Training : batch 1270 Loss: 0.033092602734167385]
[2024-04-17 12:13:01,785: INFO: main: Training : batch 1271 Loss: 0.013157129877470093]
[2024-04-17 12:13:02,413: INFO: main: Training : batch 1272 Loss: 0.010323151101562537]
[2024-04-17 12:13:03,044: INFO: main: Training : batch 1273 Loss: 0.014759922676230705]
[2024-04-17 12:13:03,672: INFO: main: Training : batch 1274 Loss: 0.02246859207959065]
[2024-04-17 12:13:04,293: INFO: main: Training : batch 1275 Loss: 0.01694260629928886]
[2024-04-17 12:13:04,927: INFO: main: Training : batch 1276 Loss: 0.01131274026628803]
[2024-04-17 12:13:05,567: INFO: main: Training : batch 1277 Loss: 0.026087774151643813]
[2024-04-17 12:13:06,212: INFO: main: Training : batch 1278 Loss: 0.009606021401315805]
[2024-04-17 12:13:06,853: INFO: main: Training : batch 1279 Loss: 0.011293385579196779]
[2024-04-17 12:13:07,492: INFO: main: Training : batch 1280 Loss: 0.023591882242554128]
[2024-04-17 12:13:08,125: INFO: main: Training : batch 1281 Loss: 0.030737988031166383]
[2024-04-17 12:13:08,757: INFO: main: Training : batch 1282 Loss: 0.024365333569774113]
[2024-04-17 12:13:09,392: INFO: main: Training : batch 1283 Loss: 0.058128561594275506]
[2024-04-17 12:13:10,025: INFO: main: Training : batch 1284 Loss: 0.010035114388288918]
[2024-04-17 12:13:10,653: INFO: main: Training : batch 1285 Loss: 0.006378433226529298]
[2024-04-17 12:13:11,279: INFO: main: Training : batch 1286 Loss: 0.01584169407761947]
[2024-04-17 12:13:11,907: INFO: main: Training : batch 1287 Loss: 0.009988743585919412]
[2024-04-17 12:13:12,543: INFO: main: Training : batch 1288 Loss: 0.04281824933087508]
[2024-04-17 12:13:13,176: INFO: main: Training : batch 1289 Loss: 0.02141933385861542]
[2024-04-17 12:13:13,803: INFO: main: Training : batch 1290 Loss: 0.015426621669601362]
[2024-04-17 12:13:14,430: INFO: main: Training : batch 1291 Loss: 0.01620197758579507]
[2024-04-17 12:13:15,063: INFO: main: Training : batch 1292 Loss: 0.018101698609181094]
[2024-04-17 12:13:15,690: INFO: main: Training : batch 1293 Loss: 0.013342775457656572]
[2024-04-17 12:13:16,325: INFO: main: Training : batch 1294 Loss: 0.007103843575179566]
[2024-04-17 12:13:16,956: INFO: main: Training : batch 1295 Loss: 0.027918291949860355]
[2024-04-17 12:13:17,585: INFO: main: Training : batch 1296 Loss: 0.024084984585755614]
[2024-04-17 12:13:18,226: INFO: main: Training : batch 1297 Loss: 0.03871484231633937]
[2024-04-17 12:13:18,863: INFO: main: Training : batch 1298 Loss: 0.012704465447827998]
[2024-04-17 12:13:19,504: INFO: main: Training : batch 1299 Loss: 0.0715572862560475]
[2024-04-17 12:13:20,142: INFO: main: Training : batch 1300 Loss: 0.0057050400755729565]
[2024-04-17 12:13:20,784: INFO: main: Training : batch 1301 Loss: 0.013881765709657901]
[2024-04-17 12:13:21,414: INFO: main: Training : batch 1302 Loss: 0.012842534331633978]
[2024-04-17 12:13:22,044: INFO: main: Training : batch 1303 Loss: 0.01762397564688643]
[2024-04-17 12:13:22,673: INFO: main: Training : batch 1304 Loss: 0.014665191713210532]
[2024-04-17 12:13:23,307: INFO: main: Training : batch 1305 Loss: 0.02009836017914841]
[2024-04-17 12:13:23,942: INFO: main: Training : batch 1306 Loss: 0.0067062029027003265]
[2024-04-17 12:13:24,570: INFO: main: Training : batch 1307 Loss: 0.019783799363260038]
[2024-04-17 12:13:25,200: INFO: main: Training : batch 1308 Loss: 0.004115396899896298]
[2024-04-17 12:13:25,832: INFO: main: Training : batch 1309 Loss: 0.031577864750356836]
[2024-04-17 12:13:26,460: INFO: main: Training : batch 1310 Loss: 0.006490108322192208]
[2024-04-17 12:13:27,090: INFO: main: Training : batch 1311 Loss: 0.004650621044092788]
[2024-04-17 12:13:27,718: INFO: main: Training : batch 1312 Loss: 0.0045965764369252935]
[2024-04-17 12:13:28,345: INFO: main: Training : batch 1313 Loss: 0.028212058088956403]
[2024-04-17 12:13:28,976: INFO: main: Training : batch 1314 Loss: 0.019621336220430826]
[2024-04-17 12:13:29,607: INFO: main: Training : batch 1315 Loss: 0.022980168090325583]
[2024-04-17 12:13:30,239: INFO: main: Training : batch 1316 Loss: 0.0075104143160389155]
[2024-04-17 12:13:30,872: INFO: main: Training : batch 1317 Loss: 0.01691353399620128]
[2024-04-17 12:13:31,510: INFO: main: Training : batch 1318 Loss: 0.009558238877817949]
[2024-04-17 12:13:32,145: INFO: main: Training : batch 1319 Loss: 0.010439484309323347]
[2024-04-17 12:13:32,784: INFO: main: Training : batch 1320 Loss: 0.01829568605799647]
[2024-04-17 12:13:33,422: INFO: main: Training : batch 1321 Loss: 0.022013179278809792]
[2024-04-17 12:13:34,057: INFO: main: Training : batch 1322 Loss: 0.021695632455376607]
[2024-04-17 12:13:34,687: INFO: main: Training : batch 1323 Loss: 0.011441598967399214]
[2024-04-17 12:13:35,315: INFO: main: Training : batch 1324 Loss: 0.03423913522297501]
[2024-04-17 12:13:35,945: INFO: main: Training : batch 1325 Loss: 0.01134038532150898]
[2024-04-17 12:13:36,576: INFO: main: Training : batch 1326 Loss: 0.0065886409138719345]
[2024-04-17 12:13:37,203: INFO: main: Training : batch 1327 Loss: 0.020507703012429573]
[2024-04-17 12:13:37,833: INFO: main: Training : batch 1328 Loss: 0.016735126425945043]
[2024-04-17 12:13:38,467: INFO: main: Training : batch 1329 Loss: 0.0177312927739075]
[2024-04-17 12:13:39,104: INFO: main: Training : batch 1330 Loss: 0.004529734200656286]
[2024-04-17 12:13:39,733: INFO: main: Training : batch 1331 Loss: 0.012390977445112913]
[2024-04-17 12:13:40,367: INFO: main: Training : batch 1332 Loss: 0.002881048225731517]
[2024-04-17 12:13:40,999: INFO: main: Training : batch 1333 Loss: 0.05878136293163983]
[2024-04-17 12:13:41,629: INFO: main: Training : batch 1334 Loss: 0.010499625897140754]
[2024-04-17 12:13:42,258: INFO: main: Training : batch 1335 Loss: 0.04015565633551542]
[2024-04-17 12:13:42,889: INFO: main: Training : batch 1336 Loss: 0.008374797382484563]
[2024-04-17 12:13:43,521: INFO: main: Training : batch 1337 Loss: 0.008195840939731537]
[2024-04-17 12:13:44,148: INFO: main: Training : batch 1338 Loss: 0.01055838859088642]
[2024-04-17 12:13:44,787: INFO: main: Training : batch 1339 Loss: 0.01114236260180537]
[2024-04-17 12:13:45,427: INFO: main: Training : batch 1340 Loss: 0.01914757426381952]
[2024-04-17 12:13:46,062: INFO: main: Training : batch 1341 Loss: 0.018345071478407055]
[2024-04-17 12:13:46,700: INFO: main: Training : batch 1342 Loss: 0.011482262676934843]
[2024-04-17 12:13:47,340: INFO: main: Training : batch 1343 Loss: 0.008289214032384876]
[2024-04-17 12:13:47,972: INFO: main: Training : batch 1344 Loss: 0.016093481678573697]
[2024-04-17 12:13:48,603: INFO: main: Training : batch 1345 Loss: 0.01968408744140595]
[2024-04-17 12:13:49,234: INFO: main: Training : batch 1346 Loss: 0.01353186083635172]
[2024-04-17 12:13:49,862: INFO: main: Training : batch 1347 Loss: 0.004469865553966819]
[2024-04-17 12:13:50,495: INFO: main: Training : batch 1348 Loss: 0.03692245881671226]
[2024-04-17 12:13:51,119: INFO: main: Training : batch 1349 Loss: 0.006635400212131023]
[2024-04-17 12:13:51,749: INFO: main: Training : batch 1350 Loss: 0.04997514129674901]
[2024-04-17 12:13:52,375: INFO: main: Training : batch 1351 Loss: 0.01825469758837776]
[2024-04-17 12:13:53,006: INFO: main: Training : batch 1352 Loss: 0.01114113795069733]
[2024-04-17 12:13:53,637: INFO: main: Training : batch 1353 Loss: 0.02259649663390144]
[2024-04-17 12:13:54,269: INFO: main: Training : batch 1354 Loss: 0.03982445190117937]
[2024-04-17 12:13:54,896: INFO: main: Training : batch 1355 Loss: 0.00159299996070498]
[2024-04-17 12:13:55,526: INFO: main: Training : batch 1356 Loss: 0.008218353685812743]
[2024-04-17 12:13:56,159: INFO: main: Training : batch 1357 Loss: 0.008468902114579766]
[2024-04-17 12:13:56,789: INFO: main: Training : batch 1358 Loss: 0.018232779114232437]
[2024-04-17 12:13:57,417: INFO: main: Training : batch 1359 Loss: 0.020831831196961713]
[2024-04-17 12:13:58,056: INFO: main: Training : batch 1360 Loss: 0.006487751604217963]
[2024-04-17 12:13:58,692: INFO: main: Training : batch 1361 Loss: 0.006012944364591917]
[2024-04-17 12:13:59,332: INFO: main: Training : batch 1362 Loss: 0.008931467064205037]
[2024-04-17 12:13:59,965: INFO: main: Training : batch 1363 Loss: 0.008799274017380867]
[2024-04-17 12:14:00,605: INFO: main: Training : batch 1364 Loss: 0.01320130491220861]
[2024-04-17 12:14:01,233: INFO: main: Training : batch 1365 Loss: 0.011871614108932182]
[2024-04-17 12:14:01,864: INFO: main: Training : batch 1366 Loss: 0.016450324612423232]
[2024-04-17 12:14:02,498: INFO: main: Training : batch 1367 Loss: 0.01303021874119608]
[2024-04-17 12:14:03,131: INFO: main: Training : batch 1368 Loss: 0.010345013396777916]
[2024-04-17 12:14:03,760: INFO: main: Training : batch 1369 Loss: 0.020454902286483778]
[2024-04-17 12:14:04,395: INFO: main: Training : batch 1370 Loss: 0.0014188766172368735]
[2024-04-17 12:14:05,029: INFO: main: Training : batch 1371 Loss: 0.025045291562386832]
[2024-04-17 12:14:05,659: INFO: main: Training : batch 1372 Loss: 0.029584730810617986]
[2024-04-17 12:14:06,285: INFO: main: Training : batch 1373 Loss: 0.005032456248172503]
[2024-04-17 12:14:06,915: INFO: main: Training : batch 1374 Loss: 0.010981080824614813]
[2024-04-17 12:14:07,543: INFO: main: Training : batch 1375 Loss: 0.020139947489947973]
[2024-04-17 12:14:08,172: INFO: main: Training : batch 1376 Loss: 0.009786834565171236]
[2024-04-17 12:14:08,801: INFO: main: Training : batch 1377 Loss: 0.02483785565486574]
[2024-04-17 12:14:09,434: INFO: main: Training : batch 1378 Loss: 0.016031320391006996]
[2024-04-17 12:14:10,061: INFO: main: Training : batch 1379 Loss: 0.010024305622007288]
[2024-04-17 12:14:10,689: INFO: main: Training : batch 1380 Loss: 0.015876278051705838]
[2024-04-17 12:14:11,328: INFO: main: Training : batch 1381 Loss: 0.01991953567031672]
[2024-04-17 12:14:11,960: INFO: main: Training : batch 1382 Loss: 0.008893112864725886]
[2024-04-17 12:14:12,596: INFO: main: Training : batch 1383 Loss: 0.01845766420918832]
[2024-04-17 12:14:13,226: INFO: main: Training : batch 1384 Loss: 0.014215516435752097]
[2024-04-17 12:14:13,866: INFO: main: Training : batch 1385 Loss: 0.018300666888720767]
[2024-04-17 12:14:14,500: INFO: main: Training : batch 1386 Loss: 0.00886040976963796]
[2024-04-17 12:14:15,131: INFO: main: Training : batch 1387 Loss: 0.0035810178981873853]
[2024-04-17 12:14:15,764: INFO: main: Training : batch 1388 Loss: 0.005350362709877194]
[2024-04-17 12:14:16,389: INFO: main: Training : batch 1389 Loss: 0.02447630145795929]
[2024-04-17 12:14:17,024: INFO: main: Training : batch 1390 Loss: 0.0064301561650797565]
[2024-04-17 12:14:17,655: INFO: main: Training : batch 1391 Loss: 0.006097169168080354]
[2024-04-17 12:14:18,290: INFO: main: Training : batch 1392 Loss: 0.00668577428219882]
[2024-04-17 12:14:18,922: INFO: main: Training : batch 1393 Loss: 0.0066969206565099925]
[2024-04-17 12:14:19,554: INFO: main: Training : batch 1394 Loss: 0.029745849398966497]
[2024-04-17 12:14:20,185: INFO: main: Training : batch 1395 Loss: 0.016430401496242614]
[2024-04-17 12:14:20,815: INFO: main: Training : batch 1396 Loss: 0.008438693479452817]
[2024-04-17 12:14:21,446: INFO: main: Training : batch 1397 Loss: 0.0075989588019534345]
[2024-04-17 12:14:22,080: INFO: main: Training : batch 1398 Loss: 0.013106308898913469]
[2024-04-17 12:14:22,708: INFO: main: Training : batch 1399 Loss: 0.002515591206697591]
[2024-04-17 12:14:23,344: INFO: main: Training : batch 1400 Loss: 0.005266737190108481]
[2024-04-17 12:14:23,973: INFO: main: Training : batch 1401 Loss: 0.018050246983568552]
[2024-04-17 12:14:24,609: INFO: main: Training : batch 1402 Loss: 0.018034914788275277]
[2024-04-17 12:14:25,242: INFO: main: Training : batch 1403 Loss: 0.02607794514106688]
[2024-04-17 12:14:25,879: INFO: main: Training : batch 1404 Loss: 0.006037303143223032]
[2024-04-17 12:14:26,523: INFO: main: Training : batch 1405 Loss: 0.02869125251112922]
[2024-04-17 12:14:27,159: INFO: main: Training : batch 1406 Loss: 0.02779708745760258]
[2024-04-17 12:14:27,790: INFO: main: Training : batch 1407 Loss: 0.03130489734484118]
[2024-04-17 12:14:28,417: INFO: main: Training : batch 1408 Loss: 0.01659903243195911]
[2024-04-17 12:14:29,045: INFO: main: Training : batch 1409 Loss: 0.023980563826516907]
[2024-04-17 12:14:29,672: INFO: main: Training : batch 1410 Loss: 0.013195724198915307]
[2024-04-17 12:14:30,298: INFO: main: Training : batch 1411 Loss: 0.028337789932553866]
[2024-04-17 12:14:30,928: INFO: main: Training : batch 1412 Loss: 0.009204092538794098]
[2024-04-17 12:14:31,556: INFO: main: Training : batch 1413 Loss: 0.007780320237324581]
[2024-04-17 12:14:32,179: INFO: main: Training : batch 1414 Loss: 0.02513504224079383]
[2024-04-17 12:14:32,809: INFO: main: Training : batch 1415 Loss: 0.011166228117381338]
[2024-04-17 12:14:33,440: INFO: main: Training : batch 1416 Loss: 0.03369655795428408]
[2024-04-17 12:14:34,071: INFO: main: Training : batch 1417 Loss: 0.01279107579770277]
[2024-04-17 12:14:34,703: INFO: main: Training : batch 1418 Loss: 0.0377018122486]
[2024-04-17 12:14:35,335: INFO: main: Training : batch 1419 Loss: 0.02037805948851842]
[2024-04-17 12:14:35,970: INFO: main: Training : batch 1420 Loss: 0.019163357564225367]
[2024-04-17 12:14:36,598: INFO: main: Training : batch 1421 Loss: 0.013021645976739459]
[2024-04-17 12:14:37,233: INFO: main: Training : batch 1422 Loss: 0.0071240676820171185]
[2024-04-17 12:14:37,868: INFO: main: Training : batch 1423 Loss: 0.007943940449675845]
[2024-04-17 12:14:38,510: INFO: main: Training : batch 1424 Loss: 0.016856161472631994]
[2024-04-17 12:14:39,154: INFO: main: Training : batch 1425 Loss: 0.01736051286578066]
[2024-04-17 12:14:39,790: INFO: main: Training : batch 1426 Loss: 0.009615786583948094]
[2024-04-17 12:14:40,429: INFO: main: Training : batch 1427 Loss: 0.016325051903916657]
[2024-04-17 12:14:41,057: INFO: main: Training : batch 1428 Loss: 0.04294503594435897]
[2024-04-17 12:14:41,686: INFO: main: Training : batch 1429 Loss: 0.019992737264940857]
[2024-04-17 12:14:41,887: INFO: main: Eval Epoch : batch 0 Loss: 0.00646468949927575]
[2024-04-17 12:14:42,087: INFO: main: Eval Epoch : batch 1 Loss: 0.004573792470099602]
[2024-04-17 12:14:42,286: INFO: main: Eval Epoch : batch 2 Loss: 0.04010519273161749]
[2024-04-17 12:14:42,483: INFO: main: Eval Epoch : batch 3 Loss: 0.01197679430181773]
[2024-04-17 12:14:42,687: INFO: main: Eval Epoch : batch 4 Loss: 0.007100019541804589]
[2024-04-17 12:14:42,887: INFO: main: Eval Epoch : batch 5 Loss: 0.01701577791602463]
[2024-04-17 12:14:43,094: INFO: main: Eval Epoch : batch 6 Loss: 0.014477958992600772]
[2024-04-17 12:14:43,295: INFO: main: Eval Epoch : batch 7 Loss: 0.008758222242055129]
[2024-04-17 12:14:43,493: INFO: main: Eval Epoch : batch 8 Loss: 0.018060681969081976]
[2024-04-17 12:14:43,695: INFO: main: Eval Epoch : batch 9 Loss: 0.022723140459661072]
[2024-04-17 12:14:43,895: INFO: main: Eval Epoch : batch 10 Loss: 0.010545980563193898]
[2024-04-17 12:14:44,095: INFO: main: Eval Epoch : batch 11 Loss: 0.042918676263259646]
[2024-04-17 12:14:44,297: INFO: main: Eval Epoch : batch 12 Loss: 0.012513009457187169]
[2024-04-17 12:14:44,495: INFO: main: Eval Epoch : batch 13 Loss: 0.013015240959933316]
[2024-04-17 12:14:44,697: INFO: main: Eval Epoch : batch 14 Loss: 0.00858283042450448]
[2024-04-17 12:14:44,895: INFO: main: Eval Epoch : batch 15 Loss: 0.03466357531626291]
[2024-04-17 12:14:45,094: INFO: main: Eval Epoch : batch 16 Loss: 0.018014908341912054]
[2024-04-17 12:14:45,295: INFO: main: Eval Epoch : batch 17 Loss: 0.025746119525658414]
[2024-04-17 12:14:45,499: INFO: main: Eval Epoch : batch 18 Loss: 0.018194570203782404]
[2024-04-17 12:14:45,702: INFO: main: Eval Epoch : batch 19 Loss: 0.009193624739609309]
[2024-04-17 12:14:45,903: INFO: main: Eval Epoch : batch 20 Loss: 0.005444532892534968]
[2024-04-17 12:14:46,103: INFO: main: Eval Epoch : batch 21 Loss: 0.009759780810794106]
[2024-04-17 12:14:46,305: INFO: main: Eval Epoch : batch 22 Loss: 0.017845631554374346]
[2024-04-17 12:14:46,509: INFO: main: Eval Epoch : batch 23 Loss: 0.026242874258024004]
[2024-04-17 12:14:46,712: INFO: main: Eval Epoch : batch 24 Loss: 0.028494615524689966]
[2024-04-17 12:14:46,914: INFO: main: Eval Epoch : batch 25 Loss: 0.013938057863297267]
[2024-04-17 12:14:47,115: INFO: main: Eval Epoch : batch 26 Loss: 0.015432538196669787]
[2024-04-17 12:14:47,315: INFO: main: Eval Epoch : batch 27 Loss: 0.013570598987716948]
[2024-04-17 12:14:47,516: INFO: main: Eval Epoch : batch 28 Loss: 0.01183061586783223]
[2024-04-17 12:14:47,724: INFO: main: Eval Epoch : batch 29 Loss: 0.012260229189120281]
[2024-04-17 12:14:47,925: INFO: main: Eval Epoch : batch 30 Loss: 0.006980318553776687]
[2024-04-17 12:14:48,126: INFO: main: Eval Epoch : batch 31 Loss: 0.02646921053615285]
[2024-04-17 12:14:48,328: INFO: main: Eval Epoch : batch 32 Loss: 0.011323842972514243]
[2024-04-17 12:14:48,532: INFO: main: Eval Epoch : batch 33 Loss: 0.013579717988730595]
[2024-04-17 12:14:48,731: INFO: main: Eval Epoch : batch 34 Loss: 0.03223489997549514]
[2024-04-17 12:14:48,930: INFO: main: Eval Epoch : batch 35 Loss: 0.02074652468242053]
[2024-04-17 12:14:49,127: INFO: main: Eval Epoch : batch 36 Loss: 0.009629989878645441]
[2024-04-17 12:14:49,327: INFO: main: Eval Epoch : batch 37 Loss: 0.020272746275974377]
[2024-04-17 12:14:49,531: INFO: main: Eval Epoch : batch 38 Loss: 0.02231912718593817]
[2024-04-17 12:14:49,738: INFO: main: Eval Epoch : batch 39 Loss: 0.012979473572620146]
[2024-04-17 12:14:49,940: INFO: main: Eval Epoch : batch 40 Loss: 0.019707001175156013]
[2024-04-17 12:14:50,139: INFO: main: Eval Epoch : batch 41 Loss: 0.022695932519724328]
[2024-04-17 12:14:50,339: INFO: main: Eval Epoch : batch 42 Loss: 0.00399063825435474]
[2024-04-17 12:14:50,546: INFO: main: Eval Epoch : batch 43 Loss: 0.01357980251561766]
[2024-04-17 12:14:50,752: INFO: main: Eval Epoch : batch 44 Loss: 0.01812981730380232]
[2024-04-17 12:14:50,955: INFO: main: Eval Epoch : batch 45 Loss: 0.009729474731149497]
[2024-04-17 12:14:51,156: INFO: main: Eval Epoch : batch 46 Loss: 0.009978166389586516]
[2024-04-17 12:14:51,364: INFO: main: Eval Epoch : batch 47 Loss: 0.03691031835649163]
[2024-04-17 12:14:51,570: INFO: main: Eval Epoch : batch 48 Loss: 0.019450443911487092]
[2024-04-17 12:14:51,782: INFO: main: Eval Epoch : batch 49 Loss: 0.005508675666714923]
[2024-04-17 12:14:51,985: INFO: main: Eval Epoch : batch 50 Loss: 0.030069549788603135]
[2024-04-17 12:14:52,185: INFO: main: Eval Epoch : batch 51 Loss: 0.05268424675163191]
[2024-04-17 12:14:52,389: INFO: main: Eval Epoch : batch 52 Loss: 0.0178606534696909]
[2024-04-17 12:14:52,599: INFO: main: Eval Epoch : batch 53 Loss: 0.016899264821299907]
[2024-04-17 12:14:52,806: INFO: main: Eval Epoch : batch 54 Loss: 0.012872359010061219]
[2024-04-17 12:14:53,017: INFO: main: Eval Epoch : batch 55 Loss: 0.026330295667115824]
[2024-04-17 12:14:53,220: INFO: main: Eval Epoch : batch 56 Loss: 0.0021348259340022305]
[2024-04-17 12:14:53,424: INFO: main: Eval Epoch : batch 57 Loss: 0.01734098240398764]
[2024-04-17 12:14:53,631: INFO: main: Eval Epoch : batch 58 Loss: 0.0017121510189030008]
[2024-04-17 12:14:53,838: INFO: main: Eval Epoch : batch 59 Loss: 0.03950463541961575]
[2024-04-17 12:14:54,041: INFO: main: Eval Epoch : batch 60 Loss: 0.006303156619968353]
[2024-04-17 12:14:54,243: INFO: main: Eval Epoch : batch 61 Loss: 0.011313556277869102]
[2024-04-17 12:14:54,446: INFO: main: Eval Epoch : batch 62 Loss: 0.005199431652061323]
[2024-04-17 12:14:54,650: INFO: main: Eval Epoch : batch 63 Loss: 0.008732658852635045]
[2024-04-17 12:14:54,850: INFO: main: Eval Epoch : batch 64 Loss: 0.02233688845752181]
[2024-04-17 12:14:55,051: INFO: main: Eval Epoch : batch 65 Loss: 0.01223110960441119]
[2024-04-17 12:14:55,250: INFO: main: Eval Epoch : batch 66 Loss: 0.0009931112854998398]
[2024-04-17 12:14:55,452: INFO: main: Eval Epoch : batch 67 Loss: 0.011903581626619683]
[2024-04-17 12:14:55,652: INFO: main: Eval Epoch : batch 68 Loss: 0.0591084588201508]
[2024-04-17 12:14:55,853: INFO: main: Eval Epoch : batch 69 Loss: 0.026535860964868378]
[2024-04-17 12:14:56,052: INFO: main: Eval Epoch : batch 70 Loss: 0.0021780184997843705]
[2024-04-17 12:14:56,253: INFO: main: Eval Epoch : batch 71 Loss: 0.012957651616081995]
[2024-04-17 12:14:56,458: INFO: main: Eval Epoch : batch 72 Loss: 0.011971233509456948]
[2024-04-17 12:14:56,660: INFO: main: Eval Epoch : batch 73 Loss: 0.0019279513908852142]
[2024-04-17 12:14:56,863: INFO: main: Eval Epoch : batch 74 Loss: 0.013321905098258377]
[2024-04-17 12:14:57,063: INFO: main: Eval Epoch : batch 75 Loss: 0.04276444933577276]
[2024-04-17 12:14:57,262: INFO: main: Eval Epoch : batch 76 Loss: 0.005958715460845734]
[2024-04-17 12:14:57,463: INFO: main: Eval Epoch : batch 77 Loss: 0.0038657436832362966]
[2024-04-17 12:14:57,663: INFO: main: Eval Epoch : batch 78 Loss: 0.0316854126571346]
[2024-04-17 12:14:57,865: INFO: main: Eval Epoch : batch 79 Loss: 0.012880851768480653]
[2024-04-17 12:14:58,067: INFO: main: Eval Epoch : batch 80 Loss: 0.03282432498230457]
[2024-04-17 12:14:58,265: INFO: main: Eval Epoch : batch 81 Loss: 0.02075156581953273]
[2024-04-17 12:14:58,467: INFO: main: Eval Epoch : batch 82 Loss: 0.007187856370325609]
[2024-04-17 12:14:58,665: INFO: main: Eval Epoch : batch 83 Loss: 0.010211608274789537]
[2024-04-17 12:14:58,867: INFO: main: Eval Epoch : batch 84 Loss: 0.0010563397241234512]
[2024-04-17 12:14:59,067: INFO: main: Eval Epoch : batch 85 Loss: 0.04487485119635203]
[2024-04-17 12:14:59,270: INFO: main: Eval Epoch : batch 86 Loss: 0.01584550799431923]
[2024-04-17 12:14:59,477: INFO: main: Eval Epoch : batch 87 Loss: 0.005266298758199279]
[2024-04-17 12:14:59,678: INFO: main: Eval Epoch : batch 88 Loss: 0.028276224656101218]
[2024-04-17 12:14:59,878: INFO: main: Eval Epoch : batch 89 Loss: 0.016067892041217124]
[2024-04-17 12:15:00,081: INFO: main: Eval Epoch : batch 90 Loss: 0.02407397694643182]
[2024-04-17 12:15:00,281: INFO: main: Eval Epoch : batch 91 Loss: 0.01268238725331846]
[2024-04-17 12:15:00,485: INFO: main: Eval Epoch : batch 92 Loss: 0.00300423158734449]
[2024-04-17 12:15:00,688: INFO: main: Eval Epoch : batch 93 Loss: 0.007039883022432806]
[2024-04-17 12:15:00,889: INFO: main: Eval Epoch : batch 94 Loss: 0.013294941051792745]
[2024-04-17 12:15:01,093: INFO: main: Eval Epoch : batch 95 Loss: 0.015270905326948154]
[2024-04-17 12:15:01,292: INFO: main: Eval Epoch : batch 96 Loss: 0.019927809786023803]
[2024-04-17 12:15:01,493: INFO: main: Eval Epoch : batch 97 Loss: 0.010983953899572307]
[2024-04-17 12:15:01,693: INFO: main: Eval Epoch : batch 98 Loss: 0.011099482011391913]
[2024-04-17 12:15:01,895: INFO: main: Eval Epoch : batch 99 Loss: 0.007686752760970481]
[2024-04-17 12:15:02,096: INFO: main: Eval Epoch : batch 100 Loss: 0.016964371434772633]
[2024-04-17 12:15:02,297: INFO: main: Eval Epoch : batch 101 Loss: 0.010891324994442217]
[2024-04-17 12:15:02,496: INFO: main: Eval Epoch : batch 102 Loss: 0.008646426533307]
[2024-04-17 12:15:02,697: INFO: main: Eval Epoch : batch 103 Loss: 0.0029171561390016138]
[2024-04-17 12:15:02,900: INFO: main: Eval Epoch : batch 104 Loss: 0.02086992847043844]
[2024-04-17 12:15:03,102: INFO: main: Eval Epoch : batch 105 Loss: 0.001699666575006988]
[2024-04-17 12:15:03,303: INFO: main: Eval Epoch : batch 106 Loss: 0.012914401112880246]
[2024-04-17 12:15:03,506: INFO: main: Eval Epoch : batch 107 Loss: 0.003788935909948803]
[2024-04-17 12:15:03,708: INFO: main: Eval Epoch : batch 108 Loss: 0.027544024752520067]
[2024-04-17 12:15:03,913: INFO: main: Eval Epoch : batch 109 Loss: 0.047624147051121224]
[2024-04-17 12:15:04,127: INFO: main: Eval Epoch : batch 110 Loss: 0.006846749983288375]
[2024-04-17 12:15:04,332: INFO: main: Eval Epoch : batch 111 Loss: 0.005900028724211053]
[2024-04-17 12:15:04,536: INFO: main: Eval Epoch : batch 112 Loss: 0.019534939665767255]
[2024-04-17 12:15:04,743: INFO: main: Eval Epoch : batch 113 Loss: 0.0038969935574478397]
[2024-04-17 12:15:04,950: INFO: main: Eval Epoch : batch 114 Loss: 0.02189585481741452]
[2024-04-17 12:15:05,155: INFO: main: Eval Epoch : batch 115 Loss: 0.01579143618272342]
[2024-04-17 12:15:05,360: INFO: main: Eval Epoch : batch 116 Loss: 0.010712859443073038]
[2024-04-17 12:15:05,562: INFO: main: Eval Epoch : batch 117 Loss: 0.006176077294750948]
[2024-04-17 12:15:05,767: INFO: main: Eval Epoch : batch 118 Loss: 0.008573863126512874]
[2024-04-17 12:15:05,973: INFO: main: Eval Epoch : batch 119 Loss: 0.02942463462838067]
[2024-04-17 12:15:06,179: INFO: main: Eval Epoch : batch 120 Loss: 0.0228206174286619]
[2024-04-17 12:15:06,384: INFO: main: Eval Epoch : batch 121 Loss: 0.008987263000424824]
[2024-04-17 12:15:06,588: INFO: main: Eval Epoch : batch 122 Loss: 0.014018650756164182]
[2024-04-17 12:15:06,796: INFO: main: Eval Epoch : batch 123 Loss: 0.024023568339820247]
[2024-04-17 12:15:07,003: INFO: main: Eval Epoch : batch 124 Loss: 0.0035331458060956825]
[2024-04-17 12:15:07,205: INFO: main: Eval Epoch : batch 125 Loss: 0.011489342240394237]
[2024-04-17 12:15:07,407: INFO: main: Eval Epoch : batch 126 Loss: 0.00646183307733084]
[2024-04-17 12:15:07,615: INFO: main: Eval Epoch : batch 127 Loss: 0.0037559531265586557]
[2024-04-17 12:15:07,813: INFO: main: Eval Epoch : batch 128 Loss: 0.011493260773752838]
[2024-04-17 12:15:08,012: INFO: main: Eval Epoch : batch 129 Loss: 0.014496885835798572]
[2024-04-17 12:15:08,213: INFO: main: Eval Epoch : batch 130 Loss: 0.01629815983912091]
[2024-04-17 12:15:08,418: INFO: main: Eval Epoch : batch 131 Loss: 0.012210212594779779]
[2024-04-17 12:15:08,616: INFO: main: Eval Epoch : batch 132 Loss: 0.003226300057179866]
[2024-04-17 12:15:08,813: INFO: main: Eval Epoch : batch 133 Loss: 0.014002049753098728]
[2024-04-17 12:15:09,015: INFO: main: Eval Epoch : batch 134 Loss: 0.014411434956953925]
[2024-04-17 12:15:09,216: INFO: main: Eval Epoch : batch 135 Loss: 0.0017834479156524983]
[2024-04-17 12:15:09,422: INFO: main: Eval Epoch : batch 136 Loss: 0.032285042791240094]
[2024-04-17 12:15:09,623: INFO: main: Eval Epoch : batch 137 Loss: 0.029788003123172705]
[2024-04-17 12:15:09,823: INFO: main: Eval Epoch : batch 138 Loss: 0.00856465500165882]
[2024-04-17 12:15:10,021: INFO: main: Eval Epoch : batch 139 Loss: 0.07305697122631584]
[2024-04-17 12:15:10,222: INFO: main: Eval Epoch : batch 140 Loss: 0.04074302758586286]
[2024-04-17 12:15:10,425: INFO: main: Eval Epoch : batch 141 Loss: 0.014028398072277987]
[2024-04-17 12:15:10,625: INFO: main: Eval Epoch : batch 142 Loss: 0.0019432091842725855]
[2024-04-17 12:15:10,824: INFO: main: Eval Epoch : batch 143 Loss: 0.02491107360039193]
[2024-04-17 12:15:11,025: INFO: main: Eval Epoch : batch 144 Loss: 0.01749159889106042]
[2024-04-17 12:15:11,224: INFO: main: Eval Epoch : batch 145 Loss: 0.017221663948267466]
[2024-04-17 12:15:11,423: INFO: main: Eval Epoch : batch 146 Loss: 0.026081802606264647]
[2024-04-17 12:15:11,626: INFO: main: Eval Epoch : batch 147 Loss: 0.01862994484251405]
[2024-04-17 12:15:11,828: INFO: main: Eval Epoch : batch 148 Loss: 0.009527066190935018]
[2024-04-17 12:15:12,028: INFO: main: Eval Epoch : batch 149 Loss: 0.0062094994868207186]
[2024-04-17 12:15:12,230: INFO: main: Eval Epoch : batch 150 Loss: 0.036322281473427254]
[2024-04-17 12:15:12,433: INFO: main: Eval Epoch : batch 151 Loss: 0.03981558640634177]
[2024-04-17 12:15:12,635: INFO: main: Eval Epoch : batch 152 Loss: 0.004900029318020413]
[2024-04-17 12:15:12,833: INFO: main: Eval Epoch : batch 153 Loss: 0.004143645769873644]
[2024-04-17 12:15:13,034: INFO: main: Eval Epoch : batch 154 Loss: 0.02724197110305041]
[2024-04-17 12:15:13,237: INFO: main: Eval Epoch : batch 155 Loss: 0.04750245126416252]
[2024-04-17 12:15:13,438: INFO: main: Eval Epoch : batch 156 Loss: 0.017542874353143174]
[2024-04-17 12:15:13,639: INFO: main: Eval Epoch : batch 157 Loss: 0.012449099511786544]
[2024-04-17 12:15:13,840: INFO: main: Eval Epoch : batch 158 Loss: 0.015603947554921645]
[2024-04-17 12:15:14,041: INFO: main: Eval Epoch : batch 159 Loss: 0.007928142326415352]
[2024-04-17 12:15:14,240: INFO: main: Eval Epoch : batch 160 Loss: 0.012124923980867985]
[2024-04-17 12:15:14,437: INFO: main: Eval Epoch : batch 161 Loss: 0.035240141167238594]
[2024-04-17 12:15:14,638: INFO: main: Eval Epoch : batch 162 Loss: 0.010206376103383007]
[2024-04-17 12:15:14,841: INFO: main: Eval Epoch : batch 163 Loss: 0.019349109298607284]
[2024-04-17 12:15:15,043: INFO: main: Eval Epoch : batch 164 Loss: 0.03315213277726622]
[2024-04-17 12:15:15,246: INFO: main: Eval Epoch : batch 165 Loss: 0.00925081409317575]
[2024-04-17 12:15:15,450: INFO: main: Eval Epoch : batch 166 Loss: 0.007784105178868848]
[2024-04-17 12:15:15,651: INFO: main: Eval Epoch : batch 167 Loss: 0.003571515879088419]
[2024-04-17 12:15:15,851: INFO: main: Eval Epoch : batch 168 Loss: 0.010957804523195676]
[2024-04-17 12:15:16,052: INFO: main: Eval Epoch : batch 169 Loss: 0.01043809426950507]
[2024-04-17 12:15:16,255: INFO: main: Eval Epoch : batch 170 Loss: 0.00556438409119383]
[2024-04-17 12:15:16,459: INFO: main: Eval Epoch : batch 171 Loss: 0.014839457736870431]
[2024-04-17 12:15:16,659: INFO: main: Eval Epoch : batch 172 Loss: 0.01782645127046484]
[2024-04-17 12:15:16,860: INFO: main: Eval Epoch : batch 173 Loss: 0.0026539475763921088]
[2024-04-17 12:15:17,061: INFO: main: Eval Epoch : batch 174 Loss: 0.0163269264976473]
[2024-04-17 12:15:17,267: INFO: main: Eval Epoch : batch 175 Loss: 0.006987025252359644]
[2024-04-17 12:15:17,471: INFO: main: Eval Epoch : batch 176 Loss: 0.022131171285099595]
[2024-04-17 12:15:17,672: INFO: main: Eval Epoch : batch 177 Loss: 0.00357816720614702]
[2024-04-17 12:15:17,876: INFO: main: Eval Epoch : batch 178 Loss: 0.009012158286273234]
[2024-04-17 12:15:18,081: INFO: main: Eval Epoch : batch 179 Loss: 0.0007398216472507613]
[2024-04-17 12:15:18,286: INFO: main: Eval Epoch : batch 180 Loss: 0.009552588230541581]
[2024-04-17 12:15:18,490: INFO: main: Eval Epoch : batch 181 Loss: 0.005615645437697006]
[2024-04-17 12:15:18,696: INFO: main: Eval Epoch : batch 182 Loss: 0.012728332844171238]
[2024-04-17 12:15:18,903: INFO: main: Eval Epoch : batch 183 Loss: 0.020854196101014227]
[2024-04-17 12:15:19,110: INFO: main: Eval Epoch : batch 184 Loss: 0.023897394274933526]
[2024-04-17 12:15:19,318: INFO: main: Eval Epoch : batch 185 Loss: 0.01369569534008411]
[2024-04-17 12:15:19,520: INFO: main: Eval Epoch : batch 186 Loss: 0.008083634762410996]
[2024-04-17 12:15:19,726: INFO: main: Eval Epoch : batch 187 Loss: 0.041244159308411674]
[2024-04-17 12:15:19,932: INFO: main: Eval Epoch : batch 188 Loss: 0.008757053494948425]
[2024-04-17 12:15:20,137: INFO: main: Eval Epoch : batch 189 Loss: 0.018914274360306172]
[2024-04-17 12:15:20,342: INFO: main: Eval Epoch : batch 190 Loss: 0.004454308853336732]
[2024-04-17 12:15:20,544: INFO: main: Eval Epoch : batch 191 Loss: 0.009292418904148734]
[2024-04-17 12:15:20,745: INFO: main: Eval Epoch : batch 192 Loss: 0.013044201063020545]
[2024-04-17 12:15:20,947: INFO: main: Eval Epoch : batch 193 Loss: 0.0039891662189459965]
[2024-04-17 12:15:21,148: INFO: main: Eval Epoch : batch 194 Loss: 0.04022135247502757]
[2024-04-17 12:15:21,347: INFO: main: Eval Epoch : batch 195 Loss: 0.013928563733625242]
[2024-04-17 12:15:21,548: INFO: main: Eval Epoch : batch 196 Loss: 0.00854009544739374]
[2024-04-17 12:15:21,756: INFO: main: Eval Epoch : batch 197 Loss: 0.011172430584892274]
[2024-04-17 12:15:21,956: INFO: main: Eval Epoch : batch 198 Loss: 0.007424734196051309]
[2024-04-17 12:15:22,155: INFO: main: Eval Epoch : batch 199 Loss: 0.007672159491429691]
[2024-04-17 12:15:22,355: INFO: main: Eval Epoch : batch 200 Loss: 0.004001166543436517]
[2024-04-17 12:15:22,554: INFO: main: Eval Epoch : batch 201 Loss: 0.02593122084623267]
[2024-04-17 12:15:22,757: INFO: main: Eval Epoch : batch 202 Loss: 0.015693044449541994]
[2024-04-17 12:15:22,960: INFO: main: Eval Epoch : batch 203 Loss: 0.012140108964624384]
[2024-04-17 12:15:23,162: INFO: main: Eval Epoch : batch 204 Loss: 0.01680712965187674]
[2024-04-17 12:15:23,364: INFO: main: Eval Epoch : batch 205 Loss: 0.00581576946135827]
[2024-04-17 12:15:23,564: INFO: main: Eval Epoch : batch 206 Loss: 0.0028852230171302904]
[2024-04-17 12:15:23,767: INFO: main: Eval Epoch : batch 207 Loss: 0.002279687432149172]
[2024-04-17 12:15:23,968: INFO: main: Eval Epoch : batch 208 Loss: 0.003482676735556955]
[2024-04-17 12:15:24,172: INFO: main: Eval Epoch : batch 209 Loss: 0.010504601864567066]
[2024-04-17 12:15:24,374: INFO: main: Eval Epoch : batch 210 Loss: 0.006211948284977216]
[2024-04-17 12:15:24,574: INFO: main: Eval Epoch : batch 211 Loss: 0.028707487499319232]
[2024-04-17 12:15:24,778: INFO: main: Eval Epoch : batch 212 Loss: 0.006685166494555568]
[2024-04-17 12:15:24,980: INFO: main: Eval Epoch : batch 213 Loss: 0.016936624183140268]
[2024-04-17 12:15:25,183: INFO: main: Eval Epoch : batch 214 Loss: 0.00793858577272925]
[2024-04-17 12:15:25,387: INFO: main: Eval Epoch : batch 215 Loss: 0.03121349883581065]
[2024-04-17 12:15:25,594: INFO: main: Eval Epoch : batch 216 Loss: 0.016438230104248178]
[2024-04-17 12:15:25,794: INFO: main: Eval Epoch : batch 217 Loss: 0.0075882040253583746]
[2024-04-17 12:15:25,993: INFO: main: Eval Epoch : batch 218 Loss: 0.015097929495786793]
[2024-04-17 12:15:26,194: INFO: main: Eval Epoch : batch 219 Loss: 0.0010611749461334062]
[2024-04-17 12:15:26,397: INFO: main: Eval Epoch : batch 220 Loss: 0.003431756269506282]
[2024-04-17 12:15:26,597: INFO: main: Eval Epoch : batch 221 Loss: 0.0196020523256509]
[2024-04-17 12:15:26,798: INFO: main: Eval Epoch : batch 222 Loss: 0.0003179531301081786]
[2024-04-17 12:15:26,996: INFO: main: Eval Epoch : batch 223 Loss: 0.031242210832370734]
[2024-04-17 12:15:27,192: INFO: main: Eval Epoch : batch 224 Loss: 0.011546795230382944]
[2024-04-17 12:15:27,394: INFO: main: Eval Epoch : batch 225 Loss: 0.024877399791444994]
[2024-04-17 12:15:27,601: INFO: main: Eval Epoch : batch 226 Loss: 0.025924557022232852]
[2024-04-17 12:15:27,801: INFO: main: Eval Epoch : batch 227 Loss: 0.07725325957991323]
[2024-04-17 12:15:28,000: INFO: main: Eval Epoch : batch 228 Loss: 0.007534801445538452]
[2024-04-17 12:15:28,198: INFO: main: Eval Epoch : batch 229 Loss: 0.00868169140709249]
[2024-04-17 12:15:28,398: INFO: main: Eval Epoch : batch 230 Loss: 0.0036435930025630063]
[2024-04-17 12:15:28,601: INFO: main: Eval Epoch : batch 231 Loss: 0.027102329048919033]
[2024-04-17 12:15:28,804: INFO: main: Eval Epoch : batch 232 Loss: 0.020304435508266223]
[2024-04-17 12:15:29,006: INFO: main: Eval Epoch : batch 233 Loss: 0.01527676692685874]
[2024-04-17 12:15:29,210: INFO: main: Eval Epoch : batch 234 Loss: 0.02142981889429375]
[2024-04-17 12:15:29,409: INFO: main: Eval Epoch : batch 235 Loss: 0.013018845757594016]
[2024-04-17 12:15:29,612: INFO: main: Eval Epoch : batch 236 Loss: 0.019461883958586067]
[2024-04-17 12:15:29,813: INFO: main: Eval Epoch : batch 237 Loss: 0.00055045120384074]
[2024-04-17 12:15:30,011: INFO: main: Eval Epoch : batch 238 Loss: 0.007309863378448656]
[2024-04-17 12:15:30,210: INFO: main: Eval Epoch : batch 239 Loss: 0.02611635546603101]
[2024-04-17 12:15:30,409: INFO: main: Eval Epoch : batch 240 Loss: 0.009205328764253855]
[2024-04-17 12:15:30,619: INFO: main: Eval Epoch : batch 241 Loss: 0.016298134069165734]
[2024-04-17 12:15:30,823: INFO: main: Eval Epoch : batch 242 Loss: 0.030997447874260856]
[2024-04-17 12:15:31,030: INFO: main: Eval Epoch : batch 243 Loss: 0.017245405689790255]
[2024-04-17 12:15:31,232: INFO: main: Eval Epoch : batch 244 Loss: 0.011141955954805922]
[2024-04-17 12:15:31,437: INFO: main: Eval Epoch : batch 245 Loss: 0.007905299543571012]
[2024-04-17 12:15:31,641: INFO: main: Eval Epoch : batch 246 Loss: 0.008794734556737648]
[2024-04-17 12:15:31,849: INFO: main: Eval Epoch : batch 247 Loss: 0.019761753092357788]
[2024-04-17 12:15:32,052: INFO: main: Eval Epoch : batch 248 Loss: 0.006273803570276801]
[2024-04-17 12:15:32,255: INFO: main: Eval Epoch : batch 249 Loss: 0.013931041692334147]
[2024-04-17 12:15:32,458: INFO: main: Eval Epoch : batch 250 Loss: 0.0017687452189069377]
[2024-04-17 12:15:32,667: INFO: main: Eval Epoch : batch 251 Loss: 0.011985028553392296]
[2024-04-17 12:15:32,874: INFO: main: Eval Epoch : batch 252 Loss: 0.09982537043585508]
[2024-04-17 12:15:33,079: INFO: main: Eval Epoch : batch 253 Loss: 0.04257696864171602]
[2024-04-17 12:15:33,281: INFO: main: Eval Epoch : batch 254 Loss: 0.015643850397790468]
[2024-04-17 12:15:33,490: INFO: main: Eval Epoch : batch 255 Loss: 0.02509766945505943]
[2024-04-17 12:15:33,697: INFO: main: Eval Epoch : batch 256 Loss: 0.020721013530888992]
[2024-04-17 12:15:33,901: INFO: main: Eval Epoch : batch 257 Loss: 0.06788283920299175]
[2024-04-17 12:15:34,103: INFO: main: Eval Epoch : batch 258 Loss: 0.026182325716476072]
[2024-04-17 12:15:34,309: INFO: main: Eval Epoch : batch 259 Loss: 0.016012155477952813]
[2024-04-17 12:15:34,512: INFO: main: Eval Epoch : batch 260 Loss: 0.025169907921153888]
[2024-04-17 12:15:34,709: INFO: main: Eval Epoch : batch 261 Loss: 0.014552240482263498]
[2024-04-17 12:15:34,909: INFO: main: Eval Epoch : batch 262 Loss: 0.010609033945932432]
[2024-04-17 12:15:35,112: INFO: main: Eval Epoch : batch 263 Loss: 0.016644681292582664]
[2024-04-17 12:15:35,314: INFO: main: Eval Epoch : batch 264 Loss: 0.0816940500075511]
[2024-04-17 12:15:35,518: INFO: main: Eval Epoch : batch 265 Loss: 0.05115831678520901]
[2024-04-17 12:15:35,717: INFO: main: Eval Epoch : batch 266 Loss: 0.01237475133463788]
[2024-04-17 12:15:35,917: INFO: main: Eval Epoch : batch 267 Loss: 0.010876380751746495]
[2024-04-17 12:15:36,118: INFO: main: Eval Epoch : batch 268 Loss: 0.00834143811794065]
[2024-04-17 12:15:36,319: INFO: main: Eval Epoch : batch 269 Loss: 0.025872360188964293]
[2024-04-17 12:15:36,518: INFO: main: Eval Epoch : batch 270 Loss: 0.03575033307824005]
[2024-04-17 12:15:36,717: INFO: main: Eval Epoch : batch 271 Loss: 0.012221040960280724]
[2024-04-17 12:15:36,918: INFO: main: Eval Epoch : batch 272 Loss: 0.021732924679168236]
[2024-04-17 12:15:37,120: INFO: main: Eval Epoch : batch 273 Loss: 0.001416068420013483]
[2024-04-17 12:15:37,327: INFO: main: Eval Epoch : batch 274 Loss: 0.003364923152123788]
[2024-04-17 12:15:37,527: INFO: main: Eval Epoch : batch 275 Loss: 0.0006041922041882733]
[2024-04-17 12:15:37,726: INFO: main: Eval Epoch : batch 276 Loss: 0.0059139706969401525]
[2024-04-17 12:15:37,928: INFO: main: Eval Epoch : batch 277 Loss: 0.010232578426224352]
[2024-04-17 12:15:38,134: INFO: main: Eval Epoch : batch 278 Loss: 0.038584880819146065]
[2024-04-17 12:15:38,336: INFO: main: Eval Epoch : batch 279 Loss: 0.009134224275245685]
[2024-04-17 12:15:38,542: INFO: main: Eval Epoch : batch 280 Loss: 0.007831225020973895]
[2024-04-17 12:15:38,739: INFO: main: Eval Epoch : batch 281 Loss: 0.010086709453152749]
[2024-04-17 12:15:38,943: INFO: main: Eval Epoch : batch 282 Loss: 0.01374555228858143]
[2024-04-17 12:15:39,142: INFO: main: Eval Epoch : batch 283 Loss: 0.0029937059690312096]
[2024-04-17 12:15:39,345: INFO: main: Eval Epoch : batch 284 Loss: 0.010437675478640333]
[2024-04-17 12:15:39,552: INFO: main: Eval Epoch : batch 285 Loss: 0.0024339227479615506]
[2024-04-17 12:15:39,751: INFO: main: Eval Epoch : batch 286 Loss: 0.007789454233176969]
[2024-04-17 12:15:39,955: INFO: main: Eval Epoch : batch 287 Loss: 0.006602463178371286]
[2024-04-17 12:15:40,157: INFO: main: Eval Epoch : batch 288 Loss: 0.0029101092057240664]
[2024-04-17 12:15:40,357: INFO: main: Eval Epoch : batch 289 Loss: 0.00998146620017781]
[2024-04-17 12:15:40,561: INFO: main: Eval Epoch : batch 290 Loss: 0.013958292737367]
[2024-04-17 12:15:40,761: INFO: main: Eval Epoch : batch 291 Loss: 0.0018811062643853875]
[2024-04-17 12:15:40,963: INFO: main: Eval Epoch : batch 292 Loss: 0.004818772703748211]
[2024-04-17 12:15:41,164: INFO: main: Eval Epoch : batch 293 Loss: 0.009632431504667877]
[2024-04-17 12:15:41,363: INFO: main: Eval Epoch : batch 294 Loss: 0.025172386940081535]
[2024-04-17 12:15:41,567: INFO: main: Eval Epoch : batch 295 Loss: 0.006575678594734719]
[2024-04-17 12:15:41,767: INFO: main: Eval Epoch : batch 296 Loss: 0.05391934838017904]
[2024-04-17 12:15:41,966: INFO: main: Eval Epoch : batch 297 Loss: 0.04121765339966015]
[2024-04-17 12:15:42,164: INFO: main: Eval Epoch : batch 298 Loss: 0.00820219391914655]
[2024-04-17 12:15:42,367: INFO: main: Eval Epoch : batch 299 Loss: 0.0269231346742467]
[2024-04-17 12:15:42,567: INFO: main: Eval Epoch : batch 300 Loss: 0.01951806567806542]
[2024-04-17 12:15:42,769: INFO: main: Eval Epoch : batch 301 Loss: 0.00964855118986082]
[2024-04-17 12:15:42,972: INFO: main: Eval Epoch : batch 302 Loss: 0.024142159614558595]
[2024-04-17 12:15:43,172: INFO: main: Eval Epoch : batch 303 Loss: 0.0010890123041370102]
[2024-04-17 12:15:43,374: INFO: main: Eval Epoch : batch 304 Loss: 0.005650899862873581]
[2024-04-17 12:15:43,577: INFO: main: Eval Epoch : batch 305 Loss: 0.011547048522741047]
[2024-04-17 12:15:43,781: INFO: main: Eval Epoch : batch 306 Loss: 0.016119297186521386]
[2024-04-17 12:15:43,987: INFO: main: Eval Epoch : batch 307 Loss: 0.007771958970370864]
[2024-04-17 12:15:44,191: INFO: main: Eval Epoch : batch 308 Loss: 0.005326748105848955]
[2024-04-17 12:15:44,395: INFO: main: Eval Epoch : batch 309 Loss: 0.019904674480788028]
[2024-04-17 12:15:44,597: INFO: main: Eval Epoch : batch 310 Loss: 0.026365484375218125]
[2024-04-17 12:15:44,802: INFO: main: Eval Epoch : batch 311 Loss: 0.01044461665817559]
[2024-04-17 12:15:45,011: INFO: main: Eval Epoch : batch 312 Loss: 0.0071259469285883835]
[2024-04-17 12:15:45,217: INFO: main: Eval Epoch : batch 313 Loss: 0.01766829913045993]
[2024-04-17 12:15:45,424: INFO: main: Eval Epoch : batch 314 Loss: 0.017235176772092756]
[2024-04-17 12:15:45,626: INFO: main: Eval Epoch : batch 315 Loss: 0.0048126686919946916]
[2024-04-17 12:15:45,848: INFO: main: Eval Epoch : batch 316 Loss: 0.012908192851807249]
[2024-04-17 12:15:46,052: INFO: main: Eval Epoch : batch 317 Loss: 0.0012063277396488006]
[2024-04-17 12:15:46,255: INFO: main: Eval Epoch : batch 318 Loss: 0.003377362403260124]
[2024-04-17 12:15:46,458: INFO: main: Eval Epoch : batch 319 Loss: 0.00838160139169974]
[2024-04-17 12:15:46,671: INFO: main: Eval Epoch : batch 320 Loss: 0.0008926611346613067]
[2024-04-17 12:15:46,880: INFO: main: Eval Epoch : batch 321 Loss: 0.004994416014683456]
[2024-04-17 12:15:47,082: INFO: main: Eval Epoch : batch 322 Loss: 0.05956695980097315]
[2024-04-17 12:15:47,286: INFO: main: Eval Epoch : batch 323 Loss: 0.04791103185874529]
[2024-04-17 12:15:47,487: INFO: main: Eval Epoch : batch 324 Loss: 0.01991329529935953]
[2024-04-17 12:15:47,690: INFO: main: Eval Epoch : batch 325 Loss: 0.012187067997702057]
[2024-04-17 12:15:47,894: INFO: main: Eval Epoch : batch 326 Loss: 0.013881740612879948]
[2024-04-17 12:15:48,094: INFO: main: Eval Epoch : batch 327 Loss: 0.01989047195614404]
[2024-04-17 12:15:48,291: INFO: main: Eval Epoch : batch 328 Loss: 0.004009195167104646]
[2024-04-17 12:15:48,492: INFO: main: Eval Epoch : batch 329 Loss: 0.008400493778220679]
[2024-04-17 12:15:48,697: INFO: main: Eval Epoch : batch 330 Loss: 0.007745145835643581]
[2024-04-17 12:15:48,900: INFO: main: Eval Epoch : batch 331 Loss: 0.010968159171263836]
[2024-04-17 12:15:49,102: INFO: main: Eval Epoch : batch 332 Loss: 0.00759742765621463]
[2024-04-17 12:15:49,298: INFO: main: Eval Epoch : batch 333 Loss: 0.023839621053774015]
[2024-04-17 12:15:49,498: INFO: main: Eval Epoch : batch 334 Loss: 0.04397115126340665]
[2024-04-17 12:15:49,705: INFO: main: Eval Epoch : batch 335 Loss: 0.001688346677926189]
[2024-04-17 12:15:49,907: INFO: main: Eval Epoch : batch 336 Loss: 0.005661795778407043]
[2024-04-17 12:15:50,109: INFO: main: Eval Epoch : batch 337 Loss: 0.03610798337712291]
[2024-04-17 12:15:50,311: INFO: main: Eval Epoch : batch 338 Loss: 0.00441550637326754]
[2024-04-17 12:15:50,512: INFO: main: Eval Epoch : batch 339 Loss: 0.0027772911669798304]
[2024-04-17 12:15:50,717: INFO: main: Eval Epoch : batch 340 Loss: 0.019223605301055557]
[2024-04-17 12:15:50,922: INFO: main: Eval Epoch : batch 341 Loss: 0.02420442875037169]
[2024-04-17 12:15:51,122: INFO: main: Eval Epoch : batch 342 Loss: 0.0010596326061543364]
[2024-04-17 12:15:51,324: INFO: main: Eval Epoch : batch 343 Loss: 0.008699172971757053]
[2024-04-17 12:15:51,524: INFO: main: Eval Epoch : batch 344 Loss: 0.018468217423947716]
[2024-04-17 12:15:51,729: INFO: main: Eval Epoch : batch 345 Loss: 0.012623833147365404]
[2024-04-17 12:15:51,931: INFO: main: Eval Epoch : batch 346 Loss: 0.006895859454498986]
[2024-04-17 12:15:52,134: INFO: main: Eval Epoch : batch 347 Loss: 0.015092935954075954]
[2024-04-17 12:15:52,333: INFO: main: Eval Epoch : batch 348 Loss: 0.012579145353076606]
[2024-04-17 12:15:52,535: INFO: main: Eval Epoch : batch 349 Loss: 0.00944574623624492]
[2024-04-17 12:15:52,737: INFO: main: Eval Epoch : batch 350 Loss: 0.0015502834318208668]
[2024-04-17 12:15:52,940: INFO: main: Eval Epoch : batch 351 Loss: 0.03861201963112742]
[2024-04-17 12:15:53,144: INFO: main: Eval Epoch : batch 352 Loss: 0.014225493116308968]
[2024-04-17 12:15:53,346: INFO: main: Eval Epoch : batch 353 Loss: 0.00869215658551903]
[2024-04-17 12:15:53,550: INFO: main: Eval Epoch : batch 354 Loss: 0.008712528047421047]
[2024-04-17 12:15:53,751: INFO: main: Eval Epoch : batch 355 Loss: 0.02852961557043161]
[2024-04-17 12:15:53,954: INFO: main: Eval Epoch : batch 356 Loss: 0.004965295661303573]
[2024-04-17 12:15:54,058: INFO: main: Eval Epoch : batch 357 Loss: 0.016171966496845245]
[2024-04-17 12:16:08,569: INFO: main: The score of the eval model is {'Accuracy': 0.9930325812822786, 'precision': 0.7662587412587413, 'recall': 0.7604754055695324, 'f1': 0.7633561196499326}]
[2024-04-17 12:16:18,110: INFO: main: Epoch: 2/5]
[2024-04-17 12:16:18,778: INFO: main: Training : batch 0 Loss: 0.008316351405058247]
[2024-04-17 12:16:19,387: INFO: main: Training : batch 1 Loss: 0.00941426448390098]
[2024-04-17 12:16:19,999: INFO: main: Training : batch 2 Loss: 0.0051027210680422656]
[2024-04-17 12:16:20,615: INFO: main: Training : batch 3 Loss: 0.003869277260182616]
[2024-04-17 12:16:21,237: INFO: main: Training : batch 4 Loss: 0.02383498008733425]
[2024-04-17 12:16:21,859: INFO: main: Training : batch 5 Loss: 0.072147685773277]
[2024-04-17 12:16:22,484: INFO: main: Training : batch 6 Loss: 0.0026462694916512897]
[2024-04-17 12:16:23,111: INFO: main: Training : batch 7 Loss: 0.03040432967583256]
[2024-04-17 12:16:23,737: INFO: main: Training : batch 8 Loss: 0.01537447261206675]
[2024-04-17 12:16:24,367: INFO: main: Training : batch 9 Loss: 0.006863707994969092]
[2024-04-17 12:16:25,002: INFO: main: Training : batch 10 Loss: 0.02064957670584166]
[2024-04-17 12:16:25,634: INFO: main: Training : batch 11 Loss: 0.0061399391210617115]
[2024-04-17 12:16:26,259: INFO: main: Training : batch 12 Loss: 0.004078129307724347]
[2024-04-17 12:16:26,888: INFO: main: Training : batch 13 Loss: 0.015657861593252834]
[2024-04-17 12:16:27,517: INFO: main: Training : batch 14 Loss: 0.04736934371182345]
[2024-04-17 12:16:28,148: INFO: main: Training : batch 15 Loss: 0.00781973319892331]
[2024-04-17 12:16:28,778: INFO: main: Training : batch 16 Loss: 0.01062148847527904]
[2024-04-17 12:16:29,412: INFO: main: Training : batch 17 Loss: 0.023766737619795815]
[2024-04-17 12:16:30,042: INFO: main: Training : batch 18 Loss: 0.02727161700660888]
[2024-04-17 12:16:30,676: INFO: main: Training : batch 19 Loss: 0.010245254717355995]
[2024-04-17 12:16:31,306: INFO: main: Training : batch 20 Loss: 0.05554170677743911]
[2024-04-17 12:16:31,942: INFO: main: Training : batch 21 Loss: 0.018133012445147225]
[2024-04-17 12:16:32,578: INFO: main: Training : batch 22 Loss: 0.0060133756732480646]
[2024-04-17 12:16:33,214: INFO: main: Training : batch 23 Loss: 0.011785702614525656]
[2024-04-17 12:16:33,853: INFO: main: Training : batch 24 Loss: 0.02234561390529197]
[2024-04-17 12:16:34,493: INFO: main: Training : batch 25 Loss: 0.008812365486156848]
[2024-04-17 12:16:35,126: INFO: main: Training : batch 26 Loss: 0.00780931820218886]
[2024-04-17 12:16:35,765: INFO: main: Training : batch 27 Loss: 0.007648501436390933]
[2024-04-17 12:16:36,407: INFO: main: Training : batch 28 Loss: 0.006666097988942603]
[2024-04-17 12:16:37,048: INFO: main: Training : batch 29 Loss: 0.018960607644220525]
[2024-04-17 12:16:37,696: INFO: main: Training : batch 30 Loss: 0.016146492502007087]
[2024-04-17 12:16:38,344: INFO: main: Training : batch 31 Loss: 0.012201994230618046]
[2024-04-17 12:16:38,991: INFO: main: Training : batch 32 Loss: 0.020593287005815794]
[2024-04-17 12:16:39,642: INFO: main: Training : batch 33 Loss: 0.04367145870421582]
[2024-04-17 12:16:40,290: INFO: main: Training : batch 34 Loss: 0.017438345736964143]
[2024-04-17 12:16:40,935: INFO: main: Training : batch 35 Loss: 0.010936965518888635]
[2024-04-17 12:16:41,582: INFO: main: Training : batch 36 Loss: 0.02528187764710096]
[2024-04-17 12:16:42,232: INFO: main: Training : batch 37 Loss: 0.03694706418456319]
[2024-04-17 12:16:42,881: INFO: main: Training : batch 38 Loss: 0.003315594039748882]
[2024-04-17 12:16:43,528: INFO: main: Training : batch 39 Loss: 0.005405184968710767]
[2024-04-17 12:16:44,179: INFO: main: Training : batch 40 Loss: 0.013883091465889017]
[2024-04-17 12:16:44,829: INFO: main: Training : batch 41 Loss: 0.014906353982934108]
[2024-04-17 12:16:45,480: INFO: main: Training : batch 42 Loss: 0.02160090824190318]
[2024-04-17 12:16:46,131: INFO: main: Training : batch 43 Loss: 0.006409234599715319]
[2024-04-17 12:16:46,790: INFO: main: Training : batch 44 Loss: 0.013528165687314254]
[2024-04-17 12:16:47,445: INFO: main: Training : batch 45 Loss: 0.018382325625371794]
[2024-04-17 12:16:48,102: INFO: main: Training : batch 46 Loss: 0.011301967867904928]
[2024-04-17 12:16:48,759: INFO: main: Training : batch 47 Loss: 0.010819638603174498]
[2024-04-17 12:16:49,410: INFO: main: Training : batch 48 Loss: 0.01158489088455865]
[2024-04-17 12:16:50,067: INFO: main: Training : batch 49 Loss: 0.0018128555689470107]
[2024-04-17 12:16:50,731: INFO: main: Training : batch 50 Loss: 0.022369189676258297]
[2024-04-17 12:16:51,388: INFO: main: Training : batch 51 Loss: 0.008916729935252046]
[2024-04-17 12:16:52,049: INFO: main: Training : batch 52 Loss: 0.046468403458158676]
[2024-04-17 12:16:52,712: INFO: main: Training : batch 53 Loss: 0.010479913018910231]
[2024-04-17 12:16:53,370: INFO: main: Training : batch 54 Loss: 0.0066974680106555535]
[2024-04-17 12:16:54,023: INFO: main: Training : batch 55 Loss: 0.009568351315913144]
[2024-04-17 12:16:54,675: INFO: main: Training : batch 56 Loss: 0.005841801372041297]
[2024-04-17 12:16:55,321: INFO: main: Training : batch 57 Loss: 0.04102278056964072]
[2024-04-17 12:16:55,972: INFO: main: Training : batch 58 Loss: 0.015552761300538096]
[2024-04-17 12:16:56,619: INFO: main: Training : batch 59 Loss: 0.01031209985162253]
[2024-04-17 12:16:57,265: INFO: main: Training : batch 60 Loss: 0.0034100031183299717]
[2024-04-17 12:16:57,910: INFO: main: Training : batch 61 Loss: 0.020814581986545132]
[2024-04-17 12:16:58,556: INFO: main: Training : batch 62 Loss: 0.01342141315553085]
[2024-04-17 12:16:59,202: INFO: main: Training : batch 63 Loss: 0.010358647688262555]
[2024-04-17 12:16:59,849: INFO: main: Training : batch 64 Loss: 0.016412029052109388]
[2024-04-17 12:17:00,495: INFO: main: Training : batch 65 Loss: 0.009939295787093369]
[2024-04-17 12:17:01,135: INFO: main: Training : batch 66 Loss: 0.008133550781541274]
[2024-04-17 12:17:01,772: INFO: main: Training : batch 67 Loss: 0.01599568400042585]
[2024-04-17 12:17:02,407: INFO: main: Training : batch 68 Loss: 0.0049487344662987685]
[2024-04-17 12:17:03,058: INFO: main: Training : batch 69 Loss: 0.02079547198922584]
[2024-04-17 12:17:03,701: INFO: main: Training : batch 70 Loss: 0.007693640090422536]
[2024-04-17 12:17:04,343: INFO: main: Training : batch 71 Loss: 0.004259175274475171]
[2024-04-17 12:17:04,991: INFO: main: Training : batch 72 Loss: 0.018351595123848895]
[2024-04-17 12:17:05,644: INFO: main: Training : batch 73 Loss: 0.012476266851878217]
[2024-04-17 12:17:06,287: INFO: main: Training : batch 74 Loss: 0.00771962569476818]
[2024-04-17 12:17:06,919: INFO: main: Training : batch 75 Loss: 0.002575073950338811]
[2024-04-17 12:17:07,556: INFO: main: Training : batch 76 Loss: 0.01463720285970203]
[2024-04-17 12:17:08,190: INFO: main: Training : batch 77 Loss: 0.025025000372334735]
[2024-04-17 12:17:08,824: INFO: main: Training : batch 78 Loss: 0.018642321000084772]
[2024-04-17 12:17:09,459: INFO: main: Training : batch 79 Loss: 0.008659372958704928]
[2024-04-17 12:17:10,089: INFO: main: Training : batch 80 Loss: 0.010967443739523804]
[2024-04-17 12:17:10,723: INFO: main: Training : batch 81 Loss: 0.008295571840934516]
[2024-04-17 12:17:11,358: INFO: main: Training : batch 82 Loss: 0.0011096680103346132]
[2024-04-17 12:17:11,989: INFO: main: Training : batch 83 Loss: 0.004219465827202975]
[2024-04-17 12:17:12,620: INFO: main: Training : batch 84 Loss: 0.008186613704580625]
[2024-04-17 12:17:13,251: INFO: main: Training : batch 85 Loss: 0.015724532059808605]
[2024-04-17 12:17:13,884: INFO: main: Training : batch 86 Loss: 0.015858178951425037]
[2024-04-17 12:17:14,514: INFO: main: Training : batch 87 Loss: 0.029966301408707763]
[2024-04-17 12:17:15,150: INFO: main: Training : batch 88 Loss: 0.014638580233033155]
[2024-04-17 12:17:15,778: INFO: main: Training : batch 89 Loss: 0.012985479603167663]
[2024-04-17 12:17:16,414: INFO: main: Training : batch 90 Loss: 0.015639998640581475]
[2024-04-17 12:17:17,044: INFO: main: Training : batch 91 Loss: 0.004042886022419109]
[2024-04-17 12:17:17,685: INFO: main: Training : batch 92 Loss: 0.011668172945529062]
[2024-04-17 12:17:18,321: INFO: main: Training : batch 93 Loss: 0.02798935273446024]
[2024-04-17 12:17:18,954: INFO: main: Training : batch 94 Loss: 0.02320953865758313]
[2024-04-17 12:17:19,581: INFO: main: Training : batch 95 Loss: 0.015953828553075377]
[2024-04-17 12:17:20,205: INFO: main: Training : batch 96 Loss: 0.018050310093717176]
[2024-04-17 12:17:20,827: INFO: main: Training : batch 97 Loss: 0.01236391616579293]
[2024-04-17 12:17:21,454: INFO: main: Training : batch 98 Loss: 0.006870254507215561]
[2024-04-17 12:17:22,081: INFO: main: Training : batch 99 Loss: 0.01886873221158202]
[2024-04-17 12:17:22,705: INFO: main: Training : batch 100 Loss: 0.012648241194620567]
[2024-04-17 12:17:23,326: INFO: main: Training : batch 101 Loss: 0.014210064438998417]
[2024-04-17 12:17:23,949: INFO: main: Training : batch 102 Loss: 0.029995357010377036]
[2024-04-17 12:17:24,572: INFO: main: Training : batch 103 Loss: 0.018012433599572674]
[2024-04-17 12:17:25,196: INFO: main: Training : batch 104 Loss: 0.022212263310489373]
[2024-04-17 12:17:25,823: INFO: main: Training : batch 105 Loss: 0.004197706035466341]
[2024-04-17 12:17:26,448: INFO: main: Training : batch 106 Loss: 0.013946722563684068]
[2024-04-17 12:17:27,064: INFO: main: Training : batch 107 Loss: 0.020262445985715652]
[2024-04-17 12:17:27,691: INFO: main: Training : batch 108 Loss: 0.010889967662339253]
[2024-04-17 12:17:28,315: INFO: main: Training : batch 109 Loss: 0.013877529544733086]
[2024-04-17 12:17:28,938: INFO: main: Training : batch 110 Loss: 0.009644675065550199]
[2024-04-17 12:17:29,562: INFO: main: Training : batch 111 Loss: 0.011472352905975567]
[2024-04-17 12:17:30,192: INFO: main: Training : batch 112 Loss: 0.010349605379042592]
[2024-04-17 12:17:30,824: INFO: main: Training : batch 113 Loss: 0.020171920073536187]
[2024-04-17 12:17:31,454: INFO: main: Training : batch 114 Loss: 0.00185119354804339]
[2024-04-17 12:17:32,085: INFO: main: Training : batch 115 Loss: 0.01968515084003384]
[2024-04-17 12:17:32,707: INFO: main: Training : batch 116 Loss: 0.009502128974492995]
[2024-04-17 12:17:33,329: INFO: main: Training : batch 117 Loss: 0.03491980227150703]
[2024-04-17 12:17:33,951: INFO: main: Training : batch 118 Loss: 0.01406399421917593]
[2024-04-17 12:17:34,576: INFO: main: Training : batch 119 Loss: 0.010418018971442795]
[2024-04-17 12:17:35,203: INFO: main: Training : batch 120 Loss: 0.013742918466274337]
[2024-04-17 12:17:35,824: INFO: main: Training : batch 121 Loss: 0.01281260284141751]
[2024-04-17 12:17:36,446: INFO: main: Training : batch 122 Loss: 0.007123999867208267]
[2024-04-17 12:17:37,068: INFO: main: Training : batch 123 Loss: 0.018936277302786644]
[2024-04-17 12:17:37,693: INFO: main: Training : batch 124 Loss: 0.022041862244978273]
[2024-04-17 12:17:38,313: INFO: main: Training : batch 125 Loss: 0.018853260225435688]
[2024-04-17 12:17:38,936: INFO: main: Training : batch 126 Loss: 0.01612739368599808]
[2024-04-17 12:17:39,559: INFO: main: Training : batch 127 Loss: 0.012963830179524093]
[2024-04-17 12:17:40,186: INFO: main: Training : batch 128 Loss: 0.00395854659125929]
[2024-04-17 12:17:40,811: INFO: main: Training : batch 129 Loss: 0.0292637826697723]
[2024-04-17 12:17:41,434: INFO: main: Training : batch 130 Loss: 0.019139351668110757]
[2024-04-17 12:17:42,057: INFO: main: Training : batch 131 Loss: 0.0016104874194085802]
[2024-04-17 12:17:42,693: INFO: main: Training : batch 132 Loss: 0.02132225123411419]
[2024-04-17 12:17:43,323: INFO: main: Training : batch 133 Loss: 0.0068271653436812706]
[2024-04-17 12:17:43,953: INFO: main: Training : batch 134 Loss: 0.006352499824879322]
[2024-04-17 12:17:44,587: INFO: main: Training : batch 135 Loss: 0.0036319455663520006]
[2024-04-17 12:17:45,217: INFO: main: Training : batch 136 Loss: 0.026292229686320907]
[2024-04-17 12:17:45,844: INFO: main: Training : batch 137 Loss: 0.005321062959966134]
[2024-04-17 12:17:46,467: INFO: main: Training : batch 138 Loss: 0.00044485335313830895]
[2024-04-17 12:17:47,094: INFO: main: Training : batch 139 Loss: 0.029429774469354426]
[2024-04-17 12:17:47,716: INFO: main: Training : batch 140 Loss: 0.019832252730824503]
[2024-04-17 12:17:48,342: INFO: main: Training : batch 141 Loss: 0.03469699015351095]
[2024-04-17 12:17:48,970: INFO: main: Training : batch 142 Loss: 0.009744295624839323]
[2024-04-17 12:17:49,596: INFO: main: Training : batch 143 Loss: 0.028269554353106106]
[2024-04-17 12:17:50,225: INFO: main: Training : batch 144 Loss: 0.02860791232904481]
[2024-04-17 12:17:50,852: INFO: main: Training : batch 145 Loss: 0.02301142951434196]
[2024-04-17 12:17:51,480: INFO: main: Training : batch 146 Loss: 0.017231617757098198]
[2024-04-17 12:17:52,107: INFO: main: Training : batch 147 Loss: 0.016877672834608597]
[2024-04-17 12:17:52,733: INFO: main: Training : batch 148 Loss: 0.015780856012975557]
[2024-04-17 12:17:53,362: INFO: main: Training : batch 149 Loss: 0.004097441801980027]
[2024-04-17 12:17:53,994: INFO: main: Training : batch 150 Loss: 0.018588154531405554]
[2024-04-17 12:17:54,622: INFO: main: Training : batch 151 Loss: 0.028691275224661507]
[2024-04-17 12:17:55,250: INFO: main: Training : batch 152 Loss: 0.0179769940739876]
[2024-04-17 12:17:55,880: INFO: main: Training : batch 153 Loss: 0.010679478271558676]
[2024-04-17 12:17:56,518: INFO: main: Training : batch 154 Loss: 0.005095569663068912]
[2024-04-17 12:17:57,157: INFO: main: Training : batch 155 Loss: 0.011895352230463459]
[2024-04-17 12:17:57,787: INFO: main: Training : batch 156 Loss: 0.004849435354583287]
[2024-04-17 12:17:58,424: INFO: main: Training : batch 157 Loss: 0.008745379941572857]
[2024-04-17 12:17:59,055: INFO: main: Training : batch 158 Loss: 0.027771519020239718]
[2024-04-17 12:17:59,683: INFO: main: Training : batch 159 Loss: 0.0154472680797309]
[2024-04-17 12:18:00,319: INFO: main: Training : batch 160 Loss: 0.012369630286818683]
[2024-04-17 12:18:00,953: INFO: main: Training : batch 161 Loss: 0.010780827507928581]
[2024-04-17 12:18:01,585: INFO: main: Training : batch 162 Loss: 0.011325744979261126]
[2024-04-17 12:18:02,216: INFO: main: Training : batch 163 Loss: 0.017847959994991756]
[2024-04-17 12:18:02,852: INFO: main: Training : batch 164 Loss: 0.00767976781297008]
[2024-04-17 12:18:03,482: INFO: main: Training : batch 165 Loss: 0.0036804086065184405]
[2024-04-17 12:18:04,116: INFO: main: Training : batch 166 Loss: 0.02721693298221088]
[2024-04-17 12:18:04,747: INFO: main: Training : batch 167 Loss: 0.01970657036550984]
[2024-04-17 12:18:05,385: INFO: main: Training : batch 168 Loss: 0.015281669419363372]
[2024-04-17 12:18:06,016: INFO: main: Training : batch 169 Loss: 0.028999311389419684]
[2024-04-17 12:18:06,648: INFO: main: Training : batch 170 Loss: 0.009293581254937214]
[2024-04-17 12:18:07,281: INFO: main: Training : batch 171 Loss: 0.011649886751896068]
[2024-04-17 12:18:07,914: INFO: main: Training : batch 172 Loss: 0.005834426136147626]
[2024-04-17 12:18:08,546: INFO: main: Training : batch 173 Loss: 0.003307197047066749]
[2024-04-17 12:18:09,183: INFO: main: Training : batch 174 Loss: 0.022881274789887593]
[2024-04-17 12:18:09,827: INFO: main: Training : batch 175 Loss: 0.012624395872539785]
[2024-04-17 12:18:10,467: INFO: main: Training : batch 176 Loss: 0.0230669872732743]
[2024-04-17 12:18:11,108: INFO: main: Training : batch 177 Loss: 0.008837553660917768]
[2024-04-17 12:18:11,749: INFO: main: Training : batch 178 Loss: 0.017877590751604014]
[2024-04-17 12:18:12,388: INFO: main: Training : batch 179 Loss: 0.019700540702562018]
[2024-04-17 12:18:13,024: INFO: main: Training : batch 180 Loss: 0.060749744442235375]
[2024-04-17 12:18:13,658: INFO: main: Training : batch 181 Loss: 0.015185917749630577]
[2024-04-17 12:18:14,292: INFO: main: Training : batch 182 Loss: 0.02781367413679699]
[2024-04-17 12:18:14,930: INFO: main: Training : batch 183 Loss: 0.02206966261046951]
[2024-04-17 12:18:15,563: INFO: main: Training : batch 184 Loss: 0.004387821542386364]
[2024-04-17 12:18:16,198: INFO: main: Training : batch 185 Loss: 0.015957366110601306]
[2024-04-17 12:18:16,834: INFO: main: Training : batch 186 Loss: 0.006749543779523757]
[2024-04-17 12:18:17,468: INFO: main: Training : batch 187 Loss: 0.041405637014025914]
[2024-04-17 12:18:18,105: INFO: main: Training : batch 188 Loss: 0.016468770789012255]
[2024-04-17 12:18:18,735: INFO: main: Training : batch 189 Loss: 0.03532365510316373]
[2024-04-17 12:18:19,372: INFO: main: Training : batch 190 Loss: 0.0080310969176516]
[2024-04-17 12:18:20,010: INFO: main: Training : batch 191 Loss: 0.025860361277378965]
[2024-04-17 12:18:20,651: INFO: main: Training : batch 192 Loss: 0.006698131791454683]
[2024-04-17 12:18:21,284: INFO: main: Training : batch 193 Loss: 0.014289319084385757]
[2024-04-17 12:18:21,921: INFO: main: Training : batch 194 Loss: 0.012933748503032675]
[2024-04-17 12:18:22,563: INFO: main: Training : batch 195 Loss: 0.007428762662958464]
[2024-04-17 12:18:23,204: INFO: main: Training : batch 196 Loss: 0.007880681517290013]
[2024-04-17 12:18:23,841: INFO: main: Training : batch 197 Loss: 0.006421059315196132]
[2024-04-17 12:18:24,483: INFO: main: Training : batch 198 Loss: 0.012630378795093853]
[2024-04-17 12:18:25,118: INFO: main: Training : batch 199 Loss: 0.014278449035218445]
[2024-04-17 12:18:25,754: INFO: main: Training : batch 200 Loss: 0.017021818577910602]
[2024-04-17 12:18:26,387: INFO: main: Training : batch 201 Loss: 0.009414285462531142]
[2024-04-17 12:18:27,022: INFO: main: Training : batch 202 Loss: 0.008485953341649864]
[2024-04-17 12:18:27,658: INFO: main: Training : batch 203 Loss: 0.007706599480794888]
[2024-04-17 12:18:28,296: INFO: main: Training : batch 204 Loss: 0.0123595004845226]
[2024-04-17 12:18:28,932: INFO: main: Training : batch 205 Loss: 0.010255983777529907]
[2024-04-17 12:18:29,565: INFO: main: Training : batch 206 Loss: 0.031048645291203464]
[2024-04-17 12:18:30,198: INFO: main: Training : batch 207 Loss: 0.008361734139361839]
[2024-04-17 12:18:30,837: INFO: main: Training : batch 208 Loss: 0.014706805407695561]
[2024-04-17 12:18:31,469: INFO: main: Training : batch 209 Loss: 0.007729509123607128]
[2024-04-17 12:18:32,102: INFO: main: Training : batch 210 Loss: 0.012000341865986208]
[2024-04-17 12:18:32,737: INFO: main: Training : batch 211 Loss: 0.015784271291013473]
[2024-04-17 12:18:33,374: INFO: main: Training : batch 212 Loss: 0.015939705554164952]
[2024-04-17 12:18:34,008: INFO: main: Training : batch 213 Loss: 0.0053561807843901755]
[2024-04-17 12:18:34,643: INFO: main: Training : batch 214 Loss: 0.013106884936591736]
[2024-04-17 12:18:35,281: INFO: main: Training : batch 215 Loss: 0.012483643127923277]
[2024-04-17 12:18:35,920: INFO: main: Training : batch 216 Loss: 0.0143419119156315]
[2024-04-17 12:18:36,557: INFO: main: Training : batch 217 Loss: 0.009178971865731906]
[2024-04-17 12:18:37,196: INFO: main: Training : batch 218 Loss: 0.005524747504051567]
[2024-04-17 12:18:37,835: INFO: main: Training : batch 219 Loss: 0.013969590555532685]
[2024-04-17 12:18:38,468: INFO: main: Training : batch 220 Loss: 0.0566707726388441]
[2024-04-17 12:18:39,100: INFO: main: Training : batch 221 Loss: 0.002720188992713458]
[2024-04-17 12:18:39,733: INFO: main: Training : batch 222 Loss: 0.008372394067482499]
[2024-04-17 12:18:40,367: INFO: main: Training : batch 223 Loss: 0.008697390846332306]
[2024-04-17 12:18:40,994: INFO: main: Training : batch 224 Loss: 0.002663232765141129]
[2024-04-17 12:18:41,625: INFO: main: Training : batch 225 Loss: 0.013847928972572587]
[2024-04-17 12:18:42,256: INFO: main: Training : batch 226 Loss: 0.01326420131765651]
[2024-04-17 12:18:42,887: INFO: main: Training : batch 227 Loss: 0.012533253407313084]
[2024-04-17 12:18:43,523: INFO: main: Training : batch 228 Loss: 0.004011916491827319]
[2024-04-17 12:18:44,152: INFO: main: Training : batch 229 Loss: 0.021166507086253332]
[2024-04-17 12:18:44,783: INFO: main: Training : batch 230 Loss: 0.008914332739579155]
[2024-04-17 12:18:45,412: INFO: main: Training : batch 231 Loss: 0.00826871012475412]
[2024-04-17 12:18:46,047: INFO: main: Training : batch 232 Loss: 0.005811670830520491]
[2024-04-17 12:18:46,678: INFO: main: Training : batch 233 Loss: 0.007344605719072575]
[2024-04-17 12:18:47,312: INFO: main: Training : batch 234 Loss: 0.022257553068256514]
[2024-04-17 12:18:47,943: INFO: main: Training : batch 235 Loss: 0.011435200788791596]
[2024-04-17 12:18:48,582: INFO: main: Training : batch 236 Loss: 0.01954637333378669]
[2024-04-17 12:18:49,217: INFO: main: Training : batch 237 Loss: 0.018698851737474768]
[2024-04-17 12:18:49,854: INFO: main: Training : batch 238 Loss: 0.011665216167926106]
[2024-04-17 12:18:50,487: INFO: main: Training : batch 239 Loss: 0.006106918587568069]
[2024-04-17 12:18:51,125: INFO: main: Training : batch 240 Loss: 0.004313977091235387]
[2024-04-17 12:18:51,756: INFO: main: Training : batch 241 Loss: 0.003085539790650645]
[2024-04-17 12:18:52,380: INFO: main: Training : batch 242 Loss: 0.015386800747152722]
[2024-04-17 12:18:53,013: INFO: main: Training : batch 243 Loss: 0.008972103514318307]
[2024-04-17 12:18:53,642: INFO: main: Training : batch 244 Loss: 0.053610067638236364]
[2024-04-17 12:18:54,274: INFO: main: Training : batch 245 Loss: 0.005868156465384716]
[2024-04-17 12:18:54,903: INFO: main: Training : batch 246 Loss: 0.002957851847061863]
[2024-04-17 12:18:55,535: INFO: main: Training : batch 247 Loss: 0.01516392636059112]
[2024-04-17 12:18:56,160: INFO: main: Training : batch 248 Loss: 0.013903848535569904]
[2024-04-17 12:18:56,793: INFO: main: Training : batch 249 Loss: 0.015880019357732846]
[2024-04-17 12:18:57,423: INFO: main: Training : batch 250 Loss: 0.020324753737423037]
[2024-04-17 12:18:58,056: INFO: main: Training : batch 251 Loss: 0.026177229221093692]
[2024-04-17 12:18:58,683: INFO: main: Training : batch 252 Loss: 0.008270689672622076]
[2024-04-17 12:18:59,310: INFO: main: Training : batch 253 Loss: 0.018847280863668728]
[2024-04-17 12:18:59,938: INFO: main: Training : batch 254 Loss: 0.007164025917031815]
[2024-04-17 12:19:00,567: INFO: main: Training : batch 255 Loss: 0.012602731220066835]
[2024-04-17 12:19:01,194: INFO: main: Training : batch 256 Loss: 0.014043760888645973]
[2024-04-17 12:19:01,831: INFO: main: Training : batch 257 Loss: 0.006892872299449434]
[2024-04-17 12:19:02,466: INFO: main: Training : batch 258 Loss: 0.0012062698578987762]
[2024-04-17 12:19:03,098: INFO: main: Training : batch 259 Loss: 0.010680223160369802]
[2024-04-17 12:19:03,732: INFO: main: Training : batch 260 Loss: 0.003958442425920138]
[2024-04-17 12:19:04,362: INFO: main: Training : batch 261 Loss: 0.036246590735460646]
[2024-04-17 12:19:04,990: INFO: main: Training : batch 262 Loss: 0.00446797360177427]
[2024-04-17 12:19:05,619: INFO: main: Training : batch 263 Loss: 0.014971492038589324]
[2024-04-17 12:19:06,250: INFO: main: Training : batch 264 Loss: 0.010553221760121509]
[2024-04-17 12:19:06,878: INFO: main: Training : batch 265 Loss: 0.005725085596694335]
[2024-04-17 12:19:07,508: INFO: main: Training : batch 266 Loss: 0.012467089669481513]
[2024-04-17 12:19:08,131: INFO: main: Training : batch 267 Loss: 0.031114200338723145]
[2024-04-17 12:19:08,765: INFO: main: Training : batch 268 Loss: 0.004727911180682179]
[2024-04-17 12:19:09,393: INFO: main: Training : batch 269 Loss: 0.020094237306348954]
[2024-04-17 12:19:10,022: INFO: main: Training : batch 270 Loss: 0.02620595029666248]
[2024-04-17 12:19:10,648: INFO: main: Training : batch 271 Loss: 0.002957348911701847]
[2024-04-17 12:19:11,282: INFO: main: Training : batch 272 Loss: 0.008524182677748343]
[2024-04-17 12:19:11,915: INFO: main: Training : batch 273 Loss: 0.007135631268592019]
[2024-04-17 12:19:12,544: INFO: main: Training : batch 274 Loss: 0.026111105759895663]
[2024-04-17 12:19:13,171: INFO: main: Training : batch 275 Loss: 0.027803195900201704]
[2024-04-17 12:19:13,801: INFO: main: Training : batch 276 Loss: 0.01097502188350443]
[2024-04-17 12:19:14,431: INFO: main: Training : batch 277 Loss: 0.011052518811233343]
[2024-04-17 12:19:15,064: INFO: main: Training : batch 278 Loss: 0.005156004912777896]
[2024-04-17 12:19:15,710: INFO: main: Training : batch 279 Loss: 0.030089780043967066]
[2024-04-17 12:19:16,348: INFO: main: Training : batch 280 Loss: 0.012707717923285868]
[2024-04-17 12:19:16,987: INFO: main: Training : batch 281 Loss: 0.023271567454081987]
[2024-04-17 12:19:17,628: INFO: main: Training : batch 282 Loss: 0.023803476245352666]
[2024-04-17 12:19:18,253: INFO: main: Training : batch 283 Loss: 0.009435186018474408]
[2024-04-17 12:19:18,887: INFO: main: Training : batch 284 Loss: 0.026322427602410714]
[2024-04-17 12:19:19,518: INFO: main: Training : batch 285 Loss: 0.04313037592973733]
[2024-04-17 12:19:20,148: INFO: main: Training : batch 286 Loss: 0.005543504655514354]
[2024-04-17 12:19:20,774: INFO: main: Training : batch 287 Loss: 0.005314818319084861]
[2024-04-17 12:19:21,402: INFO: main: Training : batch 288 Loss: 0.012550017369228795]
[2024-04-17 12:19:22,029: INFO: main: Training : batch 289 Loss: 0.014588332029421933]
[2024-04-17 12:19:22,662: INFO: main: Training : batch 290 Loss: 0.017708305437280726]
[2024-04-17 12:19:23,292: INFO: main: Training : batch 291 Loss: 0.042019770964442635]
[2024-04-17 12:19:23,923: INFO: main: Training : batch 292 Loss: 0.009455402241864622]
[2024-04-17 12:19:24,548: INFO: main: Training : batch 293 Loss: 0.017403181697477173]
[2024-04-17 12:19:25,182: INFO: main: Training : batch 294 Loss: 0.008349970990817112]
[2024-04-17 12:19:25,810: INFO: main: Training : batch 295 Loss: 0.019516534672795995]
[2024-04-17 12:19:26,436: INFO: main: Training : batch 296 Loss: 0.01130158991637647]
[2024-04-17 12:19:27,067: INFO: main: Training : batch 297 Loss: 0.014515710317568788]
[2024-04-17 12:19:27,704: INFO: main: Training : batch 298 Loss: 0.008713620937101664]
[2024-04-17 12:19:28,343: INFO: main: Training : batch 299 Loss: 0.010849850370175466]
[2024-04-17 12:19:28,977: INFO: main: Training : batch 300 Loss: 0.011137808238459222]
[2024-04-17 12:19:29,610: INFO: main: Training : batch 301 Loss: 0.00499369975923004]
[2024-04-17 12:19:30,247: INFO: main: Training : batch 302 Loss: 0.020502430841778555]
[2024-04-17 12:19:30,879: INFO: main: Training : batch 303 Loss: 0.035190645654823285]
[2024-04-17 12:19:31,511: INFO: main: Training : batch 304 Loss: 0.014104467880979415]
[2024-04-17 12:19:32,147: INFO: main: Training : batch 305 Loss: 0.009087638255536984]
[2024-04-17 12:19:32,773: INFO: main: Training : batch 306 Loss: 0.003059108462366567]
[2024-04-17 12:19:33,407: INFO: main: Training : batch 307 Loss: 0.006080625168143147]
[2024-04-17 12:19:34,038: INFO: main: Training : batch 308 Loss: 0.03195116520043834]
[2024-04-17 12:19:34,668: INFO: main: Training : batch 309 Loss: 0.023308934925600662]
[2024-04-17 12:19:35,300: INFO: main: Training : batch 310 Loss: 0.004258205097493666]
[2024-04-17 12:19:35,934: INFO: main: Training : batch 311 Loss: 0.0026103371963184393]
[2024-04-17 12:19:36,566: INFO: main: Training : batch 312 Loss: 0.011464504344813888]
[2024-04-17 12:19:37,202: INFO: main: Training : batch 313 Loss: 0.02266308566882479]
[2024-04-17 12:19:37,835: INFO: main: Training : batch 314 Loss: 0.017794334697264456]
[2024-04-17 12:19:38,465: INFO: main: Training : batch 315 Loss: 0.020514409280566106]
[2024-04-17 12:19:39,095: INFO: main: Training : batch 316 Loss: 0.008056233494680315]
[2024-04-17 12:19:39,726: INFO: main: Training : batch 317 Loss: 0.01945617485790412]
[2024-04-17 12:19:40,358: INFO: main: Training : batch 318 Loss: 0.00726084149659483]
[2024-04-17 12:19:40,995: INFO: main: Training : batch 319 Loss: 0.030752571525408504]
[2024-04-17 12:19:41,630: INFO: main: Training : batch 320 Loss: 0.017767485180237602]
[2024-04-17 12:19:42,267: INFO: main: Training : batch 321 Loss: 0.006888886553383133]
[2024-04-17 12:19:42,905: INFO: main: Training : batch 322 Loss: 0.0019282478291922482]
[2024-04-17 12:19:43,543: INFO: main: Training : batch 323 Loss: 0.006292934515872839]
[2024-04-17 12:19:44,175: INFO: main: Training : batch 324 Loss: 0.0031073326167250525]
[2024-04-17 12:19:44,803: INFO: main: Training : batch 325 Loss: 0.017634687919723807]
[2024-04-17 12:19:45,433: INFO: main: Training : batch 326 Loss: 0.005898519448650484]
[2024-04-17 12:19:46,064: INFO: main: Training : batch 327 Loss: 0.022988112797108724]
[2024-04-17 12:19:46,695: INFO: main: Training : batch 328 Loss: 0.014312592635714197]
[2024-04-17 12:19:47,331: INFO: main: Training : batch 329 Loss: 0.041298587210050426]
[2024-04-17 12:19:47,962: INFO: main: Training : batch 330 Loss: 0.01363160722150406]
[2024-04-17 12:19:48,598: INFO: main: Training : batch 331 Loss: 0.04927254160155749]
[2024-04-17 12:19:49,229: INFO: main: Training : batch 332 Loss: 0.0013608662969375803]
[2024-04-17 12:19:49,859: INFO: main: Training : batch 333 Loss: 0.012904274958785482]
[2024-04-17 12:19:50,496: INFO: main: Training : batch 334 Loss: 0.0021600443659694]
[2024-04-17 12:19:51,128: INFO: main: Training : batch 335 Loss: 0.017241512753659156]
[2024-04-17 12:19:51,762: INFO: main: Training : batch 336 Loss: 0.011307635607689347]
[2024-04-17 12:19:52,394: INFO: main: Training : batch 337 Loss: 0.005401190076338626]
[2024-04-17 12:19:53,026: INFO: main: Training : batch 338 Loss: 0.008917102427052436]
[2024-04-17 12:19:53,656: INFO: main: Training : batch 339 Loss: 0.013695964390921604]
[2024-04-17 12:19:54,296: INFO: main: Training : batch 340 Loss: 0.01791840732895553]
[2024-04-17 12:19:54,940: INFO: main: Training : batch 341 Loss: 0.004215473306318039]
[2024-04-17 12:19:55,581: INFO: main: Training : batch 342 Loss: 0.0013631305426957399]
[2024-04-17 12:19:56,221: INFO: main: Training : batch 343 Loss: 0.02062295509053229]
[2024-04-17 12:19:56,858: INFO: main: Training : batch 344 Loss: 0.03898958167176212]
[2024-04-17 12:19:57,491: INFO: main: Training : batch 345 Loss: 0.013687751142121769]
[2024-04-17 12:19:58,122: INFO: main: Training : batch 346 Loss: 0.02492452411658688]
[2024-04-17 12:19:58,757: INFO: main: Training : batch 347 Loss: 0.028969329267706997]
[2024-04-17 12:19:59,388: INFO: main: Training : batch 348 Loss: 0.012153584580043064]
[2024-04-17 12:20:00,023: INFO: main: Training : batch 349 Loss: 0.0210737424766026]
[2024-04-17 12:20:00,655: INFO: main: Training : batch 350 Loss: 0.012730574827501737]
[2024-04-17 12:20:01,286: INFO: main: Training : batch 351 Loss: 0.044765491760018934]
[2024-04-17 12:20:01,920: INFO: main: Training : batch 352 Loss: 0.014004392842842104]
[2024-04-17 12:20:02,556: INFO: main: Training : batch 353 Loss: 0.02082423181055141]
[2024-04-17 12:20:03,187: INFO: main: Training : batch 354 Loss: 0.028912094266920214]
[2024-04-17 12:20:03,817: INFO: main: Training : batch 355 Loss: 0.007395061555272954]
[2024-04-17 12:20:04,452: INFO: main: Training : batch 356 Loss: 0.0070336618265438335]
[2024-04-17 12:20:05,091: INFO: main: Training : batch 357 Loss: 0.006643932400562038]
[2024-04-17 12:20:05,720: INFO: main: Training : batch 358 Loss: 0.008964574531006983]
[2024-04-17 12:20:06,352: INFO: main: Training : batch 359 Loss: 0.022942098348853433]
[2024-04-17 12:20:06,986: INFO: main: Training : batch 360 Loss: 0.014677774986156253]
[2024-04-17 12:20:07,627: INFO: main: Training : batch 361 Loss: 0.009842533493395596]
[2024-04-17 12:20:08,263: INFO: main: Training : batch 362 Loss: 0.006249238657800903]
[2024-04-17 12:20:08,903: INFO: main: Training : batch 363 Loss: 0.01840684154577997]
[2024-04-17 12:20:09,551: INFO: main: Training : batch 364 Loss: 0.01627547324606787]
[2024-04-17 12:20:10,190: INFO: main: Training : batch 365 Loss: 0.007671241224160798]
[2024-04-17 12:20:10,819: INFO: main: Training : batch 366 Loss: 0.01553687124461237]
[2024-04-17 12:20:11,451: INFO: main: Training : batch 367 Loss: 0.019675386266771278]
[2024-04-17 12:20:12,084: INFO: main: Training : batch 368 Loss: 0.0037151894869125344]
[2024-04-17 12:20:12,721: INFO: main: Training : batch 369 Loss: 0.014842015612752847]
[2024-04-17 12:20:13,350: INFO: main: Training : batch 370 Loss: 0.0038407189450664144]
[2024-04-17 12:20:13,986: INFO: main: Training : batch 371 Loss: 0.010714708700552732]
[2024-04-17 12:20:14,616: INFO: main: Training : batch 372 Loss: 0.023710509084053887]
[2024-04-17 12:20:15,247: INFO: main: Training : batch 373 Loss: 0.008183024147306553]
[2024-04-17 12:20:15,879: INFO: main: Training : batch 374 Loss: 0.015544572794142373]
[2024-04-17 12:20:16,511: INFO: main: Training : batch 375 Loss: 0.010218721207328784]
[2024-04-17 12:20:17,142: INFO: main: Training : batch 376 Loss: 0.0174364545265651]
[2024-04-17 12:20:17,775: INFO: main: Training : batch 377 Loss: 0.03448798474050961]
[2024-04-17 12:20:18,404: INFO: main: Training : batch 378 Loss: 0.003408804905228932]
[2024-04-17 12:20:19,033: INFO: main: Training : batch 379 Loss: 0.005517459936399635]
[2024-04-17 12:20:19,662: INFO: main: Training : batch 380 Loss: 0.0052151854890130625]
[2024-04-17 12:20:20,290: INFO: main: Training : batch 381 Loss: 0.014278191089774843]
[2024-04-17 12:20:20,925: INFO: main: Training : batch 382 Loss: 0.004997493056073552]
[2024-04-17 12:20:21,563: INFO: main: Training : batch 383 Loss: 0.009356791460960965]
[2024-04-17 12:20:22,202: INFO: main: Training : batch 384 Loss: 0.005351416861357986]
[2024-04-17 12:20:22,840: INFO: main: Training : batch 385 Loss: 0.010032770399424547]
[2024-04-17 12:20:23,480: INFO: main: Training : batch 386 Loss: 0.00466690584239692]
[2024-04-17 12:20:24,109: INFO: main: Training : batch 387 Loss: 0.018788419974877096]
[2024-04-17 12:20:24,742: INFO: main: Training : batch 388 Loss: 0.052288557662902926]
[2024-04-17 12:20:25,379: INFO: main: Training : batch 389 Loss: 0.01638679222011199]
[2024-04-17 12:20:26,006: INFO: main: Training : batch 390 Loss: 0.028334856719389803]
[2024-04-17 12:20:26,644: INFO: main: Training : batch 391 Loss: 0.006581993029374681]
[2024-04-17 12:20:27,274: INFO: main: Training : batch 392 Loss: 0.029106869818261324]
[2024-04-17 12:20:27,909: INFO: main: Training : batch 393 Loss: 0.00955333301880824]
[2024-04-17 12:20:28,542: INFO: main: Training : batch 394 Loss: 0.013750530550171091]
[2024-04-17 12:20:29,171: INFO: main: Training : batch 395 Loss: 0.014093231458215867]
[2024-04-17 12:20:29,805: INFO: main: Training : batch 396 Loss: 0.0011201941901566208]
[2024-04-17 12:20:30,437: INFO: main: Training : batch 397 Loss: 0.005837478430249814]
[2024-04-17 12:20:31,065: INFO: main: Training : batch 398 Loss: 0.014499225631089827]
[2024-04-17 12:20:31,692: INFO: main: Training : batch 399 Loss: 0.019068068032903493]
[2024-04-17 12:20:32,321: INFO: main: Training : batch 400 Loss: 0.01365978740647471]
[2024-04-17 12:20:32,947: INFO: main: Training : batch 401 Loss: 0.03169079116105479]
[2024-04-17 12:20:33,577: INFO: main: Training : batch 402 Loss: 0.009828888981370333]
[2024-04-17 12:20:34,215: INFO: main: Training : batch 403 Loss: 0.012533699155151136]
[2024-04-17 12:20:34,853: INFO: main: Training : batch 404 Loss: 0.0023918267203683944]
[2024-04-17 12:20:35,489: INFO: main: Training : batch 405 Loss: 0.005732035296246974]
[2024-04-17 12:20:36,123: INFO: main: Training : batch 406 Loss: 0.013279023372229186]
[2024-04-17 12:20:36,761: INFO: main: Training : batch 407 Loss: 0.017343736868997893]
[2024-04-17 12:20:37,395: INFO: main: Training : batch 408 Loss: 0.0066677448711247225]
[2024-04-17 12:20:38,024: INFO: main: Training : batch 409 Loss: 0.014194139308867685]
[2024-04-17 12:20:38,652: INFO: main: Training : batch 410 Loss: 0.011430904947394663]
[2024-04-17 12:20:39,283: INFO: main: Training : batch 411 Loss: 0.025467972742543886]
[2024-04-17 12:20:39,910: INFO: main: Training : batch 412 Loss: 0.0078118839522357585]
[2024-04-17 12:20:40,538: INFO: main: Training : batch 413 Loss: 0.010031688927664182]
[2024-04-17 12:20:41,171: INFO: main: Training : batch 414 Loss: 0.007548408288477947]
[2024-04-17 12:20:41,799: INFO: main: Training : batch 415 Loss: 0.007725383045891455]
[2024-04-17 12:20:42,430: INFO: main: Training : batch 416 Loss: 0.014014182237447924]
[2024-04-17 12:20:43,058: INFO: main: Training : batch 417 Loss: 0.0068127235184286625]
[2024-04-17 12:20:43,690: INFO: main: Training : batch 418 Loss: 0.027487734377833698]
[2024-04-17 12:20:44,318: INFO: main: Training : batch 419 Loss: 0.013309566801267913]
[2024-04-17 12:20:44,944: INFO: main: Training : batch 420 Loss: 0.01842320397763001]
[2024-04-17 12:20:45,573: INFO: main: Training : batch 421 Loss: 0.015677356906900763]
[2024-04-17 12:20:46,201: INFO: main: Training : batch 422 Loss: 0.011086194168817023]
[2024-04-17 12:20:46,834: INFO: main: Training : batch 423 Loss: 0.023100707818152582]
[2024-04-17 12:20:47,470: INFO: main: Training : batch 424 Loss: 0.006401958815343913]
[2024-04-17 12:20:48,104: INFO: main: Training : batch 425 Loss: 0.0037263186813050386]
[2024-04-17 12:20:48,739: INFO: main: Training : batch 426 Loss: 0.011204499373613517]
[2024-04-17 12:20:49,375: INFO: main: Training : batch 427 Loss: 0.06627976878359115]
[2024-04-17 12:20:50,012: INFO: main: Training : batch 428 Loss: 0.043684051462009245]
[2024-04-17 12:20:50,644: INFO: main: Training : batch 429 Loss: 0.014784233564277497]
[2024-04-17 12:20:51,271: INFO: main: Training : batch 430 Loss: 0.005506709199946887]
[2024-04-17 12:20:51,898: INFO: main: Training : batch 431 Loss: 0.006441069542868341]
[2024-04-17 12:20:52,532: INFO: main: Training : batch 432 Loss: 0.011199650763340207]
[2024-04-17 12:20:53,158: INFO: main: Training : batch 433 Loss: 0.0053836777873049]
[2024-04-17 12:20:53,792: INFO: main: Training : batch 434 Loss: 0.009940963168379226]
[2024-04-17 12:20:54,424: INFO: main: Training : batch 435 Loss: 0.011692003243207313]
[2024-04-17 12:20:55,052: INFO: main: Training : batch 436 Loss: 0.020705285601042046]
[2024-04-17 12:20:55,683: INFO: main: Training : batch 437 Loss: 0.05412094337272634]
[2024-04-17 12:20:56,310: INFO: main: Training : batch 438 Loss: 0.0065970805289286396]
[2024-04-17 12:20:56,942: INFO: main: Training : batch 439 Loss: 0.006001155438616122]
[2024-04-17 12:20:57,569: INFO: main: Training : batch 440 Loss: 0.003100036371516161]
[2024-04-17 12:20:58,193: INFO: main: Training : batch 441 Loss: 0.012264271176542031]
[2024-04-17 12:20:58,827: INFO: main: Training : batch 442 Loss: 0.018886146124766197]
[2024-04-17 12:20:59,456: INFO: main: Training : batch 443 Loss: 0.016946555203490945]
[2024-04-17 12:21:00,092: INFO: main: Training : batch 444 Loss: 0.010743545832857712]
[2024-04-17 12:21:00,727: INFO: main: Training : batch 445 Loss: 0.015208866545804271]
[2024-04-17 12:21:01,367: INFO: main: Training : batch 446 Loss: 0.023265101283615728]
[2024-04-17 12:21:01,998: INFO: main: Training : batch 447 Loss: 0.00768725371267724]
[2024-04-17 12:21:02,634: INFO: main: Training : batch 448 Loss: 0.004102046402479721]
[2024-04-17 12:21:03,266: INFO: main: Training : batch 449 Loss: 0.01778569013697217]
[2024-04-17 12:21:03,894: INFO: main: Training : batch 450 Loss: 0.029869758975799635]
[2024-04-17 12:21:04,518: INFO: main: Training : batch 451 Loss: 0.004947602673859743]
[2024-04-17 12:21:05,148: INFO: main: Training : batch 452 Loss: 0.015585599609862416]
[2024-04-17 12:21:05,779: INFO: main: Training : batch 453 Loss: 0.009562389549875924]
[2024-04-17 12:21:06,409: INFO: main: Training : batch 454 Loss: 0.025524831858584483]
[2024-04-17 12:21:07,040: INFO: main: Training : batch 455 Loss: 0.032063686145536924]
[2024-04-17 12:21:07,672: INFO: main: Training : batch 456 Loss: 0.050379640474429783]
[2024-04-17 12:21:08,303: INFO: main: Training : batch 457 Loss: 0.031773044933592]
[2024-04-17 12:21:08,933: INFO: main: Training : batch 458 Loss: 0.015623869186660203]
[2024-04-17 12:21:09,559: INFO: main: Training : batch 459 Loss: 0.014293613557608408]
[2024-04-17 12:21:10,194: INFO: main: Training : batch 460 Loss: 0.012694460701942512]
[2024-04-17 12:21:10,822: INFO: main: Training : batch 461 Loss: 0.008468144606232004]
[2024-04-17 12:21:11,456: INFO: main: Training : batch 462 Loss: 0.004045036305885453]
[2024-04-17 12:21:12,085: INFO: main: Training : batch 463 Loss: 0.02405984816231688]
[2024-04-17 12:21:12,712: INFO: main: Training : batch 464 Loss: 0.011019860710696588]
[2024-04-17 12:21:13,353: INFO: main: Training : batch 465 Loss: 0.014076921579062603]
[2024-04-17 12:21:13,988: INFO: main: Training : batch 466 Loss: 0.010143797570452395]
[2024-04-17 12:21:14,627: INFO: main: Training : batch 467 Loss: 0.008373820815995768]
[2024-04-17 12:21:15,262: INFO: main: Training : batch 468 Loss: 0.011762002965480389]
[2024-04-17 12:21:15,903: INFO: main: Training : batch 469 Loss: 0.016427067568990778]
[2024-04-17 12:21:16,534: INFO: main: Training : batch 470 Loss: 0.009583444039521256]
[2024-04-17 12:21:17,163: INFO: main: Training : batch 471 Loss: 0.006647704293446792]
[2024-04-17 12:21:17,792: INFO: main: Training : batch 472 Loss: 0.0766838821614921]
[2024-04-17 12:21:18,420: INFO: main: Training : batch 473 Loss: 0.020801103252989214]
[2024-04-17 12:21:19,048: INFO: main: Training : batch 474 Loss: 0.0083269170672981]
[2024-04-17 12:21:19,678: INFO: main: Training : batch 475 Loss: 0.01253068782895527]
[2024-04-17 12:21:20,308: INFO: main: Training : batch 476 Loss: 0.01075656474781736]
[2024-04-17 12:21:20,940: INFO: main: Training : batch 477 Loss: 0.006465491977684409]
[2024-04-17 12:21:21,574: INFO: main: Training : batch 478 Loss: 0.020192311191142294]
[2024-04-17 12:21:22,204: INFO: main: Training : batch 479 Loss: 0.011306107965278867]
[2024-04-17 12:21:22,834: INFO: main: Training : batch 480 Loss: 0.0030470433330766697]
[2024-04-17 12:21:23,462: INFO: main: Training : batch 481 Loss: 0.018505622852321584]
[2024-04-17 12:21:24,095: INFO: main: Training : batch 482 Loss: 0.004724215782568606]
[2024-04-17 12:21:24,727: INFO: main: Training : batch 483 Loss: 0.011280631784911375]
[2024-04-17 12:21:25,361: INFO: main: Training : batch 484 Loss: 0.006637063017173082]
[2024-04-17 12:21:25,991: INFO: main: Training : batch 485 Loss: 0.024012288288796726]
[2024-04-17 12:21:26,627: INFO: main: Training : batch 486 Loss: 0.016450466399351108]
[2024-04-17 12:21:27,264: INFO: main: Training : batch 487 Loss: 0.016408376605963546]
[2024-04-17 12:21:27,894: INFO: main: Training : batch 488 Loss: 0.006552659577513594]
[2024-04-17 12:21:28,527: INFO: main: Training : batch 489 Loss: 0.009390574344694192]
[2024-04-17 12:21:29,158: INFO: main: Training : batch 490 Loss: 0.012415458856126427]
[2024-04-17 12:21:29,788: INFO: main: Training : batch 491 Loss: 0.014819808761041131]
[2024-04-17 12:21:30,421: INFO: main: Training : batch 492 Loss: 0.01589821649028311]
[2024-04-17 12:21:31,051: INFO: main: Training : batch 493 Loss: 0.008704851550748521]
[2024-04-17 12:21:31,679: INFO: main: Training : batch 494 Loss: 0.012864467007690039]
[2024-04-17 12:21:32,307: INFO: main: Training : batch 495 Loss: 0.007966672424105214]
[2024-04-17 12:21:32,945: INFO: main: Training : batch 496 Loss: 0.012655248648490977]
[2024-04-17 12:21:33,577: INFO: main: Training : batch 497 Loss: 0.003672822292521613]
[2024-04-17 12:21:34,215: INFO: main: Training : batch 498 Loss: 0.01720143898890457]
[2024-04-17 12:21:34,845: INFO: main: Training : batch 499 Loss: 0.0017966956431527843]
[2024-04-17 12:21:35,479: INFO: main: Training : batch 500 Loss: 0.03393091253163831]
[2024-04-17 12:21:36,111: INFO: main: Training : batch 501 Loss: 0.021650554488660313]
[2024-04-17 12:21:36,745: INFO: main: Training : batch 502 Loss: 0.010084101287177093]
[2024-04-17 12:21:37,373: INFO: main: Training : batch 503 Loss: 0.005468941091478845]
[2024-04-17 12:21:38,005: INFO: main: Training : batch 504 Loss: 0.0014336293186411324]
[2024-04-17 12:21:38,639: INFO: main: Training : batch 505 Loss: 0.008285089627857423]
[2024-04-17 12:21:39,269: INFO: main: Training : batch 506 Loss: 0.009330523516870134]
[2024-04-17 12:21:39,906: INFO: main: Training : batch 507 Loss: 0.005837613147144132]
[2024-04-17 12:21:40,549: INFO: main: Training : batch 508 Loss: 0.01679928303439468]
[2024-04-17 12:21:41,183: INFO: main: Training : batch 509 Loss: 0.008015394140866607]
[2024-04-17 12:21:41,824: INFO: main: Training : batch 510 Loss: 0.007422533889145784]
[2024-04-17 12:21:42,464: INFO: main: Training : batch 511 Loss: 0.027465574055011963]
[2024-04-17 12:21:43,097: INFO: main: Training : batch 512 Loss: 0.03138622048041286]
[2024-04-17 12:21:43,720: INFO: main: Training : batch 513 Loss: 0.008418542465678008]
[2024-04-17 12:21:44,353: INFO: main: Training : batch 514 Loss: 0.03333193127539768]
[2024-04-17 12:21:44,989: INFO: main: Training : batch 515 Loss: 0.0013697227323357735]
[2024-04-17 12:21:45,623: INFO: main: Training : batch 516 Loss: 0.006323412786553245]
[2024-04-17 12:21:46,252: INFO: main: Training : batch 517 Loss: 0.02817048768748444]
[2024-04-17 12:21:46,881: INFO: main: Training : batch 518 Loss: 0.03117405906844543]
[2024-04-17 12:21:47,517: INFO: main: Training : batch 519 Loss: 0.01657741176839147]
[2024-04-17 12:21:48,147: INFO: main: Training : batch 520 Loss: 0.018916538424093042]
[2024-04-17 12:21:48,779: INFO: main: Training : batch 521 Loss: 0.009387778789774768]
[2024-04-17 12:21:49,410: INFO: main: Training : batch 522 Loss: 0.03106570958974812]
[2024-04-17 12:21:50,042: INFO: main: Training : batch 523 Loss: 0.014309837634349551]
[2024-04-17 12:21:50,676: INFO: main: Training : batch 524 Loss: 0.004634006774676866]
[2024-04-17 12:21:51,309: INFO: main: Training : batch 525 Loss: 0.01626376965953141]
[2024-04-17 12:21:51,941: INFO: main: Training : batch 526 Loss: 0.01657430887204073]
[2024-04-17 12:21:52,573: INFO: main: Training : batch 527 Loss: 0.005196372751748768]
[2024-04-17 12:21:53,209: INFO: main: Training : batch 528 Loss: 0.006890072653828452]
[2024-04-17 12:21:53,852: INFO: main: Training : batch 529 Loss: 0.005190852411405595]
[2024-04-17 12:21:54,485: INFO: main: Training : batch 530 Loss: 0.008339549159525861]
[2024-04-17 12:21:55,127: INFO: main: Training : batch 531 Loss: 0.02055956141280493]
[2024-04-17 12:21:55,764: INFO: main: Training : batch 532 Loss: 0.012998349499189796]
[2024-04-17 12:21:56,394: INFO: main: Training : batch 533 Loss: 0.012978254052090882]
[2024-04-17 12:21:57,023: INFO: main: Training : batch 534 Loss: 0.007642621795021456]
[2024-04-17 12:21:57,653: INFO: main: Training : batch 535 Loss: 0.004695052421285308]
[2024-04-17 12:21:58,286: INFO: main: Training : batch 536 Loss: 0.007685563368488927]
[2024-04-17 12:21:58,921: INFO: main: Training : batch 537 Loss: 0.0060771755870339395]
[2024-04-17 12:21:59,555: INFO: main: Training : batch 538 Loss: 0.017653366376524336]
[2024-04-17 12:22:00,180: INFO: main: Training : batch 539 Loss: 0.0081734118079985]
[2024-04-17 12:22:00,811: INFO: main: Training : batch 540 Loss: 0.006581025702161031]
[2024-04-17 12:22:01,436: INFO: main: Training : batch 541 Loss: 0.0045351944620681305]
[2024-04-17 12:22:02,073: INFO: main: Training : batch 542 Loss: 0.00977998342307341]
[2024-04-17 12:22:02,698: INFO: main: Training : batch 543 Loss: 0.012676004950369822]
[2024-04-17 12:22:03,330: INFO: main: Training : batch 544 Loss: 0.016074624932428824]
[2024-04-17 12:22:03,964: INFO: main: Training : batch 545 Loss: 0.010601527190351089]
[2024-04-17 12:22:04,596: INFO: main: Training : batch 546 Loss: 0.005321042621872712]
[2024-04-17 12:22:05,224: INFO: main: Training : batch 547 Loss: 0.017084900285104933]
[2024-04-17 12:22:05,856: INFO: main: Training : batch 548 Loss: 0.007819945872941585]
[2024-04-17 12:22:06,494: INFO: main: Training : batch 549 Loss: 0.0044778983114963375]
[2024-04-17 12:22:07,133: INFO: main: Training : batch 550 Loss: 0.008053510748300461]
[2024-04-17 12:22:07,771: INFO: main: Training : batch 551 Loss: 0.0002621322333469913]
[2024-04-17 12:22:08,408: INFO: main: Training : batch 552 Loss: 0.03459033800405624]
[2024-04-17 12:22:09,045: INFO: main: Training : batch 553 Loss: 0.020912668605823453]
[2024-04-17 12:22:09,677: INFO: main: Training : batch 554 Loss: 0.033467384251002194]
[2024-04-17 12:22:10,313: INFO: main: Training : batch 555 Loss: 0.014028607452329195]
[2024-04-17 12:22:10,942: INFO: main: Training : batch 556 Loss: 0.004607289837882412]
[2024-04-17 12:22:11,570: INFO: main: Training : batch 557 Loss: 0.005243737404923421]
[2024-04-17 12:22:12,203: INFO: main: Training : batch 558 Loss: 0.004990714218834794]
[2024-04-17 12:22:12,833: INFO: main: Training : batch 559 Loss: 0.005325717577284442]
[2024-04-17 12:22:13,467: INFO: main: Training : batch 560 Loss: 0.007821459760833236]
[2024-04-17 12:22:14,098: INFO: main: Training : batch 561 Loss: 0.010006141328728228]
[2024-04-17 12:22:14,731: INFO: main: Training : batch 562 Loss: 0.021930709501015]
[2024-04-17 12:22:15,361: INFO: main: Training : batch 563 Loss: 0.0026804160306376484]
[2024-04-17 12:22:15,993: INFO: main: Training : batch 564 Loss: 0.018326990134284935]
[2024-04-17 12:22:16,628: INFO: main: Training : batch 565 Loss: 0.01378176814899551]
[2024-04-17 12:22:17,260: INFO: main: Training : batch 566 Loss: 0.007560971762318811]
[2024-04-17 12:22:17,890: INFO: main: Training : batch 567 Loss: 0.008540791731325634]
[2024-04-17 12:22:18,529: INFO: main: Training : batch 568 Loss: 0.002076198196204761]
[2024-04-17 12:22:19,160: INFO: main: Training : batch 569 Loss: 0.0007493409494574066]
[2024-04-17 12:22:19,793: INFO: main: Training : batch 570 Loss: 0.026446765500375426]
[2024-04-17 12:22:20,439: INFO: main: Training : batch 571 Loss: 0.006402860489717285]
[2024-04-17 12:22:21,072: INFO: main: Training : batch 572 Loss: 0.003455467864967842]
[2024-04-17 12:22:21,715: INFO: main: Training : batch 573 Loss: 0.007222093302722172]
[2024-04-17 12:22:22,355: INFO: main: Training : batch 574 Loss: 0.005660166648551254]
[2024-04-17 12:22:22,987: INFO: main: Training : batch 575 Loss: 0.017862196109013583]
[2024-04-17 12:22:23,621: INFO: main: Training : batch 576 Loss: 0.022984991209183264]
[2024-04-17 12:22:24,257: INFO: main: Training : batch 577 Loss: 0.01407519142118359]
[2024-04-17 12:22:24,886: INFO: main: Training : batch 578 Loss: 0.00753367482962645]
[2024-04-17 12:22:25,519: INFO: main: Training : batch 579 Loss: 0.010307602397709423]
[2024-04-17 12:22:26,148: INFO: main: Training : batch 580 Loss: 0.006059668668422603]
[2024-04-17 12:22:26,777: INFO: main: Training : batch 581 Loss: 0.02032091651206539]
[2024-04-17 12:22:27,419: INFO: main: Training : batch 582 Loss: 0.007850464936435382]
[2024-04-17 12:22:28,053: INFO: main: Training : batch 583 Loss: 0.03378550518480966]
[2024-04-17 12:22:28,687: INFO: main: Training : batch 584 Loss: 0.007822243715397812]
[2024-04-17 12:22:29,317: INFO: main: Training : batch 585 Loss: 0.00580912145326097]
[2024-04-17 12:22:29,947: INFO: main: Training : batch 586 Loss: 0.014278420211854112]
[2024-04-17 12:22:30,576: INFO: main: Training : batch 587 Loss: 0.015406184761982083]
[2024-04-17 12:22:31,209: INFO: main: Training : batch 588 Loss: 0.010092464853527763]
[2024-04-17 12:22:31,842: INFO: main: Training : batch 589 Loss: 0.003184526950434654]
[2024-04-17 12:22:32,475: INFO: main: Training : batch 590 Loss: 0.019207014668684724]
[2024-04-17 12:22:33,109: INFO: main: Training : batch 591 Loss: 0.07767839114722858]
[2024-04-17 12:22:33,753: INFO: main: Training : batch 592 Loss: 0.03292799986254632]
[2024-04-17 12:22:34,390: INFO: main: Training : batch 593 Loss: 0.006761808526735767]
[2024-04-17 12:22:35,032: INFO: main: Training : batch 594 Loss: 0.02752578183098499]
[2024-04-17 12:22:35,670: INFO: main: Training : batch 595 Loss: 0.004272582011358111]
[2024-04-17 12:22:36,301: INFO: main: Training : batch 596 Loss: 0.022829016437287318]
[2024-04-17 12:22:36,930: INFO: main: Training : batch 597 Loss: 0.00908710789417332]
[2024-04-17 12:22:37,558: INFO: main: Training : batch 598 Loss: 0.005248110913381823]
[2024-04-17 12:22:38,187: INFO: main: Training : batch 599 Loss: 0.042528832328619134]
[2024-04-17 12:22:38,818: INFO: main: Training : batch 600 Loss: 0.02499049990060975]
[2024-04-17 12:22:39,452: INFO: main: Training : batch 601 Loss: 0.041829978299399585]
[2024-04-17 12:22:40,081: INFO: main: Training : batch 602 Loss: 0.005636585601945732]
[2024-04-17 12:22:40,712: INFO: main: Training : batch 603 Loss: 0.01030898353887704]
[2024-04-17 12:22:41,346: INFO: main: Training : batch 604 Loss: 0.019120018068147823]
[2024-04-17 12:22:41,974: INFO: main: Training : batch 605 Loss: 0.006141116259852302]
[2024-04-17 12:22:42,602: INFO: main: Training : batch 606 Loss: 0.007588352173844822]
[2024-04-17 12:22:43,232: INFO: main: Training : batch 607 Loss: 0.011851740694145192]
[2024-04-17 12:22:43,860: INFO: main: Training : batch 608 Loss: 0.011672139832176803]
[2024-04-17 12:22:44,497: INFO: main: Training : batch 609 Loss: 0.005452374134588137]
[2024-04-17 12:22:45,128: INFO: main: Training : batch 610 Loss: 0.014829936607961301]
[2024-04-17 12:22:45,756: INFO: main: Training : batch 611 Loss: 0.0076880676483608796]
[2024-04-17 12:22:46,396: INFO: main: Training : batch 612 Loss: 0.0501802848646806]
[2024-04-17 12:22:47,031: INFO: main: Training : batch 613 Loss: 0.00837882639273155]
[2024-04-17 12:22:47,673: INFO: main: Training : batch 614 Loss: 0.019041811318139104]
[2024-04-17 12:22:48,308: INFO: main: Training : batch 615 Loss: 0.0053567899808212366]
[2024-04-17 12:22:48,946: INFO: main: Training : batch 616 Loss: 0.01455427890728217]
[2024-04-17 12:22:49,582: INFO: main: Training : batch 617 Loss: 0.008728508809480331]
[2024-04-17 12:22:50,209: INFO: main: Training : batch 618 Loss: 0.053173951384697486]
[2024-04-17 12:22:50,844: INFO: main: Training : batch 619 Loss: 0.008333839894423694]
[2024-04-17 12:22:51,474: INFO: main: Training : batch 620 Loss: 0.009446547295607332]
[2024-04-17 12:22:52,108: INFO: main: Training : batch 621 Loss: 0.025853927815634278]
[2024-04-17 12:22:52,741: INFO: main: Training : batch 622 Loss: 0.028814348005006395]
[2024-04-17 12:22:53,377: INFO: main: Training : batch 623 Loss: 0.0070705820868642855]
[2024-04-17 12:22:54,005: INFO: main: Training : batch 624 Loss: 0.031360838452247734]
[2024-04-17 12:22:54,634: INFO: main: Training : batch 625 Loss: 0.008625870010692182]
[2024-04-17 12:22:55,264: INFO: main: Training : batch 626 Loss: 0.01184082628564514]
[2024-04-17 12:22:55,895: INFO: main: Training : batch 627 Loss: 0.017677417896478618]
[2024-04-17 12:22:56,522: INFO: main: Training : batch 628 Loss: 0.021310300376450948]
[2024-04-17 12:22:57,153: INFO: main: Training : batch 629 Loss: 0.019803685211817766]
[2024-04-17 12:22:57,779: INFO: main: Training : batch 630 Loss: 0.00904158643347934]
[2024-04-17 12:22:58,408: INFO: main: Training : batch 631 Loss: 0.017605670404128672]
[2024-04-17 12:22:59,039: INFO: main: Training : batch 632 Loss: 0.025462959273372856]
[2024-04-17 12:22:59,686: INFO: main: Training : batch 633 Loss: 0.007419126885798367]
[2024-04-17 12:23:00,317: INFO: main: Training : batch 634 Loss: 0.02328627692807413]
[2024-04-17 12:23:00,955: INFO: main: Training : batch 635 Loss: 0.013272929977238944]
[2024-04-17 12:23:01,589: INFO: main: Training : batch 636 Loss: 0.014209577791920943]
[2024-04-17 12:23:02,225: INFO: main: Training : batch 637 Loss: 0.011993713558777633]
[2024-04-17 12:23:02,853: INFO: main: Training : batch 638 Loss: 0.006286468382481692]
[2024-04-17 12:23:03,485: INFO: main: Training : batch 639 Loss: 0.007258226768808816]
[2024-04-17 12:23:04,114: INFO: main: Training : batch 640 Loss: 0.012686173660076096]
[2024-04-17 12:23:04,746: INFO: main: Training : batch 641 Loss: 0.013702329200568065]
[2024-04-17 12:23:05,379: INFO: main: Training : batch 642 Loss: 0.008028186782949493]
[2024-04-17 12:23:06,006: INFO: main: Training : batch 643 Loss: 0.008284020464018731]
[2024-04-17 12:23:06,636: INFO: main: Training : batch 644 Loss: 0.010313971982179006]
[2024-04-17 12:23:07,269: INFO: main: Training : batch 645 Loss: 0.013679678131102359]
[2024-04-17 12:23:07,898: INFO: main: Training : batch 646 Loss: 0.003986566618478136]
[2024-04-17 12:23:08,528: INFO: main: Training : batch 647 Loss: 0.018461276395394518]
[2024-04-17 12:23:09,162: INFO: main: Training : batch 648 Loss: 0.004791886939318793]
[2024-04-17 12:23:09,795: INFO: main: Training : batch 649 Loss: 0.03291883335540942]
[2024-04-17 12:23:10,426: INFO: main: Training : batch 650 Loss: 0.034355423048344644]
[2024-04-17 12:23:11,057: INFO: main: Training : batch 651 Loss: 0.0014778235901038818]
[2024-04-17 12:23:11,687: INFO: main: Training : batch 652 Loss: 0.022247028625371614]
[2024-04-17 12:23:12,328: INFO: main: Training : batch 653 Loss: 0.005424776339006591]
[2024-04-17 12:23:12,964: INFO: main: Training : batch 654 Loss: 0.02072318760822633]
[2024-04-17 12:23:13,599: INFO: main: Training : batch 655 Loss: 0.005545540826219139]
[2024-04-17 12:23:14,240: INFO: main: Training : batch 656 Loss: 0.016093161612254776]
[2024-04-17 12:23:14,880: INFO: main: Training : batch 657 Loss: 0.013846822250722614]
[2024-04-17 12:23:15,519: INFO: main: Training : batch 658 Loss: 0.02030006347739429]
[2024-04-17 12:23:16,156: INFO: main: Training : batch 659 Loss: 0.0023884066858854557]
[2024-04-17 12:23:16,784: INFO: main: Training : batch 660 Loss: 0.012026034165064316]
[2024-04-17 12:23:17,414: INFO: main: Training : batch 661 Loss: 0.006886353316793607]
[2024-04-17 12:23:18,045: INFO: main: Training : batch 662 Loss: 0.0018126387319708941]
[2024-04-17 12:23:18,679: INFO: main: Training : batch 663 Loss: 0.005085715820013678]
[2024-04-17 12:23:19,310: INFO: main: Training : batch 664 Loss: 0.0052650176875823134]
[2024-04-17 12:23:19,939: INFO: main: Training : batch 665 Loss: 0.002871670579555461]
[2024-04-17 12:23:20,570: INFO: main: Training : batch 666 Loss: 0.020303442658788126]
[2024-04-17 12:23:21,197: INFO: main: Training : batch 667 Loss: 0.02677965726755477]
[2024-04-17 12:23:21,828: INFO: main: Training : batch 668 Loss: 0.003696843144704378]
[2024-04-17 12:23:22,457: INFO: main: Training : batch 669 Loss: 0.015233836801031237]
[2024-04-17 12:23:23,087: INFO: main: Training : batch 670 Loss: 0.013117391056796603]
[2024-04-17 12:23:23,715: INFO: main: Training : batch 671 Loss: 0.006127594838026993]
[2024-04-17 12:23:24,342: INFO: main: Training : batch 672 Loss: 0.011396761531312475]
[2024-04-17 12:23:24,974: INFO: main: Training : batch 673 Loss: 0.0030589552800858698]
[2024-04-17 12:23:25,606: INFO: main: Training : batch 674 Loss: 0.006947266745514708]
[2024-04-17 12:23:26,246: INFO: main: Training : batch 675 Loss: 0.01375004312250491]
[2024-04-17 12:23:26,881: INFO: main: Training : batch 676 Loss: 0.023796842592264427]
[2024-04-17 12:23:27,523: INFO: main: Training : batch 677 Loss: 0.014434313190869534]
[2024-04-17 12:23:28,164: INFO: main: Training : batch 678 Loss: 0.0024372743059938814]
[2024-04-17 12:23:28,797: INFO: main: Training : batch 679 Loss: 0.008090753049335767]
[2024-04-17 12:23:29,426: INFO: main: Training : batch 680 Loss: 0.007790419990766139]
[2024-04-17 12:23:30,055: INFO: main: Training : batch 681 Loss: 0.014377214351064036]
[2024-04-17 12:23:30,683: INFO: main: Training : batch 682 Loss: 0.010844706502244464]
[2024-04-17 12:23:31,314: INFO: main: Training : batch 683 Loss: 0.0061146195367142156]
[2024-04-17 12:23:31,946: INFO: main: Training : batch 684 Loss: 0.00718008787279994]
[2024-04-17 12:23:32,574: INFO: main: Training : batch 685 Loss: 0.007463446721335725]
[2024-04-17 12:23:33,205: INFO: main: Training : batch 686 Loss: 0.0037956923763098407]
[2024-04-17 12:23:33,837: INFO: main: Training : batch 687 Loss: 0.020046940250119428]
[2024-04-17 12:23:34,468: INFO: main: Training : batch 688 Loss: 0.013804733796147418]
[2024-04-17 12:23:35,099: INFO: main: Training : batch 689 Loss: 0.017562871120629812]
[2024-04-17 12:23:35,726: INFO: main: Training : batch 690 Loss: 0.01845009963772742]
[2024-04-17 12:23:36,353: INFO: main: Training : batch 691 Loss: 0.015619336991023977]
[2024-04-17 12:23:36,980: INFO: main: Training : batch 692 Loss: 0.00878885181497389]
[2024-04-17 12:23:37,607: INFO: main: Training : batch 693 Loss: 0.014386751249760474]
[2024-04-17 12:23:38,241: INFO: main: Training : batch 694 Loss: 0.006487421522065488]
[2024-04-17 12:23:38,869: INFO: main: Training : batch 695 Loss: 0.0062318687183754605]
[2024-04-17 12:23:39,518: INFO: main: Training : batch 696 Loss: 0.01925636688801227]
[2024-04-17 12:23:40,157: INFO: main: Training : batch 697 Loss: 0.012393609312501538]
[2024-04-17 12:23:40,790: INFO: main: Training : batch 698 Loss: 0.03393594357983138]
[2024-04-17 12:23:41,425: INFO: main: Training : batch 699 Loss: 0.018776990301961654]
[2024-04-17 12:23:42,069: INFO: main: Training : batch 700 Loss: 0.009929540155484466]
[2024-04-17 12:23:42,699: INFO: main: Training : batch 701 Loss: 0.008793107443934729]
[2024-04-17 12:23:43,324: INFO: main: Training : batch 702 Loss: 0.033359726705220985]
[2024-04-17 12:23:43,952: INFO: main: Training : batch 703 Loss: 0.01422253075438631]
[2024-04-17 12:23:44,582: INFO: main: Training : batch 704 Loss: 0.009614383309932315]
[2024-04-17 12:23:45,211: INFO: main: Training : batch 705 Loss: 0.010614056688490383]
[2024-04-17 12:23:45,842: INFO: main: Training : batch 706 Loss: 0.01750640861050556]
[2024-04-17 12:23:46,477: INFO: main: Training : batch 707 Loss: 0.0733262741503414]
[2024-04-17 12:23:47,110: INFO: main: Training : batch 708 Loss: 0.020553249494059254]
[2024-04-17 12:23:47,737: INFO: main: Training : batch 709 Loss: 0.019029172356098304]
[2024-04-17 12:23:48,369: INFO: main: Training : batch 710 Loss: 0.002342668769650967]
[2024-04-17 12:23:48,999: INFO: main: Training : batch 711 Loss: 0.002414301796851183]
[2024-04-17 12:23:49,628: INFO: main: Training : batch 712 Loss: 0.02260601392233732]
[2024-04-17 12:23:50,260: INFO: main: Training : batch 713 Loss: 0.022697725556585297]
[2024-04-17 12:23:50,886: INFO: main: Training : batch 714 Loss: 0.013326140141866023]
[2024-04-17 12:23:51,520: INFO: main: Training : batch 715 Loss: 0.020852469371164084]
[2024-04-17 12:23:52,159: INFO: main: Training : batch 716 Loss: 0.0056856672048007486]
[2024-04-17 12:23:52,790: INFO: main: Training : batch 717 Loss: 0.015919561953448264]
[2024-04-17 12:23:53,427: INFO: main: Training : batch 718 Loss: 0.008614554941205855]
[2024-04-17 12:23:54,064: INFO: main: Training : batch 719 Loss: 0.008451421519679902]
[2024-04-17 12:23:54,708: INFO: main: Training : batch 720 Loss: 0.007571956632313117]
[2024-04-17 12:23:55,342: INFO: main: Training : batch 721 Loss: 0.003233363667010195]
[2024-04-17 12:23:55,977: INFO: main: Training : batch 722 Loss: 0.0015232972787517294]
[2024-04-17 12:23:56,607: INFO: main: Training : batch 723 Loss: 0.0027537937812223026]
[2024-04-17 12:23:57,239: INFO: main: Training : batch 724 Loss: 0.010043702451937538]
[2024-04-17 12:23:57,871: INFO: main: Training : batch 725 Loss: 0.0059522753876768]
[2024-04-17 12:23:58,504: INFO: main: Training : batch 726 Loss: 0.014139597414826004]
[2024-04-17 12:23:59,141: INFO: main: Training : batch 727 Loss: 0.013808996177986824]
[2024-04-17 12:23:59,774: INFO: main: Training : batch 728 Loss: 0.006349007258011448]
[2024-04-17 12:24:00,401: INFO: main: Training : batch 729 Loss: 0.005478250709976027]
[2024-04-17 12:24:01,033: INFO: main: Training : batch 730 Loss: 0.0028251344620136925]
[2024-04-17 12:24:01,665: INFO: main: Training : batch 731 Loss: 0.010638017550938935]
[2024-04-17 12:24:02,300: INFO: main: Training : batch 732 Loss: 0.011427432713775313]
[2024-04-17 12:24:02,935: INFO: main: Training : batch 733 Loss: 0.01926658576769246]
[2024-04-17 12:24:03,567: INFO: main: Training : batch 734 Loss: 0.016548121331256972]
[2024-04-17 12:24:04,198: INFO: main: Training : batch 735 Loss: 0.007415484198138431]
[2024-04-17 12:24:04,826: INFO: main: Training : batch 736 Loss: 0.026785744573085652]
[2024-04-17 12:24:05,462: INFO: main: Training : batch 737 Loss: 0.011945901223550942]
[2024-04-17 12:24:06,099: INFO: main: Training : batch 738 Loss: 0.008544964120113092]
[2024-04-17 12:24:06,732: INFO: main: Training : batch 739 Loss: 0.0051059480335059505]
[2024-04-17 12:24:07,373: INFO: main: Training : batch 740 Loss: 0.013339845297509149]
[2024-04-17 12:24:08,008: INFO: main: Training : batch 741 Loss: 0.011835288088414464]
[2024-04-17 12:24:08,650: INFO: main: Training : batch 742 Loss: 0.024364553457773927]
[2024-04-17 12:24:09,281: INFO: main: Training : batch 743 Loss: 0.005563002656735953]
[2024-04-17 12:24:09,918: INFO: main: Training : batch 744 Loss: 0.014775903246714961]
[2024-04-17 12:24:10,543: INFO: main: Training : batch 745 Loss: 0.014304565536269943]
[2024-04-17 12:24:11,178: INFO: main: Training : batch 746 Loss: 0.006994197315421654]
[2024-04-17 12:24:11,810: INFO: main: Training : batch 747 Loss: 0.008677211470461937]
[2024-04-17 12:24:12,444: INFO: main: Training : batch 748 Loss: 0.008032311018024513]
[2024-04-17 12:24:13,074: INFO: main: Training : batch 749 Loss: 0.0561980262284953]
[2024-04-17 12:24:13,704: INFO: main: Training : batch 750 Loss: 0.015442754392012967]
[2024-04-17 12:24:14,330: INFO: main: Training : batch 751 Loss: 0.005194711947503641]
[2024-04-17 12:24:14,963: INFO: main: Training : batch 752 Loss: 0.011001809562629085]
[2024-04-17 12:24:15,595: INFO: main: Training : batch 753 Loss: 0.01791548056123593]
[2024-04-17 12:24:16,229: INFO: main: Training : batch 754 Loss: 0.012506875241713802]
[2024-04-17 12:24:16,862: INFO: main: Training : batch 755 Loss: 0.01009102249545551]
[2024-04-17 12:24:17,491: INFO: main: Training : batch 756 Loss: 0.010423909281978385]
[2024-04-17 12:24:18,124: INFO: main: Training : batch 757 Loss: 0.013187627525826287]
[2024-04-17 12:24:18,767: INFO: main: Training : batch 758 Loss: 0.030969255064407463]
[2024-04-17 12:24:19,403: INFO: main: Training : batch 759 Loss: 0.005718263242777921]
[2024-04-17 12:24:20,044: INFO: main: Training : batch 760 Loss: 0.00602566488848773]
[2024-04-17 12:24:20,681: INFO: main: Training : batch 761 Loss: 0.010032212686507107]
[2024-04-17 12:24:21,318: INFO: main: Training : batch 762 Loss: 0.006148506394787915]
[2024-04-17 12:24:21,955: INFO: main: Training : batch 763 Loss: 0.03572573850551907]
[2024-04-17 12:24:22,586: INFO: main: Training : batch 764 Loss: 0.008788117183072723]
[2024-04-17 12:24:23,214: INFO: main: Training : batch 765 Loss: 0.023677292680896477]
[2024-04-17 12:24:23,849: INFO: main: Training : batch 766 Loss: 0.006491280345582616]
[2024-04-17 12:24:24,476: INFO: main: Training : batch 767 Loss: 0.0035490629569617563]
[2024-04-17 12:24:25,104: INFO: main: Training : batch 768 Loss: 0.0027111048986716746]
[2024-04-17 12:24:25,736: INFO: main: Training : batch 769 Loss: 0.020026598874267307]
[2024-04-17 12:24:26,365: INFO: main: Training : batch 770 Loss: 0.04061835847954963]
[2024-04-17 12:24:26,999: INFO: main: Training : batch 771 Loss: 0.00728728215684611]
[2024-04-17 12:24:27,627: INFO: main: Training : batch 772 Loss: 0.03404786795011771]
[2024-04-17 12:24:28,261: INFO: main: Training : batch 773 Loss: 0.001218612747151058]
[2024-04-17 12:24:28,893: INFO: main: Training : batch 774 Loss: 0.018658861396459225]
[2024-04-17 12:24:29,524: INFO: main: Training : batch 775 Loss: 0.009802010626179924]
[2024-04-17 12:24:30,160: INFO: main: Training : batch 776 Loss: 0.009062015227561499]
[2024-04-17 12:24:30,790: INFO: main: Training : batch 777 Loss: 0.009494491515634442]
[2024-04-17 12:24:31,424: INFO: main: Training : batch 778 Loss: 0.02084515300026349]
[2024-04-17 12:24:32,066: INFO: main: Training : batch 779 Loss: 0.02044215053474581]
[2024-04-17 12:24:32,709: INFO: main: Training : batch 780 Loss: 0.03290735610853452]
[2024-04-17 12:24:33,346: INFO: main: Training : batch 781 Loss: 0.011353431962935883]
[2024-04-17 12:24:33,986: INFO: main: Training : batch 782 Loss: 0.009763303946562066]
[2024-04-17 12:24:34,626: INFO: main: Training : batch 783 Loss: 0.0037131551244679872]
[2024-04-17 12:24:35,256: INFO: main: Training : batch 784 Loss: 0.001008487726773466]
[2024-04-17 12:24:35,886: INFO: main: Training : batch 785 Loss: 0.011066037375375303]
[2024-04-17 12:24:36,524: INFO: main: Training : batch 786 Loss: 0.018032129440459177]
[2024-04-17 12:24:37,159: INFO: main: Training : batch 787 Loss: 0.0046589474721887075]
[2024-04-17 12:24:37,786: INFO: main: Training : batch 788 Loss: 0.00570833984636534]
[2024-04-17 12:24:38,414: INFO: main: Training : batch 789 Loss: 0.013034055118117957]
[2024-04-17 12:24:39,039: INFO: main: Training : batch 790 Loss: 0.020291776342484528]
[2024-04-17 12:24:39,669: INFO: main: Training : batch 791 Loss: 0.008825573080970435]
[2024-04-17 12:24:40,301: INFO: main: Training : batch 792 Loss: 0.0029874282371012313]
[2024-04-17 12:24:40,931: INFO: main: Training : batch 793 Loss: 0.01431809245873791]
[2024-04-17 12:24:41,560: INFO: main: Training : batch 794 Loss: 0.010048855213922509]
[2024-04-17 12:24:42,190: INFO: main: Training : batch 795 Loss: 0.016882104706274034]
[2024-04-17 12:24:42,819: INFO: main: Training : batch 796 Loss: 0.016090085892917647]
[2024-04-17 12:24:43,454: INFO: main: Training : batch 797 Loss: 0.02373951575787089]
[2024-04-17 12:24:44,090: INFO: main: Training : batch 798 Loss: 0.004176538708602995]
[2024-04-17 12:24:44,723: INFO: main: Training : batch 799 Loss: 0.007921865653733389]
[2024-04-17 12:24:45,356: INFO: main: Training : batch 800 Loss: 0.01995314189328591]
[2024-04-17 12:24:45,990: INFO: main: Training : batch 801 Loss: 0.006046433794523989]
[2024-04-17 12:24:46,627: INFO: main: Training : batch 802 Loss: 0.009134837363756311]
[2024-04-17 12:24:47,264: INFO: main: Training : batch 803 Loss: 0.01055907565044993]
[2024-04-17 12:24:47,906: INFO: main: Training : batch 804 Loss: 0.0022391059862148766]
[2024-04-17 12:24:48,538: INFO: main: Training : batch 805 Loss: 0.0032882179530359353]
[2024-04-17 12:24:49,172: INFO: main: Training : batch 806 Loss: 0.011219869153228446]
[2024-04-17 12:24:49,803: INFO: main: Training : batch 807 Loss: 0.007923001838604423]
[2024-04-17 12:24:50,431: INFO: main: Training : batch 808 Loss: 0.002401899904553851]
[2024-04-17 12:24:51,061: INFO: main: Training : batch 809 Loss: 0.010838209084742359]
[2024-04-17 12:24:51,694: INFO: main: Training : batch 810 Loss: 0.01733332079387822]
[2024-04-17 12:24:52,329: INFO: main: Training : batch 811 Loss: 0.007886328136691843]
[2024-04-17 12:24:52,962: INFO: main: Training : batch 812 Loss: 0.004606667070690039]
[2024-04-17 12:24:53,594: INFO: main: Training : batch 813 Loss: 0.012897654854197043]
[2024-04-17 12:24:54,219: INFO: main: Training : batch 814 Loss: 0.03689189644916079]
[2024-04-17 12:24:54,853: INFO: main: Training : batch 815 Loss: 0.0010625292904486749]
[2024-04-17 12:24:55,484: INFO: main: Training : batch 816 Loss: 0.011250968739073901]
[2024-04-17 12:24:56,117: INFO: main: Training : batch 817 Loss: 0.015712564570749144]
[2024-04-17 12:24:56,750: INFO: main: Training : batch 818 Loss: 0.02337518246539469]
[2024-04-17 12:24:57,379: INFO: main: Training : batch 819 Loss: 0.018460979792601143]
[2024-04-17 12:24:58,014: INFO: main: Training : batch 820 Loss: 0.025981672946459877]
[2024-04-17 12:24:58,649: INFO: main: Training : batch 821 Loss: 0.008631294686656618]
[2024-04-17 12:24:59,287: INFO: main: Training : batch 822 Loss: 0.012345903305496665]
[2024-04-17 12:24:59,927: INFO: main: Training : batch 823 Loss: 0.0066608061509568885]
[2024-04-17 12:25:00,569: INFO: main: Training : batch 824 Loss: 0.0035110444908190244]
[2024-04-17 12:25:01,206: INFO: main: Training : batch 825 Loss: 0.010538984460666144]
[2024-04-17 12:25:01,840: INFO: main: Training : batch 826 Loss: 0.010278213929833933]
[2024-04-17 12:25:02,473: INFO: main: Training : batch 827 Loss: 0.051493623522510835]
[2024-04-17 12:25:03,104: INFO: main: Training : batch 828 Loss: 0.008922652357212055]
[2024-04-17 12:25:03,735: INFO: main: Training : batch 829 Loss: 0.002862283267584829]
[2024-04-17 12:25:04,367: INFO: main: Training : batch 830 Loss: 0.011690400092688184]
[2024-04-17 12:25:04,995: INFO: main: Training : batch 831 Loss: 0.008587621776203893]
[2024-04-17 12:25:05,623: INFO: main: Training : batch 832 Loss: 0.012472014731032032]
[2024-04-17 12:25:06,258: INFO: main: Training : batch 833 Loss: 0.012110101607188723]
[2024-04-17 12:25:06,893: INFO: main: Training : batch 834 Loss: 0.002114903922992662]
[2024-04-17 12:25:07,527: INFO: main: Training : batch 835 Loss: 0.023810384390311898]
[2024-04-17 12:25:08,157: INFO: main: Training : batch 836 Loss: 0.004710493202386289]
[2024-04-17 12:25:08,790: INFO: main: Training : batch 837 Loss: 0.020677971942368856]
[2024-04-17 12:25:09,423: INFO: main: Training : batch 838 Loss: 0.017770424440805347]
[2024-04-17 12:25:10,049: INFO: main: Training : batch 839 Loss: 0.018589380678018696]
[2024-04-17 12:25:10,678: INFO: main: Training : batch 840 Loss: 0.00608515918053423]
[2024-04-17 12:25:11,312: INFO: main: Training : batch 841 Loss: 0.046151339042210526]
[2024-04-17 12:25:11,949: INFO: main: Training : batch 842 Loss: 0.01555731218354298]
[2024-04-17 12:25:12,587: INFO: main: Training : batch 843 Loss: 0.0026784402555074683]
[2024-04-17 12:25:13,221: INFO: main: Training : batch 844 Loss: 0.01615176646567366]
[2024-04-17 12:25:13,855: INFO: main: Training : batch 845 Loss: 0.028710160839003896]
[2024-04-17 12:25:14,499: INFO: main: Training : batch 846 Loss: 0.005136131268977123]
[2024-04-17 12:25:15,130: INFO: main: Training : batch 847 Loss: 0.005211452990686072]
[2024-04-17 12:25:15,760: INFO: main: Training : batch 848 Loss: 0.010886666223238526]
[2024-04-17 12:25:16,389: INFO: main: Training : batch 849 Loss: 0.005852446774954847]
[2024-04-17 12:25:17,019: INFO: main: Training : batch 850 Loss: 0.0201924259143146]
[2024-04-17 12:25:17,651: INFO: main: Training : batch 851 Loss: 0.011252103571914838]
[2024-04-17 12:25:18,285: INFO: main: Training : batch 852 Loss: 0.014022616083037889]
[2024-04-17 12:25:18,919: INFO: main: Training : batch 853 Loss: 0.006192172345388565]
[2024-04-17 12:25:19,548: INFO: main: Training : batch 854 Loss: 0.013852377869362852]
[2024-04-17 12:25:20,179: INFO: main: Training : batch 855 Loss: 0.010289670776118639]
[2024-04-17 12:25:20,813: INFO: main: Training : batch 856 Loss: 0.0056951106367414475]
[2024-04-17 12:25:21,445: INFO: main: Training : batch 857 Loss: 0.020783234591797272]
[2024-04-17 12:25:22,077: INFO: main: Training : batch 858 Loss: 0.004449027960974046]
[2024-04-17 12:25:22,708: INFO: main: Training : batch 859 Loss: 0.005845895507502811]
[2024-04-17 12:25:23,339: INFO: main: Training : batch 860 Loss: 0.008704487327960116]
[2024-04-17 12:25:23,968: INFO: main: Training : batch 861 Loss: 0.02120028006191552]
[2024-04-17 12:25:24,601: INFO: main: Training : batch 862 Loss: 0.0010539897308632698]
[2024-04-17 12:25:25,234: INFO: main: Training : batch 863 Loss: 0.016868540585060682]
[2024-04-17 12:25:25,870: INFO: main: Training : batch 864 Loss: 0.009507405632722694]
[2024-04-17 12:25:26,516: INFO: main: Training : batch 865 Loss: 0.004136947553105218]
[2024-04-17 12:25:27,157: INFO: main: Training : batch 866 Loss: 0.00729987045773777]
[2024-04-17 12:25:27,789: INFO: main: Training : batch 867 Loss: 0.006529158173817339]
[2024-04-17 12:25:28,420: INFO: main: Training : batch 868 Loss: 0.021739088725782518]
[2024-04-17 12:25:29,051: INFO: main: Training : batch 869 Loss: 0.013700914769655393]
[2024-04-17 12:25:29,684: INFO: main: Training : batch 870 Loss: 0.00732796172465798]
[2024-04-17 12:25:30,320: INFO: main: Training : batch 871 Loss: 0.015063658702253563]
[2024-04-17 12:25:30,954: INFO: main: Training : batch 872 Loss: 0.03747357539224845]
[2024-04-17 12:25:31,586: INFO: main: Training : batch 873 Loss: 0.018972190596478564]
[2024-04-17 12:25:32,212: INFO: main: Training : batch 874 Loss: 0.03211774985489025]
[2024-04-17 12:25:32,848: INFO: main: Training : batch 875 Loss: 0.013620502584587083]
[2024-04-17 12:25:33,482: INFO: main: Training : batch 876 Loss: 0.019517344685083184]
[2024-04-17 12:25:34,114: INFO: main: Training : batch 877 Loss: 0.0018352485428375555]
[2024-04-17 12:25:34,750: INFO: main: Training : batch 878 Loss: 0.0057883563299881355]
[2024-04-17 12:25:35,384: INFO: main: Training : batch 879 Loss: 0.007539043327261782]
[2024-04-17 12:25:36,019: INFO: main: Training : batch 880 Loss: 0.009853017945658064]
[2024-04-17 12:25:36,654: INFO: main: Training : batch 881 Loss: 0.0041284967166454805]
[2024-04-17 12:25:37,279: INFO: main: Training : batch 882 Loss: 0.008757007805804316]
[2024-04-17 12:25:37,913: INFO: main: Training : batch 883 Loss: 0.00663489475178659]
[2024-04-17 12:25:38,553: INFO: main: Training : batch 884 Loss: 0.02320388237861699]
[2024-04-17 12:25:39,192: INFO: main: Training : batch 885 Loss: 0.010972881646030505]
[2024-04-17 12:25:39,823: INFO: main: Training : batch 886 Loss: 0.00735570246433888]
[2024-04-17 12:25:40,462: INFO: main: Training : batch 887 Loss: 0.0035016643314504346]
[2024-04-17 12:25:41,104: INFO: main: Training : batch 888 Loss: 0.042787712682840796]
[2024-04-17 12:25:41,736: INFO: main: Training : batch 889 Loss: 0.008141175186169658]
[2024-04-17 12:25:42,367: INFO: main: Training : batch 890 Loss: 0.013384382983440711]
[2024-04-17 12:25:43,004: INFO: main: Training : batch 891 Loss: 0.009272038163180594]
[2024-04-17 12:25:43,631: INFO: main: Training : batch 892 Loss: 0.05826637028802092]
[2024-04-17 12:25:44,261: INFO: main: Training : batch 893 Loss: 0.019438210658764606]
[2024-04-17 12:25:44,891: INFO: main: Training : batch 894 Loss: 0.019303676894941824]
[2024-04-17 12:25:45,521: INFO: main: Training : batch 895 Loss: 0.020245260678817205]
[2024-04-17 12:25:46,152: INFO: main: Training : batch 896 Loss: 0.006163256153921457]
[2024-04-17 12:25:46,786: INFO: main: Training : batch 897 Loss: 0.016139884701087705]
[2024-04-17 12:25:47,414: INFO: main: Training : batch 898 Loss: 0.024116888931509168]
[2024-04-17 12:25:48,046: INFO: main: Training : batch 899 Loss: 0.007914223443137641]
[2024-04-17 12:25:48,678: INFO: main: Training : batch 900 Loss: 0.013627932232630985]
[2024-04-17 12:25:49,310: INFO: main: Training : batch 901 Loss: 0.0037681027902271473]
[2024-04-17 12:25:49,941: INFO: main: Training : batch 902 Loss: 0.02974417466545315]
[2024-04-17 12:25:50,567: INFO: main: Training : batch 903 Loss: 0.010574578671074987]
[2024-04-17 12:25:51,198: INFO: main: Training : batch 904 Loss: 0.011296299160194834]
[2024-04-17 12:25:51,833: INFO: main: Training : batch 905 Loss: 0.010099793601031361]
[2024-04-17 12:25:52,479: INFO: main: Training : batch 906 Loss: 0.014244240458316858]
[2024-04-17 12:25:53,116: INFO: main: Training : batch 907 Loss: 0.00866566483205267]
[2024-04-17 12:25:53,754: INFO: main: Training : batch 908 Loss: 0.010791938838024059]
[2024-04-17 12:25:54,402: INFO: main: Training : batch 909 Loss: 0.007557895349548528]
[2024-04-17 12:25:55,034: INFO: main: Training : batch 910 Loss: 0.012519091957531616]
[2024-04-17 12:25:55,670: INFO: main: Training : batch 911 Loss: 0.02406410080791844]
[2024-04-17 12:25:56,306: INFO: main: Training : batch 912 Loss: 0.004580323609918616]
[2024-04-17 12:25:56,938: INFO: main: Training : batch 913 Loss: 0.0029983844528412153]
[2024-04-17 12:25:57,569: INFO: main: Training : batch 914 Loss: 0.041549605765399046]
[2024-04-17 12:25:58,199: INFO: main: Training : batch 915 Loss: 0.02813643163598064]
[2024-04-17 12:25:58,826: INFO: main: Training : batch 916 Loss: 0.009015416479802944]
[2024-04-17 12:25:59,456: INFO: main: Training : batch 917 Loss: 0.010081060010642063]
[2024-04-17 12:26:00,091: INFO: main: Training : batch 918 Loss: 0.013698001242777572]
[2024-04-17 12:26:00,725: INFO: main: Training : batch 919 Loss: 0.012032607837432555]
[2024-04-17 12:26:01,357: INFO: main: Training : batch 920 Loss: 0.004588011850210129]
[2024-04-17 12:26:01,991: INFO: main: Training : batch 921 Loss: 0.015290246302299373]
[2024-04-17 12:26:02,625: INFO: main: Training : batch 922 Loss: 0.00632264438802676]
[2024-04-17 12:26:03,253: INFO: main: Training : batch 923 Loss: 0.011239069598325924]
[2024-04-17 12:26:03,889: INFO: main: Training : batch 924 Loss: 0.016329119328396]
[2024-04-17 12:26:04,511: INFO: main: Training : batch 925 Loss: 0.019452796989337242]
[2024-04-17 12:26:05,149: INFO: main: Training : batch 926 Loss: 0.024491333438012717]
[2024-04-17 12:26:05,787: INFO: main: Training : batch 927 Loss: 0.007433843713886096]
[2024-04-17 12:26:06,424: INFO: main: Training : batch 928 Loss: 0.02888057677413603]
[2024-04-17 12:26:07,062: INFO: main: Training : batch 929 Loss: 0.057101798747188855]
[2024-04-17 12:26:07,694: INFO: main: Training : batch 930 Loss: 0.008065182960578276]
[2024-04-17 12:26:08,330: INFO: main: Training : batch 931 Loss: 0.0036714995245621476]
[2024-04-17 12:26:08,958: INFO: main: Training : batch 932 Loss: 0.00757656846062221]
[2024-04-17 12:26:09,588: INFO: main: Training : batch 933 Loss: 0.01647875813629814]
[2024-04-17 12:26:10,218: INFO: main: Training : batch 934 Loss: 0.010904494668982445]
[2024-04-17 12:26:10,846: INFO: main: Training : batch 935 Loss: 0.007827940574038971]
[2024-04-17 12:26:11,477: INFO: main: Training : batch 936 Loss: 0.019789401163880248]
[2024-04-17 12:26:12,106: INFO: main: Training : batch 937 Loss: 0.01163707138162841]
[2024-04-17 12:26:12,740: INFO: main: Training : batch 938 Loss: 0.013567604378482666]
[2024-04-17 12:26:13,376: INFO: main: Training : batch 939 Loss: 0.03699259723109064]
[2024-04-17 12:26:14,004: INFO: main: Training : batch 940 Loss: 0.012172102464026547]
[2024-04-17 12:26:14,637: INFO: main: Training : batch 941 Loss: 0.013393817292833572]
[2024-04-17 12:26:15,269: INFO: main: Training : batch 942 Loss: 0.007609707254195824]
[2024-04-17 12:26:15,901: INFO: main: Training : batch 943 Loss: 0.004872430548410562]
[2024-04-17 12:26:16,536: INFO: main: Training : batch 944 Loss: 0.00939506705855879]
[2024-04-17 12:26:17,164: INFO: main: Training : batch 945 Loss: 0.0038167764607498307]
[2024-04-17 12:26:17,799: INFO: main: Training : batch 946 Loss: 0.01860498002295154]
[2024-04-17 12:26:18,444: INFO: main: Training : batch 947 Loss: 0.011471977337763879]
[2024-04-17 12:26:19,087: INFO: main: Training : batch 948 Loss: 0.0068363744317385565]
[2024-04-17 12:26:19,720: INFO: main: Training : batch 949 Loss: 0.009305775710130935]
[2024-04-17 12:26:20,362: INFO: main: Training : batch 950 Loss: 0.0190971022369811]
[2024-04-17 12:26:21,007: INFO: main: Training : batch 951 Loss: 0.003839656153327412]
[2024-04-17 12:26:21,637: INFO: main: Training : batch 952 Loss: 0.014983459421605378]
[2024-04-17 12:26:22,267: INFO: main: Training : batch 953 Loss: 0.015629967177381045]
[2024-04-17 12:26:22,898: INFO: main: Training : batch 954 Loss: 0.0059130096590622435]
[2024-04-17 12:26:23,529: INFO: main: Training : batch 955 Loss: 0.004015265273174322]
[2024-04-17 12:26:24,161: INFO: main: Training : batch 956 Loss: 0.009560152318901267]
[2024-04-17 12:26:24,791: INFO: main: Training : batch 957 Loss: 0.00818950937736656]
[2024-04-17 12:26:25,421: INFO: main: Training : batch 958 Loss: 0.002832455625831352]
[2024-04-17 12:26:26,052: INFO: main: Training : batch 959 Loss: 0.011410038611675128]
[2024-04-17 12:26:26,680: INFO: main: Training : batch 960 Loss: 0.034963929417323646]
[2024-04-17 12:26:27,313: INFO: main: Training : batch 961 Loss: 0.0049478438851766095]
[2024-04-17 12:26:27,941: INFO: main: Training : batch 962 Loss: 0.00850233727744013]
[2024-04-17 12:26:28,573: INFO: main: Training : batch 963 Loss: 0.010182399438341167]
[2024-04-17 12:26:29,205: INFO: main: Training : batch 964 Loss: 0.007665842586953869]
[2024-04-17 12:26:29,835: INFO: main: Training : batch 965 Loss: 0.04586453401057245]
[2024-04-17 12:26:30,464: INFO: main: Training : batch 966 Loss: 0.0339032174908143]
[2024-04-17 12:26:31,095: INFO: main: Training : batch 967 Loss: 0.008206033073396839]
[2024-04-17 12:26:31,727: INFO: main: Training : batch 968 Loss: 0.0010583268156084701]
[2024-04-17 12:26:32,364: INFO: main: Training : batch 969 Loss: 0.02911932334641001]
[2024-04-17 12:26:33,002: INFO: main: Training : batch 970 Loss: 0.005437545492373565]
[2024-04-17 12:26:33,645: INFO: main: Training : batch 971 Loss: 0.004270078096006274]
[2024-04-17 12:26:34,286: INFO: main: Training : batch 972 Loss: 0.0131592609752543]
[2024-04-17 12:26:34,919: INFO: main: Training : batch 973 Loss: 0.007655612140567786]
[2024-04-17 12:26:35,547: INFO: main: Training : batch 974 Loss: 0.017558974534890782]
[2024-04-17 12:26:36,179: INFO: main: Training : batch 975 Loss: 0.016356825619353352]
[2024-04-17 12:26:36,812: INFO: main: Training : batch 976 Loss: 0.0014101615786177497]
[2024-04-17 12:26:37,441: INFO: main: Training : batch 977 Loss: 0.004040468593152346]
[2024-04-17 12:26:38,073: INFO: main: Training : batch 978 Loss: 0.012789723439363361]
[2024-04-17 12:26:38,703: INFO: main: Training : batch 979 Loss: 0.007557491145559821]
[2024-04-17 12:26:39,336: INFO: main: Training : batch 980 Loss: 0.0009957496854657273]
[2024-04-17 12:26:39,965: INFO: main: Training : batch 981 Loss: 0.030290122791639132]
[2024-04-17 12:26:40,595: INFO: main: Training : batch 982 Loss: 0.011227472518697788]
[2024-04-17 12:26:41,226: INFO: main: Training : batch 983 Loss: 0.003294020480608534]
[2024-04-17 12:26:41,854: INFO: main: Training : batch 984 Loss: 0.004244681109301282]
[2024-04-17 12:26:42,483: INFO: main: Training : batch 985 Loss: 0.01113007220893135]
[2024-04-17 12:26:43,112: INFO: main: Training : batch 986 Loss: 0.005682963734788238]
[2024-04-17 12:26:43,742: INFO: main: Training : batch 987 Loss: 0.009886307960895138]
[2024-04-17 12:26:44,377: INFO: main: Training : batch 988 Loss: 0.027973138335782848]
[2024-04-17 12:26:45,012: INFO: main: Training : batch 989 Loss: 0.004794783029818905]
[2024-04-17 12:26:45,652: INFO: main: Training : batch 990 Loss: 0.011813448019863232]
[2024-04-17 12:26:46,295: INFO: main: Training : batch 991 Loss: 0.0033433343793370933]
[2024-04-17 12:26:46,942: INFO: main: Training : batch 992 Loss: 0.005235045257933024]
[2024-04-17 12:26:47,581: INFO: main: Training : batch 993 Loss: 0.016584635063857385]
[2024-04-17 12:26:48,209: INFO: main: Training : batch 994 Loss: 0.01792378924832607]
[2024-04-17 12:26:48,843: INFO: main: Training : batch 995 Loss: 0.00884813877967953]
[2024-04-17 12:26:49,478: INFO: main: Training : batch 996 Loss: 0.008440318945439866]
[2024-04-17 12:26:50,111: INFO: main: Training : batch 997 Loss: 0.0018558662021847042]
[2024-04-17 12:26:50,741: INFO: main: Training : batch 998 Loss: 0.011540828200026082]
[2024-04-17 12:26:51,370: INFO: main: Training : batch 999 Loss: 0.0229803361203155]
[2024-04-17 12:26:51,996: INFO: main: Training : batch 1000 Loss: 0.03402313454296186]
[2024-04-17 12:26:52,625: INFO: main: Training : batch 1001 Loss: 0.007249205864388082]
[2024-04-17 12:26:53,257: INFO: main: Training : batch 1002 Loss: 0.014552025948883881]
[2024-04-17 12:26:53,884: INFO: main: Training : batch 1003 Loss: 0.015341970299694427]
[2024-04-17 12:26:54,513: INFO: main: Training : batch 1004 Loss: 0.0008667861225067652]
[2024-04-17 12:26:55,144: INFO: main: Training : batch 1005 Loss: 0.004104003593206008]
[2024-04-17 12:26:55,773: INFO: main: Training : batch 1006 Loss: 0.0029930231215119178]
[2024-04-17 12:26:56,402: INFO: main: Training : batch 1007 Loss: 0.0023721086628981096]
[2024-04-17 12:26:57,033: INFO: main: Training : batch 1008 Loss: 0.025941894913893954]
[2024-04-17 12:26:57,664: INFO: main: Training : batch 1009 Loss: 0.03218861011368283]
[2024-04-17 12:26:58,303: INFO: main: Training : batch 1010 Loss: 0.013089720523243763]
[2024-04-17 12:26:58,936: INFO: main: Training : batch 1011 Loss: 0.01646767574630079]
[2024-04-17 12:26:59,573: INFO: main: Training : batch 1012 Loss: 0.0063053294133782025]
[2024-04-17 12:27:00,216: INFO: main: Training : batch 1013 Loss: 0.010091934948478896]
[2024-04-17 12:27:00,855: INFO: main: Training : batch 1014 Loss: 0.001926501242029885]
[2024-04-17 12:27:01,482: INFO: main: Training : batch 1015 Loss: 0.02065598212769003]
[2024-04-17 12:27:02,115: INFO: main: Training : batch 1016 Loss: 0.013181710091556794]
[2024-04-17 12:27:02,745: INFO: main: Training : batch 1017 Loss: 0.022216229708659576]
[2024-04-17 12:27:03,375: INFO: main: Training : batch 1018 Loss: 0.017895669256824485]
[2024-04-17 12:27:04,005: INFO: main: Training : batch 1019 Loss: 0.008520721968276992]
[2024-04-17 12:27:04,635: INFO: main: Training : batch 1020 Loss: 0.005885690543879223]
[2024-04-17 12:27:05,261: INFO: main: Training : batch 1021 Loss: 0.014912109029433887]
[2024-04-17 12:27:05,889: INFO: main: Training : batch 1022 Loss: 0.004536896064033698]
[2024-04-17 12:27:06,516: INFO: main: Training : batch 1023 Loss: 0.013300221244650514]
[2024-04-17 12:27:07,144: INFO: main: Training : batch 1024 Loss: 0.00877170179334602]
[2024-04-17 12:27:07,778: INFO: main: Training : batch 1025 Loss: 0.015366769283275903]
[2024-04-17 12:27:08,410: INFO: main: Training : batch 1026 Loss: 0.005027968397912869]
[2024-04-17 12:27:09,039: INFO: main: Training : batch 1027 Loss: 0.007820852086382835]
[2024-04-17 12:27:09,669: INFO: main: Training : batch 1028 Loss: 0.012483911947449943]
[2024-04-17 12:27:10,303: INFO: main: Training : batch 1029 Loss: 0.006437359122366217]
[2024-04-17 12:27:10,933: INFO: main: Training : batch 1030 Loss: 0.002719048702094334]
[2024-04-17 12:27:11,567: INFO: main: Training : batch 1031 Loss: 0.0047187276204759]
[2024-04-17 12:27:12,208: INFO: main: Training : batch 1032 Loss: 0.04393339503414178]
[2024-04-17 12:27:12,846: INFO: main: Training : batch 1033 Loss: 0.0042748079736947995]
[2024-04-17 12:27:13,482: INFO: main: Training : batch 1034 Loss: 0.006144994147602651]
[2024-04-17 12:27:14,118: INFO: main: Training : batch 1035 Loss: 0.014447097985628685]
[2024-04-17 12:27:14,749: INFO: main: Training : batch 1036 Loss: 0.00417669093393597]
[2024-04-17 12:27:15,380: INFO: main: Training : batch 1037 Loss: 0.023979399595863326]
[2024-04-17 12:27:16,008: INFO: main: Training : batch 1038 Loss: 0.008886638001948744]
[2024-04-17 12:27:16,634: INFO: main: Training : batch 1039 Loss: 0.009923349074168095]
[2024-04-17 12:27:17,268: INFO: main: Training : batch 1040 Loss: 0.012803584480255877]
[2024-04-17 12:27:17,898: INFO: main: Training : batch 1041 Loss: 0.0054232664140006]
[2024-04-17 12:27:18,527: INFO: main: Training : batch 1042 Loss: 0.000847755831056245]
[2024-04-17 12:27:19,155: INFO: main: Training : batch 1043 Loss: 0.010637825284917865]
[2024-04-17 12:27:19,783: INFO: main: Training : batch 1044 Loss: 0.008564367002094536]
[2024-04-17 12:27:20,416: INFO: main: Training : batch 1045 Loss: 0.015167117164879779]
[2024-04-17 12:27:21,051: INFO: main: Training : batch 1046 Loss: 0.00489572294902291]
[2024-04-17 12:27:21,681: INFO: main: Training : batch 1047 Loss: 0.0019688641363452943]
[2024-04-17 12:27:22,310: INFO: main: Training : batch 1048 Loss: 0.01739040381346615]
[2024-04-17 12:27:22,941: INFO: main: Training : batch 1049 Loss: 0.08211247951632343]
[2024-04-17 12:27:23,573: INFO: main: Training : batch 1050 Loss: 0.013227720701496852]
[2024-04-17 12:27:24,204: INFO: main: Training : batch 1051 Loss: 0.008483138655941752]
[2024-04-17 12:27:24,848: INFO: main: Training : batch 1052 Loss: 0.009812387652210966]
[2024-04-17 12:27:25,483: INFO: main: Training : batch 1053 Loss: 0.003937700795785246]
[2024-04-17 12:27:26,117: INFO: main: Training : batch 1054 Loss: 0.012662589466545257]
[2024-04-17 12:27:26,749: INFO: main: Training : batch 1055 Loss: 0.0271777781890754]
[2024-04-17 12:27:27,389: INFO: main: Training : batch 1056 Loss: 0.014395546014281443]
[2024-04-17 12:27:28,021: INFO: main: Training : batch 1057 Loss: 0.022691117190969962]
[2024-04-17 12:27:28,646: INFO: main: Training : batch 1058 Loss: 0.00803601720227836]
[2024-04-17 12:27:29,275: INFO: main: Training : batch 1059 Loss: 0.05885641216647322]
[2024-04-17 12:27:29,905: INFO: main: Training : batch 1060 Loss: 0.009172678553458637]
[2024-04-17 12:27:30,537: INFO: main: Training : batch 1061 Loss: 0.008227209753504676]
[2024-04-17 12:27:31,169: INFO: main: Training : batch 1062 Loss: 0.023059237159373466]
[2024-04-17 12:27:31,797: INFO: main: Training : batch 1063 Loss: 0.023247517937994597]
[2024-04-17 12:27:32,428: INFO: main: Training : batch 1064 Loss: 0.015263544322431433]
[2024-04-17 12:27:33,058: INFO: main: Training : batch 1065 Loss: 0.0035560504579515402]
[2024-04-17 12:27:33,684: INFO: main: Training : batch 1066 Loss: 0.005285806373295548]
[2024-04-17 12:27:34,312: INFO: main: Training : batch 1067 Loss: 0.010813028211786879]
[2024-04-17 12:27:34,940: INFO: main: Training : batch 1068 Loss: 0.026164013727276635]
[2024-04-17 12:27:35,570: INFO: main: Training : batch 1069 Loss: 0.01460245887222411]
[2024-04-17 12:27:36,204: INFO: main: Training : batch 1070 Loss: 0.02045109304653548]
[2024-04-17 12:27:36,837: INFO: main: Training : batch 1071 Loss: 0.015213490935749651]
[2024-04-17 12:27:37,464: INFO: main: Training : batch 1072 Loss: 0.0076320367818533035]
[2024-04-17 12:27:38,098: INFO: main: Training : batch 1073 Loss: 0.010835069576908154]
[2024-04-17 12:27:38,733: INFO: main: Training : batch 1074 Loss: 0.006852612126439999]
[2024-04-17 12:27:39,367: INFO: main: Training : batch 1075 Loss: 0.0036381512946525338]
[2024-04-17 12:27:40,004: INFO: main: Training : batch 1076 Loss: 0.016137986783213352]
[2024-04-17 12:27:40,639: INFO: main: Training : batch 1077 Loss: 0.006887846764177658]
[2024-04-17 12:27:41,268: INFO: main: Training : batch 1078 Loss: 0.019704775407078698]
[2024-04-17 12:27:41,900: INFO: main: Training : batch 1079 Loss: 0.02129483231393875]
[2024-04-17 12:27:42,530: INFO: main: Training : batch 1080 Loss: 0.00966800973037988]
[2024-04-17 12:27:43,161: INFO: main: Training : batch 1081 Loss: 0.0091029625174745]
[2024-04-17 12:27:43,792: INFO: main: Training : batch 1082 Loss: 0.016747206217695168]
[2024-04-17 12:27:44,419: INFO: main: Training : batch 1083 Loss: 0.01337397332788173]
[2024-04-17 12:27:45,044: INFO: main: Training : batch 1084 Loss: 0.02270564003286546]
[2024-04-17 12:27:45,674: INFO: main: Training : batch 1085 Loss: 0.008391826947250334]
[2024-04-17 12:27:46,302: INFO: main: Training : batch 1086 Loss: 0.016109160144165288]
[2024-04-17 12:27:46,931: INFO: main: Training : batch 1087 Loss: 0.012880993413965175]
[2024-04-17 12:27:47,562: INFO: main: Training : batch 1088 Loss: 0.016242172253719416]
[2024-04-17 12:27:48,189: INFO: main: Training : batch 1089 Loss: 0.011277613537821288]
[2024-04-17 12:27:48,821: INFO: main: Training : batch 1090 Loss: 0.006559291600449294]
[2024-04-17 12:27:49,456: INFO: main: Training : batch 1091 Loss: 0.004401165119028486]
[2024-04-17 12:27:50,085: INFO: main: Training : batch 1092 Loss: 0.007187468839394889]
[2024-04-17 12:27:50,714: INFO: main: Training : batch 1093 Loss: 0.00989396353348465]
[2024-04-17 12:27:51,348: INFO: main: Training : batch 1094 Loss: 0.014280392169514555]
[2024-04-17 12:27:51,981: INFO: main: Training : batch 1095 Loss: 0.004648220653118516]
[2024-04-17 12:27:52,613: INFO: main: Training : batch 1096 Loss: 0.008923274577366717]
[2024-04-17 12:27:53,245: INFO: main: Training : batch 1097 Loss: 0.019173990990102117]
[2024-04-17 12:27:53,890: INFO: main: Training : batch 1098 Loss: 0.03753825886769774]
[2024-04-17 12:27:54,521: INFO: main: Training : batch 1099 Loss: 0.035726113274833264]
[2024-04-17 12:27:55,146: INFO: main: Training : batch 1100 Loss: 0.0167975469491695]
[2024-04-17 12:27:55,779: INFO: main: Training : batch 1101 Loss: 0.00824505836494487]
[2024-04-17 12:27:56,404: INFO: main: Training : batch 1102 Loss: 0.04086030530134381]
[2024-04-17 12:27:57,035: INFO: main: Training : batch 1103 Loss: 0.015745923952988735]
[2024-04-17 12:27:57,663: INFO: main: Training : batch 1104 Loss: 0.006516447592666659]
[2024-04-17 12:27:58,298: INFO: main: Training : batch 1105 Loss: 0.01578682032965912]
[2024-04-17 12:27:58,929: INFO: main: Training : batch 1106 Loss: 0.0038573861822489976]
[2024-04-17 12:27:59,560: INFO: main: Training : batch 1107 Loss: 0.011593780775075]
[2024-04-17 12:28:00,191: INFO: main: Training : batch 1108 Loss: 0.006296138542996668]
[2024-04-17 12:28:00,821: INFO: main: Training : batch 1109 Loss: 0.005979168852651634]
[2024-04-17 12:28:01,448: INFO: main: Training : batch 1110 Loss: 0.011116555538979568]
[2024-04-17 12:28:02,077: INFO: main: Training : batch 1111 Loss: 0.011779645236132645]
[2024-04-17 12:28:02,710: INFO: main: Training : batch 1112 Loss: 0.005722557776803748]
[2024-04-17 12:28:03,341: INFO: main: Training : batch 1113 Loss: 0.014299978012417447]
[2024-04-17 12:28:03,974: INFO: main: Training : batch 1114 Loss: 0.028890994733375176]
[2024-04-17 12:28:04,609: INFO: main: Training : batch 1115 Loss: 0.011645021854051472]
[2024-04-17 12:28:05,243: INFO: main: Training : batch 1116 Loss: 0.006701890764221408]
[2024-04-17 12:28:05,882: INFO: main: Training : batch 1117 Loss: 0.011371878290592514]
[2024-04-17 12:28:06,518: INFO: main: Training : batch 1118 Loss: 0.026971788601881787]
[2024-04-17 12:28:07,159: INFO: main: Training : batch 1119 Loss: 0.015482319103932054]
[2024-04-17 12:28:07,792: INFO: main: Training : batch 1120 Loss: 0.013281652256476997]
[2024-04-17 12:28:08,423: INFO: main: Training : batch 1121 Loss: 0.005584519834760835]
[2024-04-17 12:28:09,053: INFO: main: Training : batch 1122 Loss: 0.0488518380695893]
[2024-04-17 12:28:09,680: INFO: main: Training : batch 1123 Loss: 0.00951484031541576]
[2024-04-17 12:28:10,309: INFO: main: Training : batch 1124 Loss: 0.015747341184938396]
[2024-04-17 12:28:10,939: INFO: main: Training : batch 1125 Loss: 0.008432748512363388]
[2024-04-17 12:28:11,570: INFO: main: Training : batch 1126 Loss: 0.009057861115197291]
[2024-04-17 12:28:12,202: INFO: main: Training : batch 1127 Loss: 0.01595726113399138]
[2024-04-17 12:28:12,828: INFO: main: Training : batch 1128 Loss: 0.014570078077672629]
[2024-04-17 12:28:13,461: INFO: main: Training : batch 1129 Loss: 0.002184935443633216]
[2024-04-17 12:28:14,091: INFO: main: Training : batch 1130 Loss: 0.007585787964291023]
[2024-04-17 12:28:14,723: INFO: main: Training : batch 1131 Loss: 0.025335761012988303]
[2024-04-17 12:28:15,350: INFO: main: Training : batch 1132 Loss: 0.011224089248633903]
[2024-04-17 12:28:15,988: INFO: main: Training : batch 1133 Loss: 0.027387908906528733]
[2024-04-17 12:28:16,621: INFO: main: Training : batch 1134 Loss: 0.01576526713908884]
[2024-04-17 12:28:17,249: INFO: main: Training : batch 1135 Loss: 0.014659326987504887]
[2024-04-17 12:28:17,890: INFO: main: Training : batch 1136 Loss: 0.004286795448526228]
[2024-04-17 12:28:18,532: INFO: main: Training : batch 1137 Loss: 0.002324757601710192]
[2024-04-17 12:28:19,172: INFO: main: Training : batch 1138 Loss: 0.0033679944000113487]
[2024-04-17 12:28:19,812: INFO: main: Training : batch 1139 Loss: 0.01362348866414599]
[2024-04-17 12:28:20,447: INFO: main: Training : batch 1140 Loss: 0.006422443543636576]
[2024-04-17 12:28:21,083: INFO: main: Training : batch 1141 Loss: 0.0010774181354489642]
[2024-04-17 12:28:21,711: INFO: main: Training : batch 1142 Loss: 0.033716476205466986]
[2024-04-17 12:28:22,343: INFO: main: Training : batch 1143 Loss: 0.0024340557681850573]
[2024-04-17 12:28:22,972: INFO: main: Training : batch 1144 Loss: 0.006656418507673361]
[2024-04-17 12:28:23,604: INFO: main: Training : batch 1145 Loss: 0.0031435294049318467]
[2024-04-17 12:28:24,238: INFO: main: Training : batch 1146 Loss: 0.0052502288064367325]
[2024-04-17 12:28:24,871: INFO: main: Training : batch 1147 Loss: 0.0030328396396258873]
[2024-04-17 12:28:25,498: INFO: main: Training : batch 1148 Loss: 0.02294039961786797]
[2024-04-17 12:28:26,129: INFO: main: Training : batch 1149 Loss: 0.015834718933682036]
[2024-04-17 12:28:26,759: INFO: main: Training : batch 1150 Loss: 0.03454635003555183]
[2024-04-17 12:28:27,394: INFO: main: Training : batch 1151 Loss: 0.004145497615039406]
[2024-04-17 12:28:28,024: INFO: main: Training : batch 1152 Loss: 0.010930622207883775]
[2024-04-17 12:28:28,653: INFO: main: Training : batch 1153 Loss: 0.00799408081128371]
[2024-04-17 12:28:29,280: INFO: main: Training : batch 1154 Loss: 0.017748928760707026]
[2024-04-17 12:28:29,912: INFO: main: Training : batch 1155 Loss: 0.011083345350720425]
[2024-04-17 12:28:30,546: INFO: main: Training : batch 1156 Loss: 0.011299610074663062]
[2024-04-17 12:28:31,186: INFO: main: Training : batch 1157 Loss: 0.019475630123157357]
[2024-04-17 12:28:31,822: INFO: main: Training : batch 1158 Loss: 0.01790089221770201]
[2024-04-17 12:28:32,458: INFO: main: Training : batch 1159 Loss: 0.011787674660126314]
[2024-04-17 12:28:33,098: INFO: main: Training : batch 1160 Loss: 0.007693391284454215]
[2024-04-17 12:28:33,731: INFO: main: Training : batch 1161 Loss: 0.011443044831183016]
[2024-04-17 12:28:34,355: INFO: main: Training : batch 1162 Loss: 0.010387078517853865]
[2024-04-17 12:28:34,989: INFO: main: Training : batch 1163 Loss: 0.0065909458928712595]
[2024-04-17 12:28:35,620: INFO: main: Training : batch 1164 Loss: 0.0027341914576206644]
[2024-04-17 12:28:36,249: INFO: main: Training : batch 1165 Loss: 0.0044525004513329145]
[2024-04-17 12:28:36,880: INFO: main: Training : batch 1166 Loss: 0.02685193506733793]
[2024-04-17 12:28:37,511: INFO: main: Training : batch 1167 Loss: 0.027465728699812498]
[2024-04-17 12:28:38,146: INFO: main: Training : batch 1168 Loss: 0.023485650394040856]
[2024-04-17 12:28:38,774: INFO: main: Training : batch 1169 Loss: 0.0024221953893295805]
[2024-04-17 12:28:39,406: INFO: main: Training : batch 1170 Loss: 0.012058495480917752]
[2024-04-17 12:28:40,032: INFO: main: Training : batch 1171 Loss: 0.022930707110521156]
[2024-04-17 12:28:40,665: INFO: main: Training : batch 1172 Loss: 0.007830153644201072]
[2024-04-17 12:28:41,295: INFO: main: Training : batch 1173 Loss: 0.007395282074225255]
[2024-04-17 12:28:41,930: INFO: main: Training : batch 1174 Loss: 0.005123594464044757]
[2024-04-17 12:28:42,563: INFO: main: Training : batch 1175 Loss: 0.008481465019266502]
[2024-04-17 12:28:43,195: INFO: main: Training : batch 1176 Loss: 0.004177659991749143]
[2024-04-17 12:28:43,828: INFO: main: Training : batch 1177 Loss: 0.03876633581505582]
[2024-04-17 12:28:44,467: INFO: main: Training : batch 1178 Loss: 0.006918831495994565]
[2024-04-17 12:28:45,100: INFO: main: Training : batch 1179 Loss: 0.016348378453631636]
[2024-04-17 12:28:45,741: INFO: main: Training : batch 1180 Loss: 0.013769592516693414]
[2024-04-17 12:28:46,379: INFO: main: Training : batch 1181 Loss: 0.00465303317072041]
[2024-04-17 12:28:47,020: INFO: main: Training : batch 1182 Loss: 0.002446084705834249]
[2024-04-17 12:28:47,651: INFO: main: Training : batch 1183 Loss: 0.01019911109505115]
[2024-04-17 12:28:48,281: INFO: main: Training : batch 1184 Loss: 0.008547050826536125]
[2024-04-17 12:28:48,913: INFO: main: Training : batch 1185 Loss: 0.014552766600334551]
[2024-04-17 12:28:49,546: INFO: main: Training : batch 1186 Loss: 0.005859507644564238]
[2024-04-17 12:28:50,177: INFO: main: Training : batch 1187 Loss: 0.018937792459610404]
[2024-04-17 12:28:50,807: INFO: main: Training : batch 1188 Loss: 0.003965881098239777]
[2024-04-17 12:28:51,436: INFO: main: Training : batch 1189 Loss: 0.022310955784966495]
[2024-04-17 12:28:52,065: INFO: main: Training : batch 1190 Loss: 0.018424323548957285]
[2024-04-17 12:28:52,693: INFO: main: Training : batch 1191 Loss: 0.009278324902116736]
[2024-04-17 12:28:53,324: INFO: main: Training : batch 1192 Loss: 0.007399406585372647]
[2024-04-17 12:28:53,956: INFO: main: Training : batch 1193 Loss: 0.0022876543855247455]
[2024-04-17 12:28:54,588: INFO: main: Training : batch 1194 Loss: 0.015205346575243014]
[2024-04-17 12:28:55,219: INFO: main: Training : batch 1195 Loss: 0.019056364925295975]
[2024-04-17 12:28:55,850: INFO: main: Training : batch 1196 Loss: 0.019705911124513403]
[2024-04-17 12:28:56,480: INFO: main: Training : batch 1197 Loss: 0.014756063508552263]
[2024-04-17 12:28:57,116: INFO: main: Training : batch 1198 Loss: 0.009250774676696293]
[2024-04-17 12:28:57,757: INFO: main: Training : batch 1199 Loss: 0.011473715110312182]
[2024-04-17 12:28:58,400: INFO: main: Training : batch 1200 Loss: 0.013143187378985201]
[2024-04-17 12:28:59,038: INFO: main: Training : batch 1201 Loss: 0.03937160613402738]
[2024-04-17 12:28:59,674: INFO: main: Training : batch 1202 Loss: 0.002665502082607064]
[2024-04-17 12:29:00,313: INFO: main: Training : batch 1203 Loss: 0.026635478195210713]
[2024-04-17 12:29:00,944: INFO: main: Training : batch 1204 Loss: 0.007216869704479203]
[2024-04-17 12:29:01,581: INFO: main: Training : batch 1205 Loss: 0.00304279450173004]
[2024-04-17 12:29:02,214: INFO: main: Training : batch 1206 Loss: 0.0028879488843986368]
[2024-04-17 12:29:02,848: INFO: main: Training : batch 1207 Loss: 0.016482595588065984]
[2024-04-17 12:29:03,479: INFO: main: Training : batch 1208 Loss: 0.032559420065044836]
[2024-04-17 12:29:04,108: INFO: main: Training : batch 1209 Loss: 0.0018859811699242905]
[2024-04-17 12:29:04,737: INFO: main: Training : batch 1210 Loss: 0.006120760524990943]
[2024-04-17 12:29:05,366: INFO: main: Training : batch 1211 Loss: 0.007282491888866864]
[2024-04-17 12:29:05,993: INFO: main: Training : batch 1212 Loss: 0.00876162838504231]
[2024-04-17 12:29:06,626: INFO: main: Training : batch 1213 Loss: 0.01164663500246902]
[2024-04-17 12:29:07,255: INFO: main: Training : batch 1214 Loss: 0.04149871339108977]
[2024-04-17 12:29:07,886: INFO: main: Training : batch 1215 Loss: 0.005171800785601761]
[2024-04-17 12:29:08,519: INFO: main: Training : batch 1216 Loss: 0.006475867299901961]
[2024-04-17 12:29:09,150: INFO: main: Training : batch 1217 Loss: 0.00506794984752933]
[2024-04-17 12:29:09,780: INFO: main: Training : batch 1218 Loss: 0.01517119107023924]
[2024-04-17 12:29:10,413: INFO: main: Training : batch 1219 Loss: 0.0092704594699548]
[2024-04-17 12:29:11,054: INFO: main: Training : batch 1220 Loss: 0.009862546294837194]
[2024-04-17 12:29:11,692: INFO: main: Training : batch 1221 Loss: 0.006768283387529598]
[2024-04-17 12:29:12,336: INFO: main: Training : batch 1222 Loss: 0.024585567607995733]
[2024-04-17 12:29:12,981: INFO: main: Training : batch 1223 Loss: 0.00898591639188328]
[2024-04-17 12:29:13,618: INFO: main: Training : batch 1224 Loss: 0.010327832295904355]
[2024-04-17 12:29:14,250: INFO: main: Training : batch 1225 Loss: 0.0028326551385527716]
[2024-04-17 12:29:14,881: INFO: main: Training : batch 1226 Loss: 0.008337612511435204]
[2024-04-17 12:29:15,518: INFO: main: Training : batch 1227 Loss: 0.01199133440670431]
[2024-04-17 12:29:16,150: INFO: main: Training : batch 1228 Loss: 0.01701346664455277]
[2024-04-17 12:29:16,778: INFO: main: Training : batch 1229 Loss: 0.026352247880319275]
[2024-04-17 12:29:17,408: INFO: main: Training : batch 1230 Loss: 0.022748580375310846]
[2024-04-17 12:29:18,036: INFO: main: Training : batch 1231 Loss: 0.0323486050461514]
[2024-04-17 12:29:18,667: INFO: main: Training : batch 1232 Loss: 0.002620998204094793]
[2024-04-17 12:29:19,302: INFO: main: Training : batch 1233 Loss: 0.004127015152316154]
[2024-04-17 12:29:19,936: INFO: main: Training : batch 1234 Loss: 0.008889551874102269]
[2024-04-17 12:29:20,567: INFO: main: Training : batch 1235 Loss: 0.014870030951692684]
[2024-04-17 12:29:21,195: INFO: main: Training : batch 1236 Loss: 0.004637995639920621]
[2024-04-17 12:29:21,831: INFO: main: Training : batch 1237 Loss: 0.006354605816726878]
[2024-04-17 12:29:22,464: INFO: main: Training : batch 1238 Loss: 0.013501072624405563]
[2024-04-17 12:29:23,092: INFO: main: Training : batch 1239 Loss: 0.016255764030021783]
[2024-04-17 12:29:23,724: INFO: main: Training : batch 1240 Loss: 0.028825597318435217]
[2024-04-17 12:29:24,367: INFO: main: Training : batch 1241 Loss: 0.024488066248439952]
[2024-04-17 12:29:25,006: INFO: main: Training : batch 1242 Loss: 0.01582965829575131]
[2024-04-17 12:29:25,645: INFO: main: Training : batch 1243 Loss: 0.0025363700274843404]
[2024-04-17 12:29:26,284: INFO: main: Training : batch 1244 Loss: 0.006473787606128104]
[2024-04-17 12:29:26,917: INFO: main: Training : batch 1245 Loss: 0.0035189423477872952]
[2024-04-17 12:29:27,547: INFO: main: Training : batch 1246 Loss: 0.022393361450612475]
[2024-04-17 12:29:28,178: INFO: main: Training : batch 1247 Loss: 0.011286003221061934]
[2024-04-17 12:29:28,809: INFO: main: Training : batch 1248 Loss: 0.0154869250730416]
[2024-04-17 12:29:29,439: INFO: main: Training : batch 1249 Loss: 0.006957961102486804]
[2024-04-17 12:29:30,070: INFO: main: Training : batch 1250 Loss: 0.0023523664486058077]
[2024-04-17 12:29:30,703: INFO: main: Training : batch 1251 Loss: 0.008863993154335073]
[2024-04-17 12:29:31,335: INFO: main: Training : batch 1252 Loss: 0.021674743263858622]
[2024-04-17 12:29:31,970: INFO: main: Training : batch 1253 Loss: 0.025946262117238987]
[2024-04-17 12:29:32,604: INFO: main: Training : batch 1254 Loss: 0.022676674392576028]
[2024-04-17 12:29:33,237: INFO: main: Training : batch 1255 Loss: 0.003459115003467139]
[2024-04-17 12:29:33,872: INFO: main: Training : batch 1256 Loss: 0.0162691743627747]
[2024-04-17 12:29:34,504: INFO: main: Training : batch 1257 Loss: 0.016104477976884548]
[2024-04-17 12:29:35,134: INFO: main: Training : batch 1258 Loss: 0.005419571988509732]
[2024-04-17 12:29:35,762: INFO: main: Training : batch 1259 Loss: 0.010472610054364145]
[2024-04-17 12:29:36,390: INFO: main: Training : batch 1260 Loss: 0.017673537335064538]
[2024-04-17 12:29:37,025: INFO: main: Training : batch 1261 Loss: 0.009123195862124208]
[2024-04-17 12:29:37,658: INFO: main: Training : batch 1262 Loss: 0.005535453913920811]
[2024-04-17 12:29:38,292: INFO: main: Training : batch 1263 Loss: 0.012847869619160082]
[2024-04-17 12:29:38,928: INFO: main: Training : batch 1264 Loss: 0.004442961552668468]
[2024-04-17 12:29:39,577: INFO: main: Training : batch 1265 Loss: 0.010338959613966985]
[2024-04-17 12:29:40,215: INFO: main: Training : batch 1266 Loss: 0.008331092092307268]
[2024-04-17 12:29:40,846: INFO: main: Training : batch 1267 Loss: 0.012528529459339072]
[2024-04-17 12:29:41,476: INFO: main: Training : batch 1268 Loss: 0.003670725777388463]
[2024-04-17 12:29:42,109: INFO: main: Training : batch 1269 Loss: 0.004742698865554647]
[2024-04-17 12:29:42,740: INFO: main: Training : batch 1270 Loss: 0.007891080599114962]
[2024-04-17 12:29:43,373: INFO: main: Training : batch 1271 Loss: 0.010109042274129242]
[2024-04-17 12:29:44,003: INFO: main: Training : batch 1272 Loss: 0.0120407582460459]
[2024-04-17 12:29:44,638: INFO: main: Training : batch 1273 Loss: 0.02529809006041917]
[2024-04-17 12:29:45,272: INFO: main: Training : batch 1274 Loss: 0.014809003401611629]
[2024-04-17 12:29:45,905: INFO: main: Training : batch 1275 Loss: 0.006425200935377497]
[2024-04-17 12:29:46,535: INFO: main: Training : batch 1276 Loss: 0.04383800289523077]
[2024-04-17 12:29:47,167: INFO: main: Training : batch 1277 Loss: 0.02012782260658943]
[2024-04-17 12:29:47,803: INFO: main: Training : batch 1278 Loss: 0.01900968295599066]
[2024-04-17 12:29:48,432: INFO: main: Training : batch 1279 Loss: 0.01730881388226346]
[2024-04-17 12:29:49,058: INFO: main: Training : batch 1280 Loss: 0.02786732588392691]
[2024-04-17 12:29:49,703: INFO: main: Training : batch 1281 Loss: 0.027080007142015455]
[2024-04-17 12:29:50,333: INFO: main: Training : batch 1282 Loss: 0.01972576937329035]
[2024-04-17 12:29:50,971: INFO: main: Training : batch 1283 Loss: 0.01648858892652842]
[2024-04-17 12:29:51,609: INFO: main: Training : batch 1284 Loss: 0.007212470514629387]
[2024-04-17 12:29:52,255: INFO: main: Training : batch 1285 Loss: 0.02101207594690329]
[2024-04-17 12:29:52,900: INFO: main: Training : batch 1286 Loss: 0.006288910100699636]
[2024-04-17 12:29:53,541: INFO: main: Training : batch 1287 Loss: 0.007824068372204654]
[2024-04-17 12:29:54,169: INFO: main: Training : batch 1288 Loss: 0.005516424858455584]
[2024-04-17 12:29:54,799: INFO: main: Training : batch 1289 Loss: 0.008998283334562714]
[2024-04-17 12:29:55,426: INFO: main: Training : batch 1290 Loss: 0.002713947239881672]
[2024-04-17 12:29:56,060: INFO: main: Training : batch 1291 Loss: 0.01111421779756693]
[2024-04-17 12:29:56,690: INFO: main: Training : batch 1292 Loss: 0.02775965759536806]
[2024-04-17 12:29:57,322: INFO: main: Training : batch 1293 Loss: 0.02620605561554993]
[2024-04-17 12:29:57,953: INFO: main: Training : batch 1294 Loss: 0.011183677083944278]
[2024-04-17 12:29:58,584: INFO: main: Training : batch 1295 Loss: 0.0105466573550981]
[2024-04-17 12:29:59,213: INFO: main: Training : batch 1296 Loss: 0.05262523061193839]
[2024-04-17 12:29:59,842: INFO: main: Training : batch 1297 Loss: 0.003302266301007553]
[2024-04-17 12:30:00,471: INFO: main: Training : batch 1298 Loss: 0.016092764392187427]
[2024-04-17 12:30:01,103: INFO: main: Training : batch 1299 Loss: 0.008166408438629584]
[2024-04-17 12:30:01,735: INFO: main: Training : batch 1300 Loss: 0.004022706727089058]
[2024-04-17 12:30:02,365: INFO: main: Training : batch 1301 Loss: 0.008416056865012164]
[2024-04-17 12:30:02,993: INFO: main: Training : batch 1302 Loss: 0.0052455209560240195]
[2024-04-17 12:30:03,627: INFO: main: Training : batch 1303 Loss: 0.01827224131264987]
[2024-04-17 12:30:04,270: INFO: main: Training : batch 1304 Loss: 0.005143226285998498]
[2024-04-17 12:30:04,909: INFO: main: Training : batch 1305 Loss: 0.0026539639405767377]
[2024-04-17 12:30:05,542: INFO: main: Training : batch 1306 Loss: 0.020532449037397517]
[2024-04-17 12:30:06,184: INFO: main: Training : batch 1307 Loss: 0.02799762960519591]
[2024-04-17 12:30:06,819: INFO: main: Training : batch 1308 Loss: 0.013117634555754761]
[2024-04-17 12:30:07,461: INFO: main: Training : batch 1309 Loss: 0.01349323116594635]
[2024-04-17 12:30:08,096: INFO: main: Training : batch 1310 Loss: 0.017363985677166818]
[2024-04-17 12:30:08,725: INFO: main: Training : batch 1311 Loss: 0.014607856693674799]
[2024-04-17 12:30:09,355: INFO: main: Training : batch 1312 Loss: 0.03784965814058198]
[2024-04-17 12:30:09,986: INFO: main: Training : batch 1313 Loss: 0.013828207304570768]
[2024-04-17 12:30:10,613: INFO: main: Training : batch 1314 Loss: 0.01799557328261655]
[2024-04-17 12:30:11,242: INFO: main: Training : batch 1315 Loss: 0.02556853109300575]
[2024-04-17 12:30:11,873: INFO: main: Training : batch 1316 Loss: 0.020169704226572577]
[2024-04-17 12:30:12,504: INFO: main: Training : batch 1317 Loss: 0.004190834142246393]
[2024-04-17 12:30:13,135: INFO: main: Training : batch 1318 Loss: 0.027478442655912174]
[2024-04-17 12:30:13,765: INFO: main: Training : batch 1319 Loss: 0.014573048380942038]
[2024-04-17 12:30:14,399: INFO: main: Training : batch 1320 Loss: 0.02253190247612933]
[2024-04-17 12:30:15,033: INFO: main: Training : batch 1321 Loss: 0.010641224763715944]
[2024-04-17 12:30:15,669: INFO: main: Training : batch 1322 Loss: 0.006625535742885419]
[2024-04-17 12:30:16,301: INFO: main: Training : batch 1323 Loss: 0.02084924801946003]
[2024-04-17 12:30:16,938: INFO: main: Training : batch 1324 Loss: 0.015982658012151128]
[2024-04-17 12:30:17,579: INFO: main: Training : batch 1325 Loss: 0.011314335818774085]
[2024-04-17 12:30:18,222: INFO: main: Training : batch 1326 Loss: 0.014031808261550677]
[2024-04-17 12:30:18,858: INFO: main: Training : batch 1327 Loss: 0.011249869928640478]
[2024-04-17 12:30:19,493: INFO: main: Training : batch 1328 Loss: 0.03150650416572705]
[2024-04-17 12:30:20,129: INFO: main: Training : batch 1329 Loss: 0.03297704127085722]
[2024-04-17 12:30:20,757: INFO: main: Training : batch 1330 Loss: 0.017026753269828037]
[2024-04-17 12:30:21,389: INFO: main: Training : batch 1331 Loss: 0.007262931639485804]
[2024-04-17 12:30:22,016: INFO: main: Training : batch 1332 Loss: 0.0029297919090037934]
[2024-04-17 12:30:22,646: INFO: main: Training : batch 1333 Loss: 0.0031662417273672938]
[2024-04-17 12:30:23,276: INFO: main: Training : batch 1334 Loss: 0.008326694873219218]
[2024-04-17 12:30:23,906: INFO: main: Training : batch 1335 Loss: 0.012938221806020193]
[2024-04-17 12:30:24,538: INFO: main: Training : batch 1336 Loss: 0.006972959420296847]
[2024-04-17 12:30:25,165: INFO: main: Training : batch 1337 Loss: 0.008409417125266607]
[2024-04-17 12:30:25,796: INFO: main: Training : batch 1338 Loss: 0.00867106931861149]
[2024-04-17 12:30:26,426: INFO: main: Training : batch 1339 Loss: 0.008482353617140946]
[2024-04-17 12:30:27,055: INFO: main: Training : batch 1340 Loss: 0.007163989195179313]
[2024-04-17 12:30:27,684: INFO: main: Training : batch 1341 Loss: 0.018902545901504163]
[2024-04-17 12:30:28,318: INFO: main: Training : batch 1342 Loss: 0.008368733661870851]
[2024-04-17 12:30:28,945: INFO: main: Training : batch 1343 Loss: 0.008243216564823687]
[2024-04-17 12:30:29,579: INFO: main: Training : batch 1344 Loss: 0.01232117888310121]
[2024-04-17 12:30:30,218: INFO: main: Training : batch 1345 Loss: 0.008624227648509307]
[2024-04-17 12:30:30,860: INFO: main: Training : batch 1346 Loss: 0.006924322183317431]
[2024-04-17 12:30:31,503: INFO: main: Training : batch 1347 Loss: 0.013221173158311342]
[2024-04-17 12:30:32,143: INFO: main: Training : batch 1348 Loss: 0.013252465025080876]
[2024-04-17 12:30:32,779: INFO: main: Training : batch 1349 Loss: 0.0037486027063303637]
[2024-04-17 12:30:33,407: INFO: main: Training : batch 1350 Loss: 0.021665902320800415]
[2024-04-17 12:30:34,038: INFO: main: Training : batch 1351 Loss: 0.022058111254278607]
[2024-04-17 12:30:34,669: INFO: main: Training : batch 1352 Loss: 0.04090019319608575]
[2024-04-17 12:30:35,302: INFO: main: Training : batch 1353 Loss: 0.00282386512402137]
[2024-04-17 12:30:35,927: INFO: main: Training : batch 1354 Loss: 0.016932052494921395]
[2024-04-17 12:30:36,565: INFO: main: Training : batch 1355 Loss: 0.005636756869835905]
[2024-04-17 12:30:37,196: INFO: main: Training : batch 1356 Loss: 0.018867036655239255]
[2024-04-17 12:30:37,832: INFO: main: Training : batch 1357 Loss: 0.005961476347792269]
[2024-04-17 12:30:38,467: INFO: main: Training : batch 1358 Loss: 0.014059488265628837]
[2024-04-17 12:30:39,102: INFO: main: Training : batch 1359 Loss: 0.009412960760470015]
[2024-04-17 12:30:39,733: INFO: main: Training : batch 1360 Loss: 0.011415183771900848]
[2024-04-17 12:30:40,365: INFO: main: Training : batch 1361 Loss: 0.0024960739840058205]
[2024-04-17 12:30:40,995: INFO: main: Training : batch 1362 Loss: 0.03597622322147027]
[2024-04-17 12:30:41,626: INFO: main: Training : batch 1363 Loss: 0.006945496140708977]
[2024-04-17 12:30:42,257: INFO: main: Training : batch 1364 Loss: 0.005134752896064439]
[2024-04-17 12:30:42,885: INFO: main: Training : batch 1365 Loss: 0.014469591614561857]
[2024-04-17 12:30:43,521: INFO: main: Training : batch 1366 Loss: 0.008309540349531066]
[2024-04-17 12:30:44,157: INFO: main: Training : batch 1367 Loss: 0.009065113177552358]
[2024-04-17 12:30:44,793: INFO: main: Training : batch 1368 Loss: 0.016453741222210116]
[2024-04-17 12:30:45,428: INFO: main: Training : batch 1369 Loss: 0.02443803942080385]
[2024-04-17 12:30:46,060: INFO: main: Training : batch 1370 Loss: 0.006970060569977593]
[2024-04-17 12:30:46,696: INFO: main: Training : batch 1371 Loss: 0.02777824364798468]
[2024-04-17 12:30:47,329: INFO: main: Training : batch 1372 Loss: 0.023334771707172602]
[2024-04-17 12:30:47,956: INFO: main: Training : batch 1373 Loss: 0.005114842840795643]
[2024-04-17 12:30:48,585: INFO: main: Training : batch 1374 Loss: 0.009380504119969964]
[2024-04-17 12:30:49,216: INFO: main: Training : batch 1375 Loss: 0.01278730781161426]
[2024-04-17 12:30:49,850: INFO: main: Training : batch 1376 Loss: 0.008635461007348228]
[2024-04-17 12:30:50,481: INFO: main: Training : batch 1377 Loss: 0.004709213430093501]
[2024-04-17 12:30:51,107: INFO: main: Training : batch 1378 Loss: 0.0026757563806736654]
[2024-04-17 12:30:51,738: INFO: main: Training : batch 1379 Loss: 0.004110087123539037]
[2024-04-17 12:30:52,364: INFO: main: Training : batch 1380 Loss: 0.026827237858013876]
[2024-04-17 12:30:52,994: INFO: main: Training : batch 1381 Loss: 0.02927768140785998]
[2024-04-17 12:30:53,628: INFO: main: Training : batch 1382 Loss: 0.020664407196074753]
[2024-04-17 12:30:54,258: INFO: main: Training : batch 1383 Loss: 0.002969080006966049]
[2024-04-17 12:30:54,885: INFO: main: Training : batch 1384 Loss: 0.0030285857359964806]
[2024-04-17 12:30:55,516: INFO: main: Training : batch 1385 Loss: 0.00784557101379278]
[2024-04-17 12:30:56,145: INFO: main: Training : batch 1386 Loss: 0.00837918854770537]
[2024-04-17 12:30:56,784: INFO: main: Training : batch 1387 Loss: 0.006983122171839187]
[2024-04-17 12:30:57,419: INFO: main: Training : batch 1388 Loss: 0.0036001279914104583]
[2024-04-17 12:30:58,055: INFO: main: Training : batch 1389 Loss: 0.0059729937156812445]
[2024-04-17 12:30:58,691: INFO: main: Training : batch 1390 Loss: 0.020800193830615616]
[2024-04-17 12:30:59,331: INFO: main: Training : batch 1391 Loss: 0.00562968601968555]
[2024-04-17 12:30:59,955: INFO: main: Training : batch 1392 Loss: 0.0031580540827001414]
[2024-04-17 12:31:00,590: INFO: main: Training : batch 1393 Loss: 0.023201460216887626]
[2024-04-17 12:31:01,226: INFO: main: Training : batch 1394 Loss: 0.018715868894099298]
[2024-04-17 12:31:01,854: INFO: main: Training : batch 1395 Loss: 0.019397361923877986]
[2024-04-17 12:31:02,481: INFO: main: Training : batch 1396 Loss: 0.01583339944404091]
[2024-04-17 12:31:03,109: INFO: main: Training : batch 1397 Loss: 0.04982006359678063]
[2024-04-17 12:31:03,739: INFO: main: Training : batch 1398 Loss: 0.006494937743034633]
[2024-04-17 12:31:04,365: INFO: main: Training : batch 1399 Loss: 0.0077891962622784455]
[2024-04-17 12:31:04,991: INFO: main: Training : batch 1400 Loss: 0.006882806349031233]
[2024-04-17 12:31:05,621: INFO: main: Training : batch 1401 Loss: 0.01030514351779523]
[2024-04-17 12:31:06,253: INFO: main: Training : batch 1402 Loss: 0.005331606815394228]
[2024-04-17 12:31:06,883: INFO: main: Training : batch 1403 Loss: 0.0040983884700530335]
[2024-04-17 12:31:07,520: INFO: main: Training : batch 1404 Loss: 0.013254302434772398]
[2024-04-17 12:31:08,148: INFO: main: Training : batch 1405 Loss: 0.02576165360864124]
[2024-04-17 12:31:08,775: INFO: main: Training : batch 1406 Loss: 0.011071292807570712]
[2024-04-17 12:31:09,403: INFO: main: Training : batch 1407 Loss: 0.011075602216720083]
[2024-04-17 12:31:10,041: INFO: main: Training : batch 1408 Loss: 0.01374031926085289]
[2024-04-17 12:31:10,679: INFO: main: Training : batch 1409 Loss: 0.003994362494089004]
[2024-04-17 12:31:11,312: INFO: main: Training : batch 1410 Loss: 0.022724813167494883]
[2024-04-17 12:31:11,946: INFO: main: Training : batch 1411 Loss: 0.0051975525501874965]
[2024-04-17 12:31:12,584: INFO: main: Training : batch 1412 Loss: 0.01987398639094412]
[2024-04-17 12:31:13,221: INFO: main: Training : batch 1413 Loss: 0.004108909732511866]
[2024-04-17 12:31:13,847: INFO: main: Training : batch 1414 Loss: 0.0294545751884325]
[2024-04-17 12:31:14,481: INFO: main: Training : batch 1415 Loss: 0.004343939675693613]
[2024-04-17 12:31:15,109: INFO: main: Training : batch 1416 Loss: 0.01098126444353751]
[2024-04-17 12:31:15,741: INFO: main: Training : batch 1417 Loss: 0.010423385891256855]
[2024-04-17 12:31:16,366: INFO: main: Training : batch 1418 Loss: 0.0015908743350041368]
[2024-04-17 12:31:17,000: INFO: main: Training : batch 1419 Loss: 0.025486432827836632]
[2024-04-17 12:31:17,627: INFO: main: Training : batch 1420 Loss: 0.01222916204206155]
[2024-04-17 12:31:18,259: INFO: main: Training : batch 1421 Loss: 0.014249240017758309]
[2024-04-17 12:31:18,888: INFO: main: Training : batch 1422 Loss: 0.013372434658242118]
[2024-04-17 12:31:19,515: INFO: main: Training : batch 1423 Loss: 0.01707128874402945]
[2024-04-17 12:31:20,148: INFO: main: Training : batch 1424 Loss: 0.02318751236212032]
[2024-04-17 12:31:20,775: INFO: main: Training : batch 1425 Loss: 0.0032450990099168295]
[2024-04-17 12:31:21,406: INFO: main: Training : batch 1426 Loss: 0.009079945892919146]
[2024-04-17 12:31:22,034: INFO: main: Training : batch 1427 Loss: 0.01149395985296299]
[2024-04-17 12:31:22,661: INFO: main: Training : batch 1428 Loss: 0.020448743495322722]
[2024-04-17 12:31:23,301: INFO: main: Training : batch 1429 Loss: 0.008201735536763871]
[2024-04-17 12:31:23,505: INFO: main: Eval Epoch : batch 0 Loss: 0.0033633873031849704]
[2024-04-17 12:31:23,710: INFO: main: Eval Epoch : batch 1 Loss: 0.006259358170201925]
[2024-04-17 12:31:23,913: INFO: main: Eval Epoch : batch 2 Loss: 0.03579412847815003]
[2024-04-17 12:31:24,116: INFO: main: Eval Epoch : batch 3 Loss: 0.015488581010505584]
[2024-04-17 12:31:24,325: INFO: main: Eval Epoch : batch 4 Loss: 0.003913160910805987]
[2024-04-17 12:31:24,528: INFO: main: Eval Epoch : batch 5 Loss: 0.02034127732908365]
[2024-04-17 12:31:24,733: INFO: main: Eval Epoch : batch 6 Loss: 0.012179124945235794]
[2024-04-17 12:31:24,935: INFO: main: Eval Epoch : batch 7 Loss: 0.00843469878302084]
[2024-04-17 12:31:25,138: INFO: main: Eval Epoch : batch 8 Loss: 0.027822935462530483]
[2024-04-17 12:31:25,346: INFO: main: Eval Epoch : batch 9 Loss: 0.021379116105494943]
[2024-04-17 12:31:25,551: INFO: main: Eval Epoch : batch 10 Loss: 0.011034700584289094]
[2024-04-17 12:31:25,758: INFO: main: Eval Epoch : batch 11 Loss: 0.038728347407259195]
[2024-04-17 12:31:25,961: INFO: main: Eval Epoch : batch 12 Loss: 0.01129598364027926]
[2024-04-17 12:31:26,158: INFO: main: Eval Epoch : batch 13 Loss: 0.018290459840001987]
[2024-04-17 12:31:26,361: INFO: main: Eval Epoch : batch 14 Loss: 0.007392061118489728]
[2024-04-17 12:31:26,560: INFO: main: Eval Epoch : batch 15 Loss: 0.024100695941475774]
[2024-04-17 12:31:26,760: INFO: main: Eval Epoch : batch 16 Loss: 0.018413313909334032]
[2024-04-17 12:31:26,960: INFO: main: Eval Epoch : batch 17 Loss: 0.023298006001674202]
[2024-04-17 12:31:27,161: INFO: main: Eval Epoch : batch 18 Loss: 0.02111672580388379]
[2024-04-17 12:31:27,362: INFO: main: Eval Epoch : batch 19 Loss: 0.014354935593900263]
[2024-04-17 12:31:27,567: INFO: main: Eval Epoch : batch 20 Loss: 0.005545381865164435]
[2024-04-17 12:31:27,768: INFO: main: Eval Epoch : batch 21 Loss: 0.010534425146203978]
[2024-04-17 12:31:27,969: INFO: main: Eval Epoch : batch 22 Loss: 0.012350916399944917]
[2024-04-17 12:31:28,171: INFO: main: Eval Epoch : batch 23 Loss: 0.018324617455674947]
[2024-04-17 12:31:28,376: INFO: main: Eval Epoch : batch 24 Loss: 0.028398018649193942]
[2024-04-17 12:31:28,576: INFO: main: Eval Epoch : batch 25 Loss: 0.006936677028845738]
[2024-04-17 12:31:28,773: INFO: main: Eval Epoch : batch 26 Loss: 0.007975308377012678]
[2024-04-17 12:31:28,973: INFO: main: Eval Epoch : batch 27 Loss: 0.010726523095537333]
[2024-04-17 12:31:29,170: INFO: main: Eval Epoch : batch 28 Loss: 0.008010448410882179]
[2024-04-17 12:31:29,375: INFO: main: Eval Epoch : batch 29 Loss: 0.01467713021774196]
[2024-04-17 12:31:29,574: INFO: main: Eval Epoch : batch 30 Loss: 0.008977546522739428]
[2024-04-17 12:31:29,773: INFO: main: Eval Epoch : batch 31 Loss: 0.03552740797685991]
[2024-04-17 12:31:29,976: INFO: main: Eval Epoch : batch 32 Loss: 0.0152191793560096]
[2024-04-17 12:31:30,175: INFO: main: Eval Epoch : batch 33 Loss: 0.008691820404037105]
[2024-04-17 12:31:30,378: INFO: main: Eval Epoch : batch 34 Loss: 0.031205908497434123]
[2024-04-17 12:31:30,581: INFO: main: Eval Epoch : batch 35 Loss: 0.020738492503926353]
[2024-04-17 12:31:30,779: INFO: main: Eval Epoch : batch 36 Loss: 0.01461836099154325]
[2024-04-17 12:31:30,978: INFO: main: Eval Epoch : batch 37 Loss: 0.04757542321376518]
[2024-04-17 12:31:31,176: INFO: main: Eval Epoch : batch 38 Loss: 0.022518953753203282]
[2024-04-17 12:31:31,379: INFO: main: Eval Epoch : batch 39 Loss: 0.013591807274387854]
[2024-04-17 12:31:31,580: INFO: main: Eval Epoch : batch 40 Loss: 0.018340935578442047]
[2024-04-17 12:31:31,780: INFO: main: Eval Epoch : batch 41 Loss: 0.01555299652498676]
[2024-04-17 12:31:31,982: INFO: main: Eval Epoch : batch 42 Loss: 0.006587907436597172]
[2024-04-17 12:31:32,180: INFO: main: Eval Epoch : batch 43 Loss: 0.013191421981007392]
[2024-04-17 12:31:32,382: INFO: main: Eval Epoch : batch 44 Loss: 0.01236138640382259]
[2024-04-17 12:31:32,582: INFO: main: Eval Epoch : batch 45 Loss: 0.012753063451161235]
[2024-04-17 12:31:32,784: INFO: main: Eval Epoch : batch 46 Loss: 0.017819149606571872]
[2024-04-17 12:31:32,987: INFO: main: Eval Epoch : batch 47 Loss: 0.04353075826056521]
[2024-04-17 12:31:33,185: INFO: main: Eval Epoch : batch 48 Loss: 0.02330450289420266]
[2024-04-17 12:31:33,385: INFO: main: Eval Epoch : batch 49 Loss: 0.01109495407308343]
[2024-04-17 12:31:33,579: INFO: main: Eval Epoch : batch 50 Loss: 0.0194934684336864]
[2024-04-17 12:31:33,780: INFO: main: Eval Epoch : batch 51 Loss: 0.050386186487345455]
[2024-04-17 12:31:33,983: INFO: main: Eval Epoch : batch 52 Loss: 0.012179533483205808]
[2024-04-17 12:31:34,183: INFO: main: Eval Epoch : batch 53 Loss: 0.00887833655209284]
[2024-04-17 12:31:34,385: INFO: main: Eval Epoch : batch 54 Loss: 0.018478563381176327]
[2024-04-17 12:31:34,585: INFO: main: Eval Epoch : batch 55 Loss: 0.02860409978678254]
[2024-04-17 12:31:34,783: INFO: main: Eval Epoch : batch 56 Loss: 0.0011955222004507845]
[2024-04-17 12:31:34,982: INFO: main: Eval Epoch : batch 57 Loss: 0.011646414812518439]
[2024-04-17 12:31:35,181: INFO: main: Eval Epoch : batch 58 Loss: 0.0007799913705834184]
[2024-04-17 12:31:35,379: INFO: main: Eval Epoch : batch 59 Loss: 0.03988033776712094]
[2024-04-17 12:31:35,580: INFO: main: Eval Epoch : batch 60 Loss: 0.005812531204801953]
[2024-04-17 12:31:35,785: INFO: main: Eval Epoch : batch 61 Loss: 0.01177309452903315]
[2024-04-17 12:31:35,989: INFO: main: Eval Epoch : batch 62 Loss: 0.007681063324982365]
[2024-04-17 12:31:36,193: INFO: main: Eval Epoch : batch 63 Loss: 0.007498514788333315]
[2024-04-17 12:31:36,397: INFO: main: Eval Epoch : batch 64 Loss: 0.0053398694731433315]
[2024-04-17 12:31:36,604: INFO: main: Eval Epoch : batch 65 Loss: 0.011348017815140573]
[2024-04-17 12:31:36,805: INFO: main: Eval Epoch : batch 66 Loss: 0.0015143375643632887]
[2024-04-17 12:31:37,012: INFO: main: Eval Epoch : batch 67 Loss: 0.01551969489405737]
[2024-04-17 12:31:37,217: INFO: main: Eval Epoch : batch 68 Loss: 0.053846667074817556]
[2024-04-17 12:31:37,420: INFO: main: Eval Epoch : batch 69 Loss: 0.01770383333329699]
[2024-04-17 12:31:37,623: INFO: main: Eval Epoch : batch 70 Loss: 0.003758114742084586]
[2024-04-17 12:31:37,831: INFO: main: Eval Epoch : batch 71 Loss: 0.009488240186524229]
[2024-04-17 12:31:38,040: INFO: main: Eval Epoch : batch 72 Loss: 0.016601340621937077]
[2024-04-17 12:31:38,252: INFO: main: Eval Epoch : batch 73 Loss: 0.002840613074797757]
[2024-04-17 12:31:38,453: INFO: main: Eval Epoch : batch 74 Loss: 0.010469108950987816]
[2024-04-17 12:31:38,657: INFO: main: Eval Epoch : batch 75 Loss: 0.017353322306180644]
[2024-04-17 12:31:38,862: INFO: main: Eval Epoch : batch 76 Loss: 0.0031936704111253577]
[2024-04-17 12:31:39,067: INFO: main: Eval Epoch : batch 77 Loss: 0.00209551216218385]
[2024-04-17 12:31:39,278: INFO: main: Eval Epoch : batch 78 Loss: 0.039612526312098086]
[2024-04-17 12:31:39,482: INFO: main: Eval Epoch : batch 79 Loss: 0.018171849842934063]
[2024-04-17 12:31:39,683: INFO: main: Eval Epoch : batch 80 Loss: 0.01217070092775569]
[2024-04-17 12:31:39,887: INFO: main: Eval Epoch : batch 81 Loss: 0.03170554581613519]
[2024-04-17 12:31:40,088: INFO: main: Eval Epoch : batch 82 Loss: 0.005440379387542258]
[2024-04-17 12:31:40,290: INFO: main: Eval Epoch : batch 83 Loss: 0.012239896435703152]
[2024-04-17 12:31:40,489: INFO: main: Eval Epoch : batch 84 Loss: 0.000544020851361642]
[2024-04-17 12:31:40,686: INFO: main: Eval Epoch : batch 85 Loss: 0.03558062923641215]
[2024-04-17 12:31:40,886: INFO: main: Eval Epoch : batch 86 Loss: 0.025884259138158703]
[2024-04-17 12:31:41,088: INFO: main: Eval Epoch : batch 87 Loss: 0.005183365422292879]
[2024-04-17 12:31:41,290: INFO: main: Eval Epoch : batch 88 Loss: 0.01988503464535149]
[2024-04-17 12:31:41,493: INFO: main: Eval Epoch : batch 89 Loss: 0.025206404104605878]
[2024-04-17 12:31:41,694: INFO: main: Eval Epoch : batch 90 Loss: 0.02353222164973557]
[2024-04-17 12:31:41,894: INFO: main: Eval Epoch : batch 91 Loss: 0.01208538151993106]
[2024-04-17 12:31:42,096: INFO: main: Eval Epoch : batch 92 Loss: 0.004602514172386474]
[2024-04-17 12:31:42,296: INFO: main: Eval Epoch : batch 93 Loss: 0.0053849072540613]
[2024-04-17 12:31:42,498: INFO: main: Eval Epoch : batch 94 Loss: 0.03587735106798288]
[2024-04-17 12:31:42,699: INFO: main: Eval Epoch : batch 95 Loss: 0.01173188923523011]
[2024-04-17 12:31:42,896: INFO: main: Eval Epoch : batch 96 Loss: 0.01728833201675671]
[2024-04-17 12:31:43,097: INFO: main: Eval Epoch : batch 97 Loss: 0.009344613365521772]
[2024-04-17 12:31:43,295: INFO: main: Eval Epoch : batch 98 Loss: 0.011847010011787144]
[2024-04-17 12:31:43,499: INFO: main: Eval Epoch : batch 99 Loss: 0.013341942088132799]
[2024-04-17 12:31:43,700: INFO: main: Eval Epoch : batch 100 Loss: 0.016781165362641383]
[2024-04-17 12:31:43,901: INFO: main: Eval Epoch : batch 101 Loss: 0.012859214644038677]
[2024-04-17 12:31:44,106: INFO: main: Eval Epoch : batch 102 Loss: 0.011823692389766672]
[2024-04-17 12:31:44,306: INFO: main: Eval Epoch : batch 103 Loss: 0.0011701053840735994]
[2024-04-17 12:31:44,507: INFO: main: Eval Epoch : batch 104 Loss: 0.01923734697271714]
[2024-04-17 12:31:44,709: INFO: main: Eval Epoch : batch 105 Loss: 0.002554117164156516]
[2024-04-17 12:31:44,911: INFO: main: Eval Epoch : batch 106 Loss: 0.007313002867174981]
[2024-04-17 12:31:45,112: INFO: main: Eval Epoch : batch 107 Loss: 0.004167518553803492]
[2024-04-17 12:31:45,316: INFO: main: Eval Epoch : batch 108 Loss: 0.023147976862523425]
[2024-04-17 12:31:45,515: INFO: main: Eval Epoch : batch 109 Loss: 0.012159372776371939]
[2024-04-17 12:31:45,714: INFO: main: Eval Epoch : batch 110 Loss: 0.0075928544258333985]
[2024-04-17 12:31:45,917: INFO: main: Eval Epoch : batch 111 Loss: 0.002738091987676881]
[2024-04-17 12:31:46,116: INFO: main: Eval Epoch : batch 112 Loss: 0.02528849385885074]
[2024-04-17 12:31:46,315: INFO: main: Eval Epoch : batch 113 Loss: 0.00573603252992091]
[2024-04-17 12:31:46,516: INFO: main: Eval Epoch : batch 114 Loss: 0.019605260814874185]
[2024-04-17 12:31:46,715: INFO: main: Eval Epoch : batch 115 Loss: 0.008584019950947819]
[2024-04-17 12:31:46,917: INFO: main: Eval Epoch : batch 116 Loss: 0.010822835191623656]
[2024-04-17 12:31:47,120: INFO: main: Eval Epoch : batch 117 Loss: 0.01589203952496312]
[2024-04-17 12:31:47,319: INFO: main: Eval Epoch : batch 118 Loss: 0.0075759746224291795]
[2024-04-17 12:31:47,519: INFO: main: Eval Epoch : batch 119 Loss: 0.029348346073003947]
[2024-04-17 12:31:47,717: INFO: main: Eval Epoch : batch 120 Loss: 0.002110360284537347]
[2024-04-17 12:31:47,916: INFO: main: Eval Epoch : batch 121 Loss: 0.010568993456891976]
[2024-04-17 12:31:48,117: INFO: main: Eval Epoch : batch 122 Loss: 0.015640057606583973]
[2024-04-17 12:31:48,320: INFO: main: Eval Epoch : batch 123 Loss: 0.02142684945119216]
[2024-04-17 12:31:48,522: INFO: main: Eval Epoch : batch 124 Loss: 0.0019582813965210574]
[2024-04-17 12:31:48,725: INFO: main: Eval Epoch : batch 125 Loss: 0.022114908780143407]
[2024-04-17 12:31:48,928: INFO: main: Eval Epoch : batch 126 Loss: 0.004172648451768415]
[2024-04-17 12:31:49,129: INFO: main: Eval Epoch : batch 127 Loss: 0.006168640702681103]
[2024-04-17 12:31:49,330: INFO: main: Eval Epoch : batch 128 Loss: 0.011756886315692276]
[2024-04-17 12:31:49,534: INFO: main: Eval Epoch : batch 129 Loss: 0.012067246371610474]
[2024-04-17 12:31:49,737: INFO: main: Eval Epoch : batch 130 Loss: 0.010867189265590896]
[2024-04-17 12:31:49,941: INFO: main: Eval Epoch : batch 131 Loss: 0.015359231014033956]
[2024-04-17 12:31:50,144: INFO: main: Eval Epoch : batch 132 Loss: 0.005824443432456147]
[2024-04-17 12:31:50,351: INFO: main: Eval Epoch : batch 133 Loss: 0.005548906986747951]
[2024-04-17 12:31:50,559: INFO: main: Eval Epoch : batch 134 Loss: 0.009500726395149824]
[2024-04-17 12:31:50,761: INFO: main: Eval Epoch : batch 135 Loss: 0.0053512683791123354]
[2024-04-17 12:31:50,970: INFO: main: Eval Epoch : batch 136 Loss: 0.02384582733353017]
[2024-04-17 12:31:51,175: INFO: main: Eval Epoch : batch 137 Loss: 0.020611871501201362]
[2024-04-17 12:31:51,379: INFO: main: Eval Epoch : batch 138 Loss: 0.013039457608538802]
[2024-04-17 12:31:51,585: INFO: main: Eval Epoch : batch 139 Loss: 0.09168645541754328]
[2024-04-17 12:31:51,788: INFO: main: Eval Epoch : batch 140 Loss: 0.021835925417103268]
[2024-04-17 12:31:51,993: INFO: main: Eval Epoch : batch 141 Loss: 0.021023098550517004]
[2024-04-17 12:31:52,196: INFO: main: Eval Epoch : batch 142 Loss: 0.0013249680706610657]
[2024-04-17 12:31:52,399: INFO: main: Eval Epoch : batch 143 Loss: 0.020700733896165998]
[2024-04-17 12:31:52,604: INFO: main: Eval Epoch : batch 144 Loss: 0.02158879737488151]
[2024-04-17 12:31:52,809: INFO: main: Eval Epoch : batch 145 Loss: 0.01808169272293825]
[2024-04-17 12:31:53,015: INFO: main: Eval Epoch : batch 146 Loss: 0.022195558766523668]
[2024-04-17 12:31:53,216: INFO: main: Eval Epoch : batch 147 Loss: 0.022488116052067582]
[2024-04-17 12:31:53,416: INFO: main: Eval Epoch : batch 148 Loss: 0.023479545920973843]
[2024-04-17 12:31:53,617: INFO: main: Eval Epoch : batch 149 Loss: 0.0066958561006310756]
[2024-04-17 12:31:53,816: INFO: main: Eval Epoch : batch 150 Loss: 0.03758829995571495]
[2024-04-17 12:31:54,015: INFO: main: Eval Epoch : batch 151 Loss: 0.03807386932263859]
[2024-04-17 12:31:54,217: INFO: main: Eval Epoch : batch 152 Loss: 0.0027692595856683893]
[2024-04-17 12:31:54,415: INFO: main: Eval Epoch : batch 153 Loss: 0.0035564687187628814]
[2024-04-17 12:31:54,618: INFO: main: Eval Epoch : batch 154 Loss: 0.029649211185678908]
[2024-04-17 12:31:54,821: INFO: main: Eval Epoch : batch 155 Loss: 0.036664618539829254]
[2024-04-17 12:31:55,022: INFO: main: Eval Epoch : batch 156 Loss: 0.01697043808830726]
[2024-04-17 12:31:55,223: INFO: main: Eval Epoch : batch 157 Loss: 0.010231084224318733]
[2024-04-17 12:31:55,424: INFO: main: Eval Epoch : batch 158 Loss: 0.00798117497477136]
[2024-04-17 12:31:55,626: INFO: main: Eval Epoch : batch 159 Loss: 0.00523410566169597]
[2024-04-17 12:31:55,824: INFO: main: Eval Epoch : batch 160 Loss: 0.008178948508597928]
[2024-04-17 12:31:56,020: INFO: main: Eval Epoch : batch 161 Loss: 0.01689565243078191]
[2024-04-17 12:31:56,222: INFO: main: Eval Epoch : batch 162 Loss: 0.009472003156795738]
[2024-04-17 12:31:56,426: INFO: main: Eval Epoch : batch 163 Loss: 0.008747285141368666]
[2024-04-17 12:31:56,629: INFO: main: Eval Epoch : batch 164 Loss: 0.028945677222013126]
[2024-04-17 12:31:56,830: INFO: main: Eval Epoch : batch 165 Loss: 0.01760980309080382]
[2024-04-17 12:31:57,030: INFO: main: Eval Epoch : batch 166 Loss: 0.011063301924632053]
[2024-04-17 12:31:57,231: INFO: main: Eval Epoch : batch 167 Loss: 0.0025342698169808654]
[2024-04-17 12:31:57,433: INFO: main: Eval Epoch : batch 168 Loss: 0.00984090062276466]
[2024-04-17 12:31:57,633: INFO: main: Eval Epoch : batch 169 Loss: 0.010017265878203964]
[2024-04-17 12:31:57,833: INFO: main: Eval Epoch : batch 170 Loss: 0.008423881861351764]
[2024-04-17 12:31:58,030: INFO: main: Eval Epoch : batch 171 Loss: 0.013919169801545134]
[2024-04-17 12:31:58,232: INFO: main: Eval Epoch : batch 172 Loss: 0.022290835767162512]
[2024-04-17 12:31:58,431: INFO: main: Eval Epoch : batch 173 Loss: 0.0018187703424673409]
[2024-04-17 12:31:58,634: INFO: main: Eval Epoch : batch 174 Loss: 0.020614645850294756]
[2024-04-17 12:31:58,836: INFO: main: Eval Epoch : batch 175 Loss: 0.005089487605364107]
[2024-04-17 12:31:59,037: INFO: main: Eval Epoch : batch 176 Loss: 0.011330454395112129]
[2024-04-17 12:31:59,234: INFO: main: Eval Epoch : batch 177 Loss: 0.002133119435527756]
[2024-04-17 12:31:59,436: INFO: main: Eval Epoch : batch 178 Loss: 0.008009796494610892]
[2024-04-17 12:31:59,638: INFO: main: Eval Epoch : batch 179 Loss: 0.0019708527722206415]
[2024-04-17 12:31:59,838: INFO: main: Eval Epoch : batch 180 Loss: 0.015334880318033752]
[2024-04-17 12:32:00,040: INFO: main: Eval Epoch : batch 181 Loss: 0.00728208497680675]
[2024-04-17 12:32:00,240: INFO: main: Eval Epoch : batch 182 Loss: 0.009512554407900694]
[2024-04-17 12:32:00,440: INFO: main: Eval Epoch : batch 183 Loss: 0.011653014478379718]
[2024-04-17 12:32:00,641: INFO: main: Eval Epoch : batch 184 Loss: 0.020137204091052616]
[2024-04-17 12:32:00,842: INFO: main: Eval Epoch : batch 185 Loss: 0.006971975766506054]
[2024-04-17 12:32:01,041: INFO: main: Eval Epoch : batch 186 Loss: 0.00411497874946423]
[2024-04-17 12:32:01,243: INFO: main: Eval Epoch : batch 187 Loss: 0.05640737402462087]
[2024-04-17 12:32:01,445: INFO: main: Eval Epoch : batch 188 Loss: 0.012013062715882657]
[2024-04-17 12:32:01,647: INFO: main: Eval Epoch : batch 189 Loss: 0.01330690469792232]
[2024-04-17 12:32:01,849: INFO: main: Eval Epoch : batch 190 Loss: 0.00571162703611871]
[2024-04-17 12:32:02,047: INFO: main: Eval Epoch : batch 191 Loss: 0.016060436663913796]
[2024-04-17 12:32:02,249: INFO: main: Eval Epoch : batch 192 Loss: 0.007031751843935652]
[2024-04-17 12:32:02,451: INFO: main: Eval Epoch : batch 193 Loss: 0.004352125701307914]
[2024-04-17 12:32:02,652: INFO: main: Eval Epoch : batch 194 Loss: 0.019081657612197354]
[2024-04-17 12:32:02,850: INFO: main: Eval Epoch : batch 195 Loss: 0.012667883630257034]
[2024-04-17 12:32:03,054: INFO: main: Eval Epoch : batch 196 Loss: 0.010753337894675388]
[2024-04-17 12:32:03,260: INFO: main: Eval Epoch : batch 197 Loss: 0.013236255119975355]
[2024-04-17 12:32:03,463: INFO: main: Eval Epoch : batch 198 Loss: 0.006932868801161194]
[2024-04-17 12:32:03,670: INFO: main: Eval Epoch : batch 199 Loss: 0.00857584973886165]
[2024-04-17 12:32:03,873: INFO: main: Eval Epoch : batch 200 Loss: 0.005639436730259918]
[2024-04-17 12:32:04,076: INFO: main: Eval Epoch : batch 201 Loss: 0.03470340302125419]
[2024-04-17 12:32:04,283: INFO: main: Eval Epoch : batch 202 Loss: 0.014801781993199193]
[2024-04-17 12:32:04,491: INFO: main: Eval Epoch : batch 203 Loss: 0.015419751555989888]
[2024-04-17 12:32:04,695: INFO: main: Eval Epoch : batch 204 Loss: 0.020762603869709684]
[2024-04-17 12:32:04,900: INFO: main: Eval Epoch : batch 205 Loss: 0.005461444278435811]
[2024-04-17 12:32:05,109: INFO: main: Eval Epoch : batch 206 Loss: 0.004008571269003384]
[2024-04-17 12:32:05,313: INFO: main: Eval Epoch : batch 207 Loss: 0.003243164260511688]
[2024-04-17 12:32:05,518: INFO: main: Eval Epoch : batch 208 Loss: 0.0015681224623832026]
[2024-04-17 12:32:05,724: INFO: main: Eval Epoch : batch 209 Loss: 0.0014786598674795671]
[2024-04-17 12:32:05,929: INFO: main: Eval Epoch : batch 210 Loss: 0.007245293140080699]
[2024-04-17 12:32:06,134: INFO: main: Eval Epoch : batch 211 Loss: 0.031920575299325474]
[2024-04-17 12:32:06,338: INFO: main: Eval Epoch : batch 212 Loss: 0.006473269668373984]
[2024-04-17 12:32:06,543: INFO: main: Eval Epoch : batch 213 Loss: 0.017574009043328377]
[2024-04-17 12:32:06,750: INFO: main: Eval Epoch : batch 214 Loss: 0.011345275966560401]
[2024-04-17 12:32:06,950: INFO: main: Eval Epoch : batch 215 Loss: 0.03139003329103332]
[2024-04-17 12:32:07,152: INFO: main: Eval Epoch : batch 216 Loss: 0.015810283140835796]
[2024-04-17 12:32:07,352: INFO: main: Eval Epoch : batch 217 Loss: 0.008700194256324656]
[2024-04-17 12:32:07,557: INFO: main: Eval Epoch : batch 218 Loss: 0.023089587558288827]
[2024-04-17 12:32:07,759: INFO: main: Eval Epoch : batch 219 Loss: 0.0010749310616143706]
[2024-04-17 12:32:07,960: INFO: main: Eval Epoch : batch 220 Loss: 0.008392395410291058]
[2024-04-17 12:32:08,158: INFO: main: Eval Epoch : batch 221 Loss: 0.026946212290045478]
[2024-04-17 12:32:08,355: INFO: main: Eval Epoch : batch 222 Loss: 0.00027743730088499477]
[2024-04-17 12:32:08,555: INFO: main: Eval Epoch : batch 223 Loss: 0.02434033363589865]
[2024-04-17 12:32:08,754: INFO: main: Eval Epoch : batch 224 Loss: 0.005719582235917354]
[2024-04-17 12:32:08,954: INFO: main: Eval Epoch : batch 225 Loss: 0.028978302007302576]
[2024-04-17 12:32:09,154: INFO: main: Eval Epoch : batch 226 Loss: 0.03133481498863459]
[2024-04-17 12:32:09,355: INFO: main: Eval Epoch : batch 227 Loss: 0.020842922325344578]
[2024-04-17 12:32:09,559: INFO: main: Eval Epoch : batch 228 Loss: 0.008570319696859115]
[2024-04-17 12:32:09,763: INFO: main: Eval Epoch : batch 229 Loss: 0.0035590838965286176]
[2024-04-17 12:32:09,963: INFO: main: Eval Epoch : batch 230 Loss: 0.00683500131144742]
[2024-04-17 12:32:10,160: INFO: main: Eval Epoch : batch 231 Loss: 0.02365683080346259]
[2024-04-17 12:32:10,361: INFO: main: Eval Epoch : batch 232 Loss: 0.014945129648300748]
[2024-04-17 12:32:10,563: INFO: main: Eval Epoch : batch 233 Loss: 0.013982160560650906]
[2024-04-17 12:32:10,768: INFO: main: Eval Epoch : batch 234 Loss: 0.01781172735791893]
[2024-04-17 12:32:10,968: INFO: main: Eval Epoch : batch 235 Loss: 0.009246472238511978]
[2024-04-17 12:32:11,168: INFO: main: Eval Epoch : batch 236 Loss: 0.027007787350960145]
[2024-04-17 12:32:11,365: INFO: main: Eval Epoch : batch 237 Loss: 0.0011026924711434929]
[2024-04-17 12:32:11,562: INFO: main: Eval Epoch : batch 238 Loss: 0.003960713692903837]
[2024-04-17 12:32:11,765: INFO: main: Eval Epoch : batch 239 Loss: 0.027248035132628794]
[2024-04-17 12:32:11,968: INFO: main: Eval Epoch : batch 240 Loss: 0.008792192399646348]
[2024-04-17 12:32:12,171: INFO: main: Eval Epoch : batch 241 Loss: 0.011464467021269666]
[2024-04-17 12:32:12,370: INFO: main: Eval Epoch : batch 242 Loss: 0.020670798148353788]
[2024-04-17 12:32:12,574: INFO: main: Eval Epoch : batch 243 Loss: 0.01181043348343514]
[2024-04-17 12:32:12,774: INFO: main: Eval Epoch : batch 244 Loss: 0.006188549154899769]
[2024-04-17 12:32:12,977: INFO: main: Eval Epoch : batch 245 Loss: 0.007417602392878915]
[2024-04-17 12:32:13,178: INFO: main: Eval Epoch : batch 246 Loss: 0.008974056494554131]
[2024-04-17 12:32:13,379: INFO: main: Eval Epoch : batch 247 Loss: 0.007446716550528093]
[2024-04-17 12:32:13,582: INFO: main: Eval Epoch : batch 248 Loss: 0.005923138942952117]
[2024-04-17 12:32:13,787: INFO: main: Eval Epoch : batch 249 Loss: 0.018618562569585814]
[2024-04-17 12:32:13,990: INFO: main: Eval Epoch : batch 250 Loss: 0.0032344059423262653]
[2024-04-17 12:32:14,192: INFO: main: Eval Epoch : batch 251 Loss: 0.007784796393004891]
[2024-04-17 12:32:14,392: INFO: main: Eval Epoch : batch 252 Loss: 0.11823151391622067]
[2024-04-17 12:32:14,595: INFO: main: Eval Epoch : batch 253 Loss: 0.030286701042605415]
[2024-04-17 12:32:14,801: INFO: main: Eval Epoch : batch 254 Loss: 0.011278211098072586]
[2024-04-17 12:32:15,003: INFO: main: Eval Epoch : batch 255 Loss: 0.01307469655006675]
[2024-04-17 12:32:15,203: INFO: main: Eval Epoch : batch 256 Loss: 0.012012223513261209]
[2024-04-17 12:32:15,403: INFO: main: Eval Epoch : batch 257 Loss: 0.049081203539156994]
[2024-04-17 12:32:15,606: INFO: main: Eval Epoch : batch 258 Loss: 0.062188645904577226]
[2024-04-17 12:32:15,815: INFO: main: Eval Epoch : batch 259 Loss: 0.014338436476025871]
[2024-04-17 12:32:16,012: INFO: main: Eval Epoch : batch 260 Loss: 0.022441150734084418]
[2024-04-17 12:32:16,211: INFO: main: Eval Epoch : batch 261 Loss: 0.022676102487404842]
[2024-04-17 12:32:16,413: INFO: main: Eval Epoch : batch 262 Loss: 0.009376839745704902]
[2024-04-17 12:32:16,619: INFO: main: Eval Epoch : batch 263 Loss: 0.01051675225649864]
[2024-04-17 12:32:16,828: INFO: main: Eval Epoch : batch 264 Loss: 0.03784099283396298]
[2024-04-17 12:32:17,035: INFO: main: Eval Epoch : batch 265 Loss: 0.05005855256749025]
[2024-04-17 12:32:17,243: INFO: main: Eval Epoch : batch 266 Loss: 0.01350026058934087]
[2024-04-17 12:32:17,445: INFO: main: Eval Epoch : batch 267 Loss: 0.004763594550248063]
[2024-04-17 12:32:17,648: INFO: main: Eval Epoch : batch 268 Loss: 0.007142019217312874]
[2024-04-17 12:32:17,855: INFO: main: Eval Epoch : batch 269 Loss: 0.024579267088795636]
[2024-04-17 12:32:18,060: INFO: main: Eval Epoch : batch 270 Loss: 0.03846715430772769]
[2024-04-17 12:32:18,267: INFO: main: Eval Epoch : batch 271 Loss: 0.004622454069537136]
[2024-04-17 12:32:18,472: INFO: main: Eval Epoch : batch 272 Loss: 0.01569446140419667]
[2024-04-17 12:32:18,677: INFO: main: Eval Epoch : batch 273 Loss: 0.009550421661903223]
[2024-04-17 12:32:18,885: INFO: main: Eval Epoch : batch 274 Loss: 0.0037696368815816595]
[2024-04-17 12:32:19,090: INFO: main: Eval Epoch : batch 275 Loss: 0.004301350213052093]
[2024-04-17 12:32:19,306: INFO: main: Eval Epoch : batch 276 Loss: 0.011800135162608773]
[2024-04-17 12:32:19,512: INFO: main: Eval Epoch : batch 277 Loss: 0.013504637631131449]
[2024-04-17 12:32:19,718: INFO: main: Eval Epoch : batch 278 Loss: 0.022427158649571448]
[2024-04-17 12:32:19,918: INFO: main: Eval Epoch : batch 279 Loss: 0.005620900312447385]
[2024-04-17 12:32:20,124: INFO: main: Eval Epoch : batch 280 Loss: 0.01170983320826302]
[2024-04-17 12:32:20,324: INFO: main: Eval Epoch : batch 281 Loss: 0.007724438653122468]
[2024-04-17 12:32:20,525: INFO: main: Eval Epoch : batch 282 Loss: 0.00807660588497364]
[2024-04-17 12:32:20,728: INFO: main: Eval Epoch : batch 283 Loss: 0.003780543189936377]
[2024-04-17 12:32:20,927: INFO: main: Eval Epoch : batch 284 Loss: 0.009973983981820039]
[2024-04-17 12:32:21,134: INFO: main: Eval Epoch : batch 285 Loss: 0.003156114292457869]
[2024-04-17 12:32:21,333: INFO: main: Eval Epoch : batch 286 Loss: 0.004442754474875477]
[2024-04-17 12:32:21,535: INFO: main: Eval Epoch : batch 287 Loss: 0.004503921009378482]
[2024-04-17 12:32:21,736: INFO: main: Eval Epoch : batch 288 Loss: 0.004542307329632875]
[2024-04-17 12:32:21,936: INFO: main: Eval Epoch : batch 289 Loss: 0.0061087890587588276]
[2024-04-17 12:32:22,140: INFO: main: Eval Epoch : batch 290 Loss: 0.0188899157413106]
[2024-04-17 12:32:22,342: INFO: main: Eval Epoch : batch 291 Loss: 0.00244083326484216]
[2024-04-17 12:32:22,541: INFO: main: Eval Epoch : batch 292 Loss: 0.008219138573113492]
[2024-04-17 12:32:22,739: INFO: main: Eval Epoch : batch 293 Loss: 0.006439245722336026]
[2024-04-17 12:32:22,936: INFO: main: Eval Epoch : batch 294 Loss: 0.008484828668586148]
[2024-04-17 12:32:23,138: INFO: main: Eval Epoch : batch 295 Loss: 0.007657234895793412]
[2024-04-17 12:32:23,339: INFO: main: Eval Epoch : batch 296 Loss: 0.031577218451260415]
[2024-04-17 12:32:23,538: INFO: main: Eval Epoch : batch 297 Loss: 0.03929849874056303]
[2024-04-17 12:32:23,739: INFO: main: Eval Epoch : batch 298 Loss: 0.015911087098048695]
[2024-04-17 12:32:23,942: INFO: main: Eval Epoch : batch 299 Loss: 0.032691312204732835]
[2024-04-17 12:32:24,147: INFO: main: Eval Epoch : batch 300 Loss: 0.02684481506223364]
[2024-04-17 12:32:24,350: INFO: main: Eval Epoch : batch 301 Loss: 0.002887352000675886]
[2024-04-17 12:32:24,549: INFO: main: Eval Epoch : batch 302 Loss: 0.008369570066412075]
[2024-04-17 12:32:24,748: INFO: main: Eval Epoch : batch 303 Loss: 0.0016607355367605791]
[2024-04-17 12:32:24,950: INFO: main: Eval Epoch : batch 304 Loss: 0.0082360862898075]
[2024-04-17 12:32:25,152: INFO: main: Eval Epoch : batch 305 Loss: 0.011055030226535343]
[2024-04-17 12:32:25,358: INFO: main: Eval Epoch : batch 306 Loss: 0.012509017995829699]
[2024-04-17 12:32:25,555: INFO: main: Eval Epoch : batch 307 Loss: 0.00883343914975402]
[2024-04-17 12:32:25,757: INFO: main: Eval Epoch : batch 308 Loss: 0.012105765154536012]
[2024-04-17 12:32:25,958: INFO: main: Eval Epoch : batch 309 Loss: 0.019289787521904984]
[2024-04-17 12:32:26,161: INFO: main: Eval Epoch : batch 310 Loss: 0.02500319012415164]
[2024-04-17 12:32:26,364: INFO: main: Eval Epoch : batch 311 Loss: 0.010694000383243677]
[2024-04-17 12:32:26,565: INFO: main: Eval Epoch : batch 312 Loss: 0.0066618897422944376]
[2024-04-17 12:32:26,765: INFO: main: Eval Epoch : batch 313 Loss: 0.011556641913490248]
[2024-04-17 12:32:26,965: INFO: main: Eval Epoch : batch 314 Loss: 0.01693230575420351]
[2024-04-17 12:32:27,165: INFO: main: Eval Epoch : batch 315 Loss: 0.008436737085080805]
[2024-04-17 12:32:27,365: INFO: main: Eval Epoch : batch 316 Loss: 0.012456325156080554]
[2024-04-17 12:32:27,566: INFO: main: Eval Epoch : batch 317 Loss: 0.0011426693444587172]
[2024-04-17 12:32:27,766: INFO: main: Eval Epoch : batch 318 Loss: 0.012968725874262428]
[2024-04-17 12:32:27,966: INFO: main: Eval Epoch : batch 319 Loss: 0.005323034286324355]
[2024-04-17 12:32:28,170: INFO: main: Eval Epoch : batch 320 Loss: 0.0006671848765074522]
[2024-04-17 12:32:28,371: INFO: main: Eval Epoch : batch 321 Loss: 0.003726509004156422]
[2024-04-17 12:32:28,574: INFO: main: Eval Epoch : batch 322 Loss: 0.04286285690728686]
[2024-04-17 12:32:28,774: INFO: main: Eval Epoch : batch 323 Loss: 0.014709916198460899]
[2024-04-17 12:32:28,975: INFO: main: Eval Epoch : batch 324 Loss: 0.024760732793753612]
[2024-04-17 12:32:29,176: INFO: main: Eval Epoch : batch 325 Loss: 0.007935032627511131]
[2024-04-17 12:32:29,378: INFO: main: Eval Epoch : batch 326 Loss: 0.017346848676541134]
[2024-04-17 12:32:29,582: INFO: main: Eval Epoch : batch 327 Loss: 0.01337324346181595]
[2024-04-17 12:32:29,783: INFO: main: Eval Epoch : batch 328 Loss: 0.003156517508288886]
[2024-04-17 12:32:29,984: INFO: main: Eval Epoch : batch 329 Loss: 0.011062685376940048]
[2024-04-17 12:32:30,191: INFO: main: Eval Epoch : batch 330 Loss: 0.010337758579956792]
[2024-04-17 12:32:30,396: INFO: main: Eval Epoch : batch 331 Loss: 0.017437759185644934]
[2024-04-17 12:32:30,604: INFO: main: Eval Epoch : batch 332 Loss: 0.0071865657398426834]
[2024-04-17 12:32:30,810: INFO: main: Eval Epoch : batch 333 Loss: 0.01845632817507583]
[2024-04-17 12:32:31,014: INFO: main: Eval Epoch : batch 334 Loss: 0.05494458470457402]
[2024-04-17 12:32:31,224: INFO: main: Eval Epoch : batch 335 Loss: 0.0024794408219379702]
[2024-04-17 12:32:31,430: INFO: main: Eval Epoch : batch 336 Loss: 0.004977020641544689]
[2024-04-17 12:32:31,634: INFO: main: Eval Epoch : batch 337 Loss: 0.037504047475863514]
[2024-04-17 12:32:31,838: INFO: main: Eval Epoch : batch 338 Loss: 0.011384166834540448]
[2024-04-17 12:32:32,045: INFO: main: Eval Epoch : batch 339 Loss: 0.004282137213376992]
[2024-04-17 12:32:32,250: INFO: main: Eval Epoch : batch 340 Loss: 0.01834676059666786]
[2024-04-17 12:32:32,459: INFO: main: Eval Epoch : batch 341 Loss: 0.023043443480414687]
[2024-04-17 12:32:32,666: INFO: main: Eval Epoch : batch 342 Loss: 0.0007675278215396549]
[2024-04-17 12:32:32,876: INFO: main: Eval Epoch : batch 343 Loss: 0.02098576768241922]
[2024-04-17 12:32:33,079: INFO: main: Eval Epoch : batch 344 Loss: 0.028062682839458335]
[2024-04-17 12:32:33,278: INFO: main: Eval Epoch : batch 345 Loss: 0.021615241028541274]
[2024-04-17 12:32:33,480: INFO: main: Eval Epoch : batch 346 Loss: 0.006207187697436066]
[2024-04-17 12:32:33,682: INFO: main: Eval Epoch : batch 347 Loss: 0.022370266558121236]
[2024-04-17 12:32:33,885: INFO: main: Eval Epoch : batch 348 Loss: 0.013557682141132848]
[2024-04-17 12:32:34,083: INFO: main: Eval Epoch : batch 349 Loss: 0.006114073752233892]
[2024-04-17 12:32:34,282: INFO: main: Eval Epoch : batch 350 Loss: 0.00277484685940383]
[2024-04-17 12:32:34,486: INFO: main: Eval Epoch : batch 351 Loss: 0.019070445727950025]
[2024-04-17 12:32:34,690: INFO: main: Eval Epoch : batch 352 Loss: 0.0068420980429551425]
[2024-04-17 12:32:34,896: INFO: main: Eval Epoch : batch 353 Loss: 0.007891837929776366]
[2024-04-17 12:32:35,094: INFO: main: Eval Epoch : batch 354 Loss: 0.016950672365478783]
[2024-04-17 12:32:35,293: INFO: main: Eval Epoch : batch 355 Loss: 0.02750821818222608]
[2024-04-17 12:32:35,499: INFO: main: Eval Epoch : batch 356 Loss: 0.004333767450294795]
[2024-04-17 12:32:35,602: INFO: main: Eval Epoch : batch 357 Loss: 0.006808447253721351]
[2024-04-17 12:32:49,880: INFO: main: The score of the eval model is {'Accuracy': 0.9927505514537783, 'precision': 0.7034180138568129, 'recall': 0.8807726786385588, 'f1': 0.7821676660546218}]
[2024-04-17 12:32:55,501: INFO: main: Epoch: 3/5]
[2024-04-17 12:32:56,168: INFO: main: Training : batch 0 Loss: 0.009637629931894666]
[2024-04-17 12:32:56,787: INFO: main: Training : batch 1 Loss: 0.009254789280703212]
[2024-04-17 12:32:57,416: INFO: main: Training : batch 2 Loss: 0.012395657011978803]
[2024-04-17 12:32:58,044: INFO: main: Training : batch 3 Loss: 0.008113649488065299]
[2024-04-17 12:32:58,669: INFO: main: Training : batch 4 Loss: 0.014919049479773577]
[2024-04-17 12:32:59,294: INFO: main: Training : batch 5 Loss: 0.006177372725839518]
[2024-04-17 12:32:59,924: INFO: main: Training : batch 6 Loss: 0.006478207826198909]
[2024-04-17 12:33:00,545: INFO: main: Training : batch 7 Loss: 0.024539027260542363]
[2024-04-17 12:33:01,164: INFO: main: Training : batch 8 Loss: 0.010465709344195918]
[2024-04-17 12:33:01,788: INFO: main: Training : batch 9 Loss: 0.014402279506386626]
[2024-04-17 12:33:02,413: INFO: main: Training : batch 10 Loss: 0.007153576322231007]
[2024-04-17 12:33:03,036: INFO: main: Training : batch 11 Loss: 0.011302137336598985]
[2024-04-17 12:33:03,657: INFO: main: Training : batch 12 Loss: 0.006350577025704906]
[2024-04-17 12:33:04,289: INFO: main: Training : batch 13 Loss: 0.005975563247219094]
[2024-04-17 12:33:04,913: INFO: main: Training : batch 14 Loss: 0.005146555811624485]
[2024-04-17 12:33:05,544: INFO: main: Training : batch 15 Loss: 0.003055851220506688]
[2024-04-17 12:33:06,173: INFO: main: Training : batch 16 Loss: 0.008444766142006802]
[2024-04-17 12:33:06,803: INFO: main: Training : batch 17 Loss: 0.004560023513179016]
[2024-04-17 12:33:07,439: INFO: main: Training : batch 18 Loss: 0.00709658401068551]
[2024-04-17 12:33:08,067: INFO: main: Training : batch 19 Loss: 0.006269012382890887]
[2024-04-17 12:33:08,700: INFO: main: Training : batch 20 Loss: 0.03725210944116599]
[2024-04-17 12:33:09,330: INFO: main: Training : batch 21 Loss: 0.002074290068575813]
[2024-04-17 12:33:09,965: INFO: main: Training : batch 22 Loss: 0.027128469922179457]
[2024-04-17 12:33:10,616: INFO: main: Training : batch 23 Loss: 0.0027170356911348655]
[2024-04-17 12:33:11,257: INFO: main: Training : batch 24 Loss: 0.01224825913591703]
[2024-04-17 12:33:11,901: INFO: main: Training : batch 25 Loss: 0.01201431776299385]
[2024-04-17 12:33:12,546: INFO: main: Training : batch 26 Loss: 0.0035628165287835147]
[2024-04-17 12:33:13,190: INFO: main: Training : batch 27 Loss: 0.0041870027794829396]
[2024-04-17 12:33:13,837: INFO: main: Training : batch 28 Loss: 0.004149331054715581]
[2024-04-17 12:33:14,485: INFO: main: Training : batch 29 Loss: 0.018852013475489513]
[2024-04-17 12:33:15,132: INFO: main: Training : batch 30 Loss: 0.009205456483098312]
[2024-04-17 12:33:15,778: INFO: main: Training : batch 31 Loss: 0.0044195185686413474]
[2024-04-17 12:33:16,426: INFO: main: Training : batch 32 Loss: 0.01880820904789726]
[2024-04-17 12:33:17,068: INFO: main: Training : batch 33 Loss: 0.004343228673296906]
[2024-04-17 12:33:17,712: INFO: main: Training : batch 34 Loss: 0.021638475745051802]
[2024-04-17 12:33:18,353: INFO: main: Training : batch 35 Loss: 0.007003543497018881]
[2024-04-17 12:33:19,001: INFO: main: Training : batch 36 Loss: 0.014438809189125802]
[2024-04-17 12:33:19,648: INFO: main: Training : batch 37 Loss: 0.010616032965716912]
[2024-04-17 12:33:20,298: INFO: main: Training : batch 38 Loss: 0.0035459824674698385]
[2024-04-17 12:33:20,946: INFO: main: Training : batch 39 Loss: 0.03788211229072954]
[2024-04-17 12:33:21,595: INFO: main: Training : batch 40 Loss: 0.012301802408862618]
[2024-04-17 12:33:22,244: INFO: main: Training : batch 41 Loss: 0.0045206512690018534]
[2024-04-17 12:33:22,892: INFO: main: Training : batch 42 Loss: 0.0077030482674508485]
[2024-04-17 12:33:23,545: INFO: main: Training : batch 43 Loss: 0.0030687056991185323]
[2024-04-17 12:33:24,202: INFO: main: Training : batch 44 Loss: 0.0039719713880006806]
[2024-04-17 12:33:24,862: INFO: main: Training : batch 45 Loss: 0.011135430161736709]
[2024-04-17 12:33:25,521: INFO: main: Training : batch 46 Loss: 0.009605345808387152]
[2024-04-17 12:33:26,183: INFO: main: Training : batch 47 Loss: 0.02573431635811145]
[2024-04-17 12:33:26,847: INFO: main: Training : batch 48 Loss: 0.015281934034626412]
[2024-04-17 12:33:27,509: INFO: main: Training : batch 49 Loss: 0.010842348168691422]
[2024-04-17 12:33:28,162: INFO: main: Training : batch 50 Loss: 0.020978868454835757]
[2024-04-17 12:33:28,817: INFO: main: Training : batch 51 Loss: 0.013626088817867707]
[2024-04-17 12:33:29,476: INFO: main: Training : batch 52 Loss: 0.0045612560128324665]
[2024-04-17 12:33:30,133: INFO: main: Training : batch 53 Loss: 0.0034544916198835872]
[2024-04-17 12:33:30,788: INFO: main: Training : batch 54 Loss: 0.011812301683945023]
[2024-04-17 12:33:31,442: INFO: main: Training : batch 55 Loss: 0.027176355968517536]
[2024-04-17 12:33:32,099: INFO: main: Training : batch 56 Loss: 0.005551988628912494]
[2024-04-17 12:33:32,755: INFO: main: Training : batch 57 Loss: 0.02154111997641867]
[2024-04-17 12:33:33,401: INFO: main: Training : batch 58 Loss: 0.023917967262229008]
[2024-04-17 12:33:34,051: INFO: main: Training : batch 59 Loss: 0.004183586232106488]
[2024-04-17 12:33:34,695: INFO: main: Training : batch 60 Loss: 0.008125328592148168]
[2024-04-17 12:33:35,342: INFO: main: Training : batch 61 Loss: 0.0015645944063103136]
[2024-04-17 12:33:35,990: INFO: main: Training : batch 62 Loss: 0.01182105290741479]
[2024-04-17 12:33:36,637: INFO: main: Training : batch 63 Loss: 0.00403806977948257]
[2024-04-17 12:33:37,283: INFO: main: Training : batch 64 Loss: 0.027765120404718135]
[2024-04-17 12:33:37,931: INFO: main: Training : batch 65 Loss: 0.001521445181311594]
[2024-04-17 12:33:38,579: INFO: main: Training : batch 66 Loss: 0.00292261741117959]
[2024-04-17 12:33:39,223: INFO: main: Training : batch 67 Loss: 0.01008927499903337]
[2024-04-17 12:33:39,867: INFO: main: Training : batch 68 Loss: 0.006736165684615796]
[2024-04-17 12:33:40,506: INFO: main: Training : batch 69 Loss: 0.01427158093963406]
[2024-04-17 12:33:41,148: INFO: main: Training : batch 70 Loss: 0.016708949212631993]
[2024-04-17 12:33:41,791: INFO: main: Training : batch 71 Loss: 0.01978805795843041]
[2024-04-17 12:33:42,430: INFO: main: Training : batch 72 Loss: 0.009947391234770408]
[2024-04-17 12:33:43,069: INFO: main: Training : batch 73 Loss: 0.01715644395303568]
[2024-04-17 12:33:43,706: INFO: main: Training : batch 74 Loss: 0.0065381377706484585]
[2024-04-17 12:33:44,349: INFO: main: Training : batch 75 Loss: 0.007226345704892802]
[2024-04-17 12:33:44,985: INFO: main: Training : batch 76 Loss: 0.0077136343895679105]
[2024-04-17 12:33:45,622: INFO: main: Training : batch 77 Loss: 0.014660804201324805]
[2024-04-17 12:33:46,256: INFO: main: Training : batch 78 Loss: 0.014862431922788792]
[2024-04-17 12:33:46,888: INFO: main: Training : batch 79 Loss: 0.00730817771875424]
[2024-04-17 12:33:47,521: INFO: main: Training : batch 80 Loss: 0.017132810672783563]
[2024-04-17 12:33:48,152: INFO: main: Training : batch 81 Loss: 0.007358902998301937]
[2024-04-17 12:33:48,788: INFO: main: Training : batch 82 Loss: 0.014657771035175364]
[2024-04-17 12:33:49,422: INFO: main: Training : batch 83 Loss: 0.003851517946455348]
[2024-04-17 12:33:50,053: INFO: main: Training : batch 84 Loss: 0.014436439892357149]
[2024-04-17 12:33:50,690: INFO: main: Training : batch 85 Loss: 0.026573112399026405]
[2024-04-17 12:33:51,331: INFO: main: Training : batch 86 Loss: 0.008493624990925873]
[2024-04-17 12:33:51,965: INFO: main: Training : batch 87 Loss: 0.011034597585812298]
[2024-04-17 12:33:52,606: INFO: main: Training : batch 88 Loss: 0.011497189685845073]
[2024-04-17 12:33:53,240: INFO: main: Training : batch 89 Loss: 0.003471318510183225]
[2024-04-17 12:33:53,866: INFO: main: Training : batch 90 Loss: 0.0036018317808952883]
[2024-04-17 12:33:54,492: INFO: main: Training : batch 91 Loss: 0.005119180808341961]
[2024-04-17 12:33:55,121: INFO: main: Training : batch 92 Loss: 0.0032685658015811953]
[2024-04-17 12:33:55,747: INFO: main: Training : batch 93 Loss: 0.015558764722618157]
[2024-04-17 12:33:56,374: INFO: main: Training : batch 94 Loss: 0.004713286361570606]
[2024-04-17 12:33:56,997: INFO: main: Training : batch 95 Loss: 0.0034885807853580714]
[2024-04-17 12:33:57,625: INFO: main: Training : batch 96 Loss: 0.016392209194888555]
[2024-04-17 12:33:58,247: INFO: main: Training : batch 97 Loss: 0.0020307726334149482]
[2024-04-17 12:33:58,875: INFO: main: Training : batch 98 Loss: 0.014779125233141543]
[2024-04-17 12:33:59,501: INFO: main: Training : batch 99 Loss: 0.019895237349980433]
[2024-04-17 12:34:00,124: INFO: main: Training : batch 100 Loss: 0.002210390827250567]
[2024-04-17 12:34:00,751: INFO: main: Training : batch 101 Loss: 0.0038472241039458027]
[2024-04-17 12:34:01,370: INFO: main: Training : batch 102 Loss: 0.03610523766809916]
[2024-04-17 12:34:01,991: INFO: main: Training : batch 103 Loss: 0.0033473057255370506]
[2024-04-17 12:34:02,619: INFO: main: Training : batch 104 Loss: 0.002110601628170509]
[2024-04-17 12:34:03,239: INFO: main: Training : batch 105 Loss: 0.0057465176600716055]
[2024-04-17 12:34:03,873: INFO: main: Training : batch 106 Loss: 0.01364637982848342]
[2024-04-17 12:34:04,508: INFO: main: Training : batch 107 Loss: 0.008029024468918795]
[2024-04-17 12:34:05,131: INFO: main: Training : batch 108 Loss: 0.011466140013539454]
[2024-04-17 12:34:05,760: INFO: main: Training : batch 109 Loss: 0.009670913160328289]
[2024-04-17 12:34:06,387: INFO: main: Training : batch 110 Loss: 0.0019217900180708918]
[2024-04-17 12:34:07,015: INFO: main: Training : batch 111 Loss: 0.01123576237333241]
[2024-04-17 12:34:07,643: INFO: main: Training : batch 112 Loss: 0.017964037221035165]
[2024-04-17 12:34:08,268: INFO: main: Training : batch 113 Loss: 0.022050562548046642]
[2024-04-17 12:34:08,887: INFO: main: Training : batch 114 Loss: 0.0002309698432719693]
[2024-04-17 12:34:09,511: INFO: main: Training : batch 115 Loss: 0.0148340698467847]
[2024-04-17 12:34:10,133: INFO: main: Training : batch 116 Loss: 0.0012515164597746078]
[2024-04-17 12:34:10,754: INFO: main: Training : batch 117 Loss: 0.055977602427937534]
[2024-04-17 12:34:11,376: INFO: main: Training : batch 118 Loss: 0.004668198174300763]
[2024-04-17 12:34:11,998: INFO: main: Training : batch 119 Loss: 0.0022223024360195015]
[2024-04-17 12:34:12,620: INFO: main: Training : batch 120 Loss: 0.004002151871121573]
[2024-04-17 12:34:13,243: INFO: main: Training : batch 121 Loss: 0.012584041469507002]
[2024-04-17 12:34:13,861: INFO: main: Training : batch 122 Loss: 0.01033188003095233]
[2024-04-17 12:34:14,482: INFO: main: Training : batch 123 Loss: 0.015119556359252107]
[2024-04-17 12:34:15,107: INFO: main: Training : batch 124 Loss: 0.005652910241820021]
[2024-04-17 12:34:15,731: INFO: main: Training : batch 125 Loss: 0.01546321376640602]
[2024-04-17 12:34:16,365: INFO: main: Training : batch 126 Loss: 0.006875143888909848]
[2024-04-17 12:34:16,992: INFO: main: Training : batch 127 Loss: 0.010536967772060197]
[2024-04-17 12:34:17,620: INFO: main: Training : batch 128 Loss: 0.00848733949016968]
[2024-04-17 12:34:18,251: INFO: main: Training : batch 129 Loss: 0.007578564135078748]
[2024-04-17 12:34:18,878: INFO: main: Training : batch 130 Loss: 0.0014788740596475623]
[2024-04-17 12:34:19,508: INFO: main: Training : batch 131 Loss: 0.021749988423823037]
[2024-04-17 12:34:20,140: INFO: main: Training : batch 132 Loss: 0.012573730739637334]
[2024-04-17 12:34:20,763: INFO: main: Training : batch 133 Loss: 0.01256269697773568]
[2024-04-17 12:34:21,384: INFO: main: Training : batch 134 Loss: 0.009451569292869567]
[2024-04-17 12:34:22,008: INFO: main: Training : batch 135 Loss: 0.01216974386844848]
[2024-04-17 12:34:22,633: INFO: main: Training : batch 136 Loss: 0.011779637801722514]
[2024-04-17 12:34:23,255: INFO: main: Training : batch 137 Loss: 0.0581855101317506]
[2024-04-17 12:34:23,880: INFO: main: Training : batch 138 Loss: 0.013573786032288588]
[2024-04-17 12:34:24,507: INFO: main: Training : batch 139 Loss: 0.007385315783035879]
[2024-04-17 12:34:25,133: INFO: main: Training : batch 140 Loss: 0.0038351783413017536]
[2024-04-17 12:34:25,760: INFO: main: Training : batch 141 Loss: 0.004547037776466769]
[2024-04-17 12:34:26,384: INFO: main: Training : batch 142 Loss: 0.0048819984403501325]
[2024-04-17 12:34:27,011: INFO: main: Training : batch 143 Loss: 0.004409579693557249]
[2024-04-17 12:34:27,636: INFO: main: Training : batch 144 Loss: 0.007983957054812493]
[2024-04-17 12:34:28,261: INFO: main: Training : batch 145 Loss: 0.011189486504848705]
[2024-04-17 12:34:28,889: INFO: main: Training : batch 146 Loss: 0.007680640567474088]
[2024-04-17 12:34:29,515: INFO: main: Training : batch 147 Loss: 0.00572537820536169]
[2024-04-17 12:34:30,143: INFO: main: Training : batch 148 Loss: 0.0106724815833968]
[2024-04-17 12:34:30,777: INFO: main: Training : batch 149 Loss: 0.006773466202581758]
[2024-04-17 12:34:31,409: INFO: main: Training : batch 150 Loss: 0.010610902418793831]
[2024-04-17 12:34:32,047: INFO: main: Training : batch 151 Loss: 0.012432855416128909]
[2024-04-17 12:34:32,684: INFO: main: Training : batch 152 Loss: 0.01479276877325619]
[2024-04-17 12:34:33,321: INFO: main: Training : batch 153 Loss: 0.03163441322996819]
[2024-04-17 12:34:33,950: INFO: main: Training : batch 154 Loss: 0.013290882277167973]
[2024-04-17 12:34:34,576: INFO: main: Training : batch 155 Loss: 0.051742123661887754]
[2024-04-17 12:34:35,209: INFO: main: Training : batch 156 Loss: 0.0006946528161312319]
[2024-04-17 12:34:35,837: INFO: main: Training : batch 157 Loss: 0.004341422993079037]
[2024-04-17 12:34:36,467: INFO: main: Training : batch 158 Loss: 0.007793117773942007]
[2024-04-17 12:34:37,096: INFO: main: Training : batch 159 Loss: 0.007169326516472102]
[2024-04-17 12:34:37,728: INFO: main: Training : batch 160 Loss: 0.0077373067767947815]
[2024-04-17 12:34:38,358: INFO: main: Training : batch 161 Loss: 0.012126058699258528]
[2024-04-17 12:34:38,988: INFO: main: Training : batch 162 Loss: 0.0014671535981437426]
[2024-04-17 12:34:39,619: INFO: main: Training : batch 163 Loss: 0.012341692280596514]
[2024-04-17 12:34:40,249: INFO: main: Training : batch 164 Loss: 0.0023774039512543983]
[2024-04-17 12:34:40,879: INFO: main: Training : batch 165 Loss: 0.008351389888871949]
[2024-04-17 12:34:41,510: INFO: main: Training : batch 166 Loss: 0.016363963846467402]
[2024-04-17 12:34:42,143: INFO: main: Training : batch 167 Loss: 0.006963419536924633]
[2024-04-17 12:34:42,772: INFO: main: Training : batch 168 Loss: 0.0025383118069191757]
[2024-04-17 12:34:43,405: INFO: main: Training : batch 169 Loss: 0.0021686070105135253]
[2024-04-17 12:34:44,047: INFO: main: Training : batch 170 Loss: 0.004737601498925623]
[2024-04-17 12:34:44,689: INFO: main: Training : batch 171 Loss: 0.02883938436154057]
[2024-04-17 12:34:45,331: INFO: main: Training : batch 172 Loss: 0.002188370648287958]
[2024-04-17 12:34:45,976: INFO: main: Training : batch 173 Loss: 0.003497839979440397]
[2024-04-17 12:34:46,625: INFO: main: Training : batch 174 Loss: 0.002025938003010527]
[2024-04-17 12:34:47,259: INFO: main: Training : batch 175 Loss: 0.0037695194648504037]
[2024-04-17 12:34:47,892: INFO: main: Training : batch 176 Loss: 0.002466266478956775]
[2024-04-17 12:34:48,527: INFO: main: Training : batch 177 Loss: 0.01564583918433827]
[2024-04-17 12:34:49,165: INFO: main: Training : batch 178 Loss: 0.017186930387802585]
[2024-04-17 12:34:49,801: INFO: main: Training : batch 179 Loss: 0.017072266858925902]
[2024-04-17 12:34:50,449: INFO: main: Training : batch 180 Loss: 0.01812330354627641]
[2024-04-17 12:34:51,085: INFO: main: Training : batch 181 Loss: 0.01444458611318141]
[2024-04-17 12:34:51,722: INFO: main: Training : batch 182 Loss: 0.018279225762716355]
[2024-04-17 12:34:52,361: INFO: main: Training : batch 183 Loss: 0.016156139974157113]
[2024-04-17 12:34:53,003: INFO: main: Training : batch 184 Loss: 0.018590718297513485]
[2024-04-17 12:34:53,637: INFO: main: Training : batch 185 Loss: 0.018092720286981202]
[2024-04-17 12:34:54,278: INFO: main: Training : batch 186 Loss: 0.0031713780799662653]
[2024-04-17 12:34:54,916: INFO: main: Training : batch 187 Loss: 0.02335314319125956]
[2024-04-17 12:34:55,565: INFO: main: Training : batch 188 Loss: 0.009357290312487235]
[2024-04-17 12:34:56,208: INFO: main: Training : batch 189 Loss: 0.011020758031613247]
[2024-04-17 12:34:56,846: INFO: main: Training : batch 190 Loss: 0.004240645099209541]
[2024-04-17 12:34:57,492: INFO: main: Training : batch 191 Loss: 0.017487634116233833]
[2024-04-17 12:34:58,142: INFO: main: Training : batch 192 Loss: 0.005182321442435839]
[2024-04-17 12:34:58,789: INFO: main: Training : batch 193 Loss: 0.003128557621392431]
[2024-04-17 12:34:59,435: INFO: main: Training : batch 194 Loss: 0.0030163342562268105]
[2024-04-17 12:35:00,085: INFO: main: Training : batch 195 Loss: 0.0066203691671991695]
[2024-04-17 12:35:00,728: INFO: main: Training : batch 196 Loss: 0.02109509211936669]
[2024-04-17 12:35:01,363: INFO: main: Training : batch 197 Loss: 0.001855930332914214]
[2024-04-17 12:35:02,005: INFO: main: Training : batch 198 Loss: 0.009697132822946665]
[2024-04-17 12:35:02,644: INFO: main: Training : batch 199 Loss: 0.0026963440568359637]
[2024-04-17 12:35:03,284: INFO: main: Training : batch 200 Loss: 0.00473743759454414]
[2024-04-17 12:35:03,925: INFO: main: Training : batch 201 Loss: 0.019382218575758905]
[2024-04-17 12:35:04,563: INFO: main: Training : batch 202 Loss: 0.0029931821953930334]
[2024-04-17 12:35:05,206: INFO: main: Training : batch 203 Loss: 0.0045082029347441365]
[2024-04-17 12:35:05,845: INFO: main: Training : batch 204 Loss: 0.01140424398828667]
[2024-04-17 12:35:06,480: INFO: main: Training : batch 205 Loss: 0.010706227210679825]
[2024-04-17 12:35:07,116: INFO: main: Training : batch 206 Loss: 0.007974101894135614]
[2024-04-17 12:35:07,755: INFO: main: Training : batch 207 Loss: 0.008654710244354382]
[2024-04-17 12:35:08,394: INFO: main: Training : batch 208 Loss: 0.004339642181315157]
[2024-04-17 12:35:09,029: INFO: main: Training : batch 209 Loss: 0.03640678408257037]
[2024-04-17 12:35:09,668: INFO: main: Training : batch 210 Loss: 0.028015797220183193]
[2024-04-17 12:35:10,303: INFO: main: Training : batch 211 Loss: 0.002062125653089496]
[2024-04-17 12:35:10,942: INFO: main: Training : batch 212 Loss: 0.006176590672590482]
[2024-04-17 12:35:11,591: INFO: main: Training : batch 213 Loss: 0.0004112825780959221]
[2024-04-17 12:35:12,236: INFO: main: Training : batch 214 Loss: 0.016458333900359223]
[2024-04-17 12:35:12,883: INFO: main: Training : batch 215 Loss: 0.0011390234886356231]
[2024-04-17 12:35:13,533: INFO: main: Training : batch 216 Loss: 0.0053777333822301165]
[2024-04-17 12:35:14,180: INFO: main: Training : batch 217 Loss: 0.011754568748000046]
[2024-04-17 12:35:14,818: INFO: main: Training : batch 218 Loss: 0.0114797756812099]
[2024-04-17 12:35:15,453: INFO: main: Training : batch 219 Loss: 0.00494141823961904]
[2024-04-17 12:35:16,089: INFO: main: Training : batch 220 Loss: 0.003575653359094249]
[2024-04-17 12:35:16,723: INFO: main: Training : batch 221 Loss: 0.021710731417825212]
[2024-04-17 12:35:17,362: INFO: main: Training : batch 222 Loss: 0.02145152683629587]
[2024-04-17 12:35:17,996: INFO: main: Training : batch 223 Loss: 0.006194187205975684]
[2024-04-17 12:35:18,631: INFO: main: Training : batch 224 Loss: 0.011267516150252973]
[2024-04-17 12:35:19,262: INFO: main: Training : batch 225 Loss: 0.0028689149379030176]
[2024-04-17 12:35:19,895: INFO: main: Training : batch 226 Loss: 0.016198657052361072]
[2024-04-17 12:35:20,530: INFO: main: Training : batch 227 Loss: 0.013095192413032919]
[2024-04-17 12:35:21,163: INFO: main: Training : batch 228 Loss: 0.015395327590600616]
[2024-04-17 12:35:21,796: INFO: main: Training : batch 229 Loss: 0.020989118410424504]
[2024-04-17 12:35:22,423: INFO: main: Training : batch 230 Loss: 0.004815605243604401]
[2024-04-17 12:35:23,054: INFO: main: Training : batch 231 Loss: 0.0017189089485278551]
[2024-04-17 12:35:23,687: INFO: main: Training : batch 232 Loss: 0.013699122591653888]
[2024-04-17 12:35:24,331: INFO: main: Training : batch 233 Loss: 0.02424788886219127]
[2024-04-17 12:35:24,980: INFO: main: Training : batch 234 Loss: 0.004957681337892755]
[2024-04-17 12:35:25,618: INFO: main: Training : batch 235 Loss: 0.01902811865496]
[2024-04-17 12:35:26,256: INFO: main: Training : batch 236 Loss: 0.013162086649481802]
[2024-04-17 12:35:26,891: INFO: main: Training : batch 237 Loss: 0.0065515208779273765]
[2024-04-17 12:35:27,523: INFO: main: Training : batch 238 Loss: 0.0016110570618156556]
[2024-04-17 12:35:28,148: INFO: main: Training : batch 239 Loss: 0.017925099330736213]
[2024-04-17 12:35:28,779: INFO: main: Training : batch 240 Loss: 0.008300009655969259]
[2024-04-17 12:35:29,411: INFO: main: Training : batch 241 Loss: 0.00376825469539492]
[2024-04-17 12:35:30,042: INFO: main: Training : batch 242 Loss: 0.005704145384403613]
[2024-04-17 12:35:30,669: INFO: main: Training : batch 243 Loss: 0.015464883521343948]
[2024-04-17 12:35:31,304: INFO: main: Training : batch 244 Loss: 0.012390708944294369]
[2024-04-17 12:35:31,935: INFO: main: Training : batch 245 Loss: 0.006855502947102224]
[2024-04-17 12:35:32,566: INFO: main: Training : batch 246 Loss: 0.004598760128382806]
[2024-04-17 12:35:33,199: INFO: main: Training : batch 247 Loss: 0.006799545717050874]
[2024-04-17 12:35:33,826: INFO: main: Training : batch 248 Loss: 0.007222945071788097]
[2024-04-17 12:35:34,461: INFO: main: Training : batch 249 Loss: 0.01524466634929132]
[2024-04-17 12:35:35,092: INFO: main: Training : batch 250 Loss: 0.020377572761865432]
[2024-04-17 12:35:35,721: INFO: main: Training : batch 251 Loss: 0.01685681588643626]
[2024-04-17 12:35:36,349: INFO: main: Training : batch 252 Loss: 0.010270225317541157]
[2024-04-17 12:35:36,980: INFO: main: Training : batch 253 Loss: 0.005005619721680158]
[2024-04-17 12:35:37,614: INFO: main: Training : batch 254 Loss: 0.009390418372984377]
[2024-04-17 12:35:38,250: INFO: main: Training : batch 255 Loss: 0.012437741053373412]
[2024-04-17 12:35:38,885: INFO: main: Training : batch 256 Loss: 0.03603325922399465]
[2024-04-17 12:35:39,520: INFO: main: Training : batch 257 Loss: 0.009199210362034294]
[2024-04-17 12:35:40,160: INFO: main: Training : batch 258 Loss: 0.0019785951912573564]
[2024-04-17 12:35:40,788: INFO: main: Training : batch 259 Loss: 0.01700311318847102]
[2024-04-17 12:35:41,419: INFO: main: Training : batch 260 Loss: 0.007995331071032598]
[2024-04-17 12:35:42,063: INFO: main: Training : batch 261 Loss: 0.008177179314050088]
[2024-04-17 12:35:42,693: INFO: main: Training : batch 262 Loss: 0.008101906567772526]
[2024-04-17 12:35:43,323: INFO: main: Training : batch 263 Loss: 0.018778476966827124]
[2024-04-17 12:35:43,946: INFO: main: Training : batch 264 Loss: 0.017822879215952415]
[2024-04-17 12:35:44,574: INFO: main: Training : batch 265 Loss: 0.015285946989230394]
[2024-04-17 12:35:45,203: INFO: main: Training : batch 266 Loss: 0.005529239177780857]
[2024-04-17 12:35:45,830: INFO: main: Training : batch 267 Loss: 0.004888716246943267]
[2024-04-17 12:35:46,458: INFO: main: Training : batch 268 Loss: 0.0039812312150356195]
[2024-04-17 12:35:47,086: INFO: main: Training : batch 269 Loss: 0.018119264059797857]
[2024-04-17 12:35:47,715: INFO: main: Training : batch 270 Loss: 0.015312198949391564]
[2024-04-17 12:35:48,345: INFO: main: Training : batch 271 Loss: 0.022101950963366267]
[2024-04-17 12:35:48,974: INFO: main: Training : batch 272 Loss: 0.0013929014105716615]
[2024-04-17 12:35:49,600: INFO: main: Training : batch 273 Loss: 0.006068217372751385]
[2024-04-17 12:35:50,225: INFO: main: Training : batch 274 Loss: 0.0034187195466993093]
[2024-04-17 12:35:50,864: INFO: main: Training : batch 275 Loss: 0.008658920023276995]
[2024-04-17 12:35:51,500: INFO: main: Training : batch 276 Loss: 0.012335175567937427]
[2024-04-17 12:35:52,132: INFO: main: Training : batch 277 Loss: 0.007704410336485353]
[2024-04-17 12:35:52,771: INFO: main: Training : batch 278 Loss: 0.00409393582901015]
[2024-04-17 12:35:53,412: INFO: main: Training : batch 279 Loss: 0.007995706979711852]
[2024-04-17 12:35:54,042: INFO: main: Training : batch 280 Loss: 0.003821540921189918]
[2024-04-17 12:35:54,675: INFO: main: Training : batch 281 Loss: 0.008834711428568889]
[2024-04-17 12:35:55,301: INFO: main: Training : batch 282 Loss: 0.003232474893923276]
[2024-04-17 12:35:55,925: INFO: main: Training : batch 283 Loss: 0.00766464732256906]
[2024-04-17 12:35:56,555: INFO: main: Training : batch 284 Loss: 0.010610898586645549]
[2024-04-17 12:35:57,186: INFO: main: Training : batch 285 Loss: 0.019523004621356792]
[2024-04-17 12:35:57,813: INFO: main: Training : batch 286 Loss: 0.0009309308620316569]
[2024-04-17 12:35:58,443: INFO: main: Training : batch 287 Loss: 0.015612937726074593]
[2024-04-17 12:35:59,069: INFO: main: Training : batch 288 Loss: 0.0004059179051348277]
[2024-04-17 12:35:59,698: INFO: main: Training : batch 289 Loss: 0.006232488969196727]
[2024-04-17 12:36:00,323: INFO: main: Training : batch 290 Loss: 0.004969148949792454]
[2024-04-17 12:36:00,952: INFO: main: Training : batch 291 Loss: 0.004303931794080181]
[2024-04-17 12:36:01,585: INFO: main: Training : batch 292 Loss: 0.025232751231320138]
[2024-04-17 12:36:02,217: INFO: main: Training : batch 293 Loss: 0.004043883912286735]
[2024-04-17 12:36:02,845: INFO: main: Training : batch 294 Loss: 0.004159174365911214]
[2024-04-17 12:36:03,472: INFO: main: Training : batch 295 Loss: 0.010119110534971661]
[2024-04-17 12:36:04,117: INFO: main: Training : batch 296 Loss: 0.005187192056173799]
[2024-04-17 12:36:04,753: INFO: main: Training : batch 297 Loss: 0.0023652847998739362]
[2024-04-17 12:36:05,392: INFO: main: Training : batch 298 Loss: 0.020700367888625244]
[2024-04-17 12:36:06,027: INFO: main: Training : batch 299 Loss: 0.004276948985762924]
[2024-04-17 12:36:06,663: INFO: main: Training : batch 300 Loss: 0.007179535030759495]
[2024-04-17 12:36:07,296: INFO: main: Training : batch 301 Loss: 0.01784344210639101]
[2024-04-17 12:36:07,930: INFO: main: Training : batch 302 Loss: 0.023547335381328763]
[2024-04-17 12:36:08,562: INFO: main: Training : batch 303 Loss: 0.001083534616226985]
[2024-04-17 12:36:09,197: INFO: main: Training : batch 304 Loss: 0.00907718714350228]
[2024-04-17 12:36:09,830: INFO: main: Training : batch 305 Loss: 0.0008737561218502702]
[2024-04-17 12:36:10,459: INFO: main: Training : batch 306 Loss: 0.005107341631281521]
[2024-04-17 12:36:11,090: INFO: main: Training : batch 307 Loss: 0.011730342853161898]
[2024-04-17 12:36:11,720: INFO: main: Training : batch 308 Loss: 0.016290259082687483]
[2024-04-17 12:36:12,347: INFO: main: Training : batch 309 Loss: 0.007096437368556144]
[2024-04-17 12:36:12,974: INFO: main: Training : batch 310 Loss: 0.0020820143318091627]
[2024-04-17 12:36:13,604: INFO: main: Training : batch 311 Loss: 0.0070440052387574364]
[2024-04-17 12:36:14,236: INFO: main: Training : batch 312 Loss: 0.005694102613633689]
[2024-04-17 12:36:14,866: INFO: main: Training : batch 313 Loss: 0.0042679724690117314]
[2024-04-17 12:36:15,495: INFO: main: Training : batch 314 Loss: 0.006430079473771635]
[2024-04-17 12:36:16,125: INFO: main: Training : batch 315 Loss: 0.0020169334420408985]
[2024-04-17 12:36:16,759: INFO: main: Training : batch 316 Loss: 0.007382661521409625]
[2024-04-17 12:36:17,396: INFO: main: Training : batch 317 Loss: 0.009975619238669799]
[2024-04-17 12:36:18,031: INFO: main: Training : batch 318 Loss: 0.009595522677288627]
[2024-04-17 12:36:18,678: INFO: main: Training : batch 319 Loss: 0.0031946073667767213]
[2024-04-17 12:36:19,315: INFO: main: Training : batch 320 Loss: 0.014924575782985188]
[2024-04-17 12:36:19,953: INFO: main: Training : batch 321 Loss: 0.00416143013121233]
[2024-04-17 12:36:20,587: INFO: main: Training : batch 322 Loss: 0.0038860528805026795]
[2024-04-17 12:36:21,220: INFO: main: Training : batch 323 Loss: 0.006532366474203451]
[2024-04-17 12:36:21,851: INFO: main: Training : batch 324 Loss: 0.01855835884047949]
[2024-04-17 12:36:22,484: INFO: main: Training : batch 325 Loss: 0.0009258905794639827]
[2024-04-17 12:36:23,117: INFO: main: Training : batch 326 Loss: 0.016052868758735243]
[2024-04-17 12:36:23,744: INFO: main: Training : batch 327 Loss: 0.0202390358158971]
[2024-04-17 12:36:24,387: INFO: main: Training : batch 328 Loss: 0.0027378897756657004]
[2024-04-17 12:36:25,021: INFO: main: Training : batch 329 Loss: 0.007281182730257252]
[2024-04-17 12:36:25,652: INFO: main: Training : batch 330 Loss: 0.008395155134787518]
[2024-04-17 12:36:26,282: INFO: main: Training : batch 331 Loss: 0.015520423608043035]
[2024-04-17 12:36:26,915: INFO: main: Training : batch 332 Loss: 0.012263958245755656]
[2024-04-17 12:36:27,546: INFO: main: Training : batch 333 Loss: 0.010902399115073336]
[2024-04-17 12:36:28,180: INFO: main: Training : batch 334 Loss: 0.007716326219174915]
[2024-04-17 12:36:28,817: INFO: main: Training : batch 335 Loss: 0.008615117532727824]
[2024-04-17 12:36:29,448: INFO: main: Training : batch 336 Loss: 0.013330690932838913]
[2024-04-17 12:36:30,085: INFO: main: Training : batch 337 Loss: 0.01169970552006994]
[2024-04-17 12:36:30,723: INFO: main: Training : batch 338 Loss: 0.0034112609581879384]
[2024-04-17 12:36:31,358: INFO: main: Training : batch 339 Loss: 0.01388636736422954]
[2024-04-17 12:36:32,005: INFO: main: Training : batch 340 Loss: 0.006102129565191981]
[2024-04-17 12:36:32,646: INFO: main: Training : batch 341 Loss: 0.016266706230380297]
[2024-04-17 12:36:33,288: INFO: main: Training : batch 342 Loss: 0.00675648891362376]
[2024-04-17 12:36:33,929: INFO: main: Training : batch 343 Loss: 0.008676959741103536]
[2024-04-17 12:36:34,560: INFO: main: Training : batch 344 Loss: 0.005106402775600121]
[2024-04-17 12:36:35,191: INFO: main: Training : batch 345 Loss: 0.006726713811119222]
[2024-04-17 12:36:35,825: INFO: main: Training : batch 346 Loss: 0.013458655676321766]
[2024-04-17 12:36:36,452: INFO: main: Training : batch 347 Loss: 0.005823565875337471]
[2024-04-17 12:36:37,087: INFO: main: Training : batch 348 Loss: 0.0020252897390394256]
[2024-04-17 12:36:37,719: INFO: main: Training : batch 349 Loss: 0.0037782278678299783]
[2024-04-17 12:36:38,355: INFO: main: Training : batch 350 Loss: 0.0018667679393036083]
[2024-04-17 12:36:38,985: INFO: main: Training : batch 351 Loss: 0.011900446868935976]
[2024-04-17 12:36:39,619: INFO: main: Training : batch 352 Loss: 0.012538233568396558]
[2024-04-17 12:36:40,251: INFO: main: Training : batch 353 Loss: 0.011171410929082306]
[2024-04-17 12:36:40,888: INFO: main: Training : batch 354 Loss: 0.007559054504116362]
[2024-04-17 12:36:41,523: INFO: main: Training : batch 355 Loss: 0.0013469903490618547]
[2024-04-17 12:36:42,156: INFO: main: Training : batch 356 Loss: 0.010328493966670764]
[2024-04-17 12:36:42,791: INFO: main: Training : batch 357 Loss: 0.014929704831172515]
[2024-04-17 12:36:43,421: INFO: main: Training : batch 358 Loss: 0.004828383349636044]
[2024-04-17 12:36:44,062: INFO: main: Training : batch 359 Loss: 0.005197336910153657]
[2024-04-17 12:36:44,702: INFO: main: Training : batch 360 Loss: 0.008325793897355082]
[2024-04-17 12:36:45,340: INFO: main: Training : batch 361 Loss: 0.004015859909659338]
[2024-04-17 12:36:45,975: INFO: main: Training : batch 362 Loss: 0.005242460789695696]
[2024-04-17 12:36:46,615: INFO: main: Training : batch 363 Loss: 0.0010196657751228892]
[2024-04-17 12:36:47,258: INFO: main: Training : batch 364 Loss: 0.010648976288430147]
[2024-04-17 12:36:47,893: INFO: main: Training : batch 365 Loss: 0.02653587726814639]
[2024-04-17 12:36:48,529: INFO: main: Training : batch 366 Loss: 0.0081026189254396]
[2024-04-17 12:36:49,160: INFO: main: Training : batch 367 Loss: 0.013390431742415516]
[2024-04-17 12:36:49,795: INFO: main: Training : batch 368 Loss: 0.007488826101186247]
[2024-04-17 12:36:50,432: INFO: main: Training : batch 369 Loss: 0.007377441746199749]
[2024-04-17 12:36:51,066: INFO: main: Training : batch 370 Loss: 0.002246597921157312]
[2024-04-17 12:36:51,700: INFO: main: Training : batch 371 Loss: 0.005784715945178894]
[2024-04-17 12:36:52,334: INFO: main: Training : batch 372 Loss: 0.047637412249301715]
[2024-04-17 12:36:52,967: INFO: main: Training : batch 373 Loss: 0.0047249784357111845]
[2024-04-17 12:36:53,597: INFO: main: Training : batch 374 Loss: 0.0037640406743907747]
[2024-04-17 12:36:54,226: INFO: main: Training : batch 375 Loss: 0.00752027844797455]
[2024-04-17 12:36:54,859: INFO: main: Training : batch 376 Loss: 0.025064313807043447]
[2024-04-17 12:36:55,492: INFO: main: Training : batch 377 Loss: 0.009900792647980576]
[2024-04-17 12:36:56,125: INFO: main: Training : batch 378 Loss: 0.013687795786207444]
[2024-04-17 12:36:56,758: INFO: main: Training : batch 379 Loss: 0.0005738578149620538]
[2024-04-17 12:36:57,395: INFO: main: Training : batch 380 Loss: 0.013669500579332366]
[2024-04-17 12:36:58,041: INFO: main: Training : batch 381 Loss: 0.018772031846120214]
[2024-04-17 12:36:58,679: INFO: main: Training : batch 382 Loss: 0.010251348750624922]
[2024-04-17 12:36:59,317: INFO: main: Training : batch 383 Loss: 0.002072214273841401]
[2024-04-17 12:36:59,959: INFO: main: Training : batch 384 Loss: 0.013369100530481948]
[2024-04-17 12:37:00,603: INFO: main: Training : batch 385 Loss: 0.0034182126648137916]
[2024-04-17 12:37:01,238: INFO: main: Training : batch 386 Loss: 0.012408685991320504]
[2024-04-17 12:37:01,872: INFO: main: Training : batch 387 Loss: 0.008119013441355948]
[2024-04-17 12:37:02,500: INFO: main: Training : batch 388 Loss: 0.0012571933963544245]
[2024-04-17 12:37:03,132: INFO: main: Training : batch 389 Loss: 0.008130438391202213]
[2024-04-17 12:37:03,767: INFO: main: Training : batch 390 Loss: 0.009604629085164029]
[2024-04-17 12:37:04,400: INFO: main: Training : batch 391 Loss: 0.009335283628932472]
[2024-04-17 12:37:05,029: INFO: main: Training : batch 392 Loss: 0.009251990592861347]
[2024-04-17 12:37:05,660: INFO: main: Training : batch 393 Loss: 0.004472239023247192]
[2024-04-17 12:37:06,291: INFO: main: Training : batch 394 Loss: 0.012284836044210184]
[2024-04-17 12:37:06,925: INFO: main: Training : batch 395 Loss: 0.013068512755997018]
[2024-04-17 12:37:07,556: INFO: main: Training : batch 396 Loss: 0.007718006329897108]
[2024-04-17 12:37:08,189: INFO: main: Training : batch 397 Loss: 0.003108724754177049]
[2024-04-17 12:37:08,822: INFO: main: Training : batch 398 Loss: 0.0031899962411210264]
[2024-04-17 12:37:09,447: INFO: main: Training : batch 399 Loss: 0.005861597429852482]
[2024-04-17 12:37:10,073: INFO: main: Training : batch 400 Loss: 0.012203513721590988]
[2024-04-17 12:37:10,701: INFO: main: Training : batch 401 Loss: 0.004482861158134357]
[2024-04-17 12:37:11,340: INFO: main: Training : batch 402 Loss: 0.006016662812302107]
[2024-04-17 12:37:11,984: INFO: main: Training : batch 403 Loss: 0.009550728662313891]
[2024-04-17 12:37:12,619: INFO: main: Training : batch 404 Loss: 0.003951485392412278]
[2024-04-17 12:37:13,253: INFO: main: Training : batch 405 Loss: 0.003993067355844648]
[2024-04-17 12:37:13,892: INFO: main: Training : batch 406 Loss: 0.0017481681971049541]
[2024-04-17 12:37:14,524: INFO: main: Training : batch 407 Loss: 0.01076229930144731]
[2024-04-17 12:37:15,157: INFO: main: Training : batch 408 Loss: 0.016942644597670423]
[2024-04-17 12:37:15,786: INFO: main: Training : batch 409 Loss: 0.013295040138191851]
[2024-04-17 12:37:16,418: INFO: main: Training : batch 410 Loss: 0.015115836366960594]
[2024-04-17 12:37:17,049: INFO: main: Training : batch 411 Loss: 0.0024969860799125593]
[2024-04-17 12:37:17,684: INFO: main: Training : batch 412 Loss: 0.0016038088844218194]
[2024-04-17 12:37:18,313: INFO: main: Training : batch 413 Loss: 0.021249477876283344]
[2024-04-17 12:37:18,942: INFO: main: Training : batch 414 Loss: 0.010800780001407028]
[2024-04-17 12:37:19,575: INFO: main: Training : batch 415 Loss: 0.01821328518959449]
[2024-04-17 12:37:20,208: INFO: main: Training : batch 416 Loss: 0.00906817648158506]
[2024-04-17 12:37:20,840: INFO: main: Training : batch 417 Loss: 0.013435010119227955]
[2024-04-17 12:37:21,470: INFO: main: Training : batch 418 Loss: 0.0115416356948218]
[2024-04-17 12:37:22,102: INFO: main: Training : batch 419 Loss: 0.023400820558560374]
[2024-04-17 12:37:22,734: INFO: main: Training : batch 420 Loss: 0.0024062287429850557]
[2024-04-17 12:37:23,365: INFO: main: Training : batch 421 Loss: 0.017617055065206]
[2024-04-17 12:37:23,997: INFO: main: Training : batch 422 Loss: 0.012453207122444668]
[2024-04-17 12:37:24,635: INFO: main: Training : batch 423 Loss: 0.004102831507110427]
[2024-04-17 12:37:25,271: INFO: main: Training : batch 424 Loss: 0.012839394071811493]
[2024-04-17 12:37:25,906: INFO: main: Training : batch 425 Loss: 0.018472015823464732]
[2024-04-17 12:37:26,541: INFO: main: Training : batch 426 Loss: 0.0033930580244887977]
[2024-04-17 12:37:27,176: INFO: main: Training : batch 427 Loss: 0.00878604193771017]
[2024-04-17 12:37:27,805: INFO: main: Training : batch 428 Loss: 0.005686783501007554]
[2024-04-17 12:37:28,437: INFO: main: Training : batch 429 Loss: 0.00785251830759106]
[2024-04-17 12:37:29,071: INFO: main: Training : batch 430 Loss: 0.011409525145106032]
[2024-04-17 12:37:29,710: INFO: main: Training : batch 431 Loss: 0.006586986930838792]
[2024-04-17 12:37:30,337: INFO: main: Training : batch 432 Loss: 0.012220014706371332]
[2024-04-17 12:37:30,969: INFO: main: Training : batch 433 Loss: 0.004395672792454713]
[2024-04-17 12:37:31,600: INFO: main: Training : batch 434 Loss: 0.006720692210729934]
[2024-04-17 12:37:32,233: INFO: main: Training : batch 435 Loss: 0.013474428768796844]
[2024-04-17 12:37:32,862: INFO: main: Training : batch 436 Loss: 0.010879591855969432]
[2024-04-17 12:37:33,491: INFO: main: Training : batch 437 Loss: 0.015031267265341543]
[2024-04-17 12:37:34,117: INFO: main: Training : batch 438 Loss: 0.005441278840056932]
[2024-04-17 12:37:34,749: INFO: main: Training : batch 439 Loss: 0.0050571012283983025]
[2024-04-17 12:37:35,382: INFO: main: Training : batch 440 Loss: 0.0013664175469558556]
[2024-04-17 12:37:36,016: INFO: main: Training : batch 441 Loss: 0.0031135722129900394]
[2024-04-17 12:37:36,645: INFO: main: Training : batch 442 Loss: 0.004182313889840342]
[2024-04-17 12:37:37,277: INFO: main: Training : batch 443 Loss: 0.0014406976685192299]
[2024-04-17 12:37:37,912: INFO: main: Training : batch 444 Loss: 0.0007674774876988448]
[2024-04-17 12:37:38,553: INFO: main: Training : batch 445 Loss: 0.020747438791076253]
[2024-04-17 12:37:39,189: INFO: main: Training : batch 446 Loss: 0.004222749649760017]
[2024-04-17 12:37:39,828: INFO: main: Training : batch 447 Loss: 0.011447307424223233]
[2024-04-17 12:37:40,463: INFO: main: Training : batch 448 Loss: 0.005619762614771053]
[2024-04-17 12:37:41,097: INFO: main: Training : batch 449 Loss: 0.009253006971500284]
[2024-04-17 12:37:41,723: INFO: main: Training : batch 450 Loss: 0.02823034531008725]
[2024-04-17 12:37:42,354: INFO: main: Training : batch 451 Loss: 0.0019425193427668234]
[2024-04-17 12:37:42,981: INFO: main: Training : batch 452 Loss: 0.015653415179208895]
[2024-04-17 12:37:43,613: INFO: main: Training : batch 453 Loss: 0.008730742929359098]
[2024-04-17 12:37:44,241: INFO: main: Training : batch 454 Loss: 0.0033419246548809867]
[2024-04-17 12:37:44,874: INFO: main: Training : batch 455 Loss: 0.009884595856936297]
[2024-04-17 12:37:45,508: INFO: main: Training : batch 456 Loss: 0.0331356552110947]
[2024-04-17 12:37:46,139: INFO: main: Training : batch 457 Loss: 0.0023405353708401285]
[2024-04-17 12:37:46,769: INFO: main: Training : batch 458 Loss: 0.01723535616504972]
[2024-04-17 12:37:47,400: INFO: main: Training : batch 459 Loss: 0.004766733511808494]
[2024-04-17 12:37:48,026: INFO: main: Training : batch 460 Loss: 0.0040830607853522115]
[2024-04-17 12:37:48,656: INFO: main: Training : batch 461 Loss: 0.0022973077990216233]
[2024-04-17 12:37:49,284: INFO: main: Training : batch 462 Loss: 0.008157379352916197]
[2024-04-17 12:37:49,915: INFO: main: Training : batch 463 Loss: 0.015457980318723802]
[2024-04-17 12:37:50,546: INFO: main: Training : batch 464 Loss: 0.007884227110432998]
[2024-04-17 12:37:51,187: INFO: main: Training : batch 465 Loss: 0.00861004977785029]
[2024-04-17 12:37:51,818: INFO: main: Training : batch 466 Loss: 0.002991204386528044]
[2024-04-17 12:37:52,462: INFO: main: Training : batch 467 Loss: 0.006674772270910382]
[2024-04-17 12:37:53,106: INFO: main: Training : batch 468 Loss: 0.0067175875361682675]
[2024-04-17 12:37:53,744: INFO: main: Training : batch 469 Loss: 0.01668037410917054]
[2024-04-17 12:37:54,373: INFO: main: Training : batch 470 Loss: 0.00729854155394589]
[2024-04-17 12:37:55,006: INFO: main: Training : batch 471 Loss: 0.026792775141681234]
[2024-04-17 12:37:55,637: INFO: main: Training : batch 472 Loss: 0.0030757171550199703]
[2024-04-17 12:37:56,268: INFO: main: Training : batch 473 Loss: 0.023943141821540226]
[2024-04-17 12:37:56,897: INFO: main: Training : batch 474 Loss: 0.0038313538287977956]
[2024-04-17 12:37:57,527: INFO: main: Training : batch 475 Loss: 0.008566637844954202]
[2024-04-17 12:37:58,158: INFO: main: Training : batch 476 Loss: 0.009805186395860957]
[2024-04-17 12:37:58,789: INFO: main: Training : batch 477 Loss: 0.0040272222919046615]
[2024-04-17 12:37:59,421: INFO: main: Training : batch 478 Loss: 0.008279751830496031]
[2024-04-17 12:38:00,048: INFO: main: Training : batch 479 Loss: 0.005808839986803344]
[2024-04-17 12:38:00,683: INFO: main: Training : batch 480 Loss: 0.0041270062320708945]
[2024-04-17 12:38:01,315: INFO: main: Training : batch 481 Loss: 0.004435967064177996]
[2024-04-17 12:38:01,939: INFO: main: Training : batch 482 Loss: 0.023627870398164365]
[2024-04-17 12:38:02,568: INFO: main: Training : batch 483 Loss: 0.012212110496664645]
[2024-04-17 12:38:03,193: INFO: main: Training : batch 484 Loss: 0.005592304275034445]
[2024-04-17 12:38:03,825: INFO: main: Training : batch 485 Loss: 0.009790831450574598]
[2024-04-17 12:38:04,459: INFO: main: Training : batch 486 Loss: 0.0059600089409929825]
[2024-04-17 12:38:05,097: INFO: main: Training : batch 487 Loss: 0.0074628023434462135]
[2024-04-17 12:38:05,735: INFO: main: Training : batch 488 Loss: 0.003745080253701068]
[2024-04-17 12:38:06,370: INFO: main: Training : batch 489 Loss: 0.002261909895051091]
[2024-04-17 12:38:07,008: INFO: main: Training : batch 490 Loss: 0.008461364766532906]
[2024-04-17 12:38:07,648: INFO: main: Training : batch 491 Loss: 0.023367662311272342]
[2024-04-17 12:38:08,280: INFO: main: Training : batch 492 Loss: 0.0071896495614456445]
[2024-04-17 12:38:08,909: INFO: main: Training : batch 493 Loss: 0.00721231611136399]
[2024-04-17 12:38:09,540: INFO: main: Training : batch 494 Loss: 0.018182205256581586]
[2024-04-17 12:38:10,169: INFO: main: Training : batch 495 Loss: 0.01948180004329887]
[2024-04-17 12:38:10,799: INFO: main: Training : batch 496 Loss: 0.005674352937955346]
[2024-04-17 12:38:11,431: INFO: main: Training : batch 497 Loss: 0.016519822738570816]
[2024-04-17 12:38:12,059: INFO: main: Training : batch 498 Loss: 0.0084708968296527]
[2024-04-17 12:38:12,691: INFO: main: Training : batch 499 Loss: 0.04074845759981491]
[2024-04-17 12:38:13,323: INFO: main: Training : batch 500 Loss: 0.010036483960232746]
[2024-04-17 12:38:13,957: INFO: main: Training : batch 501 Loss: 0.005921277446357896]
[2024-04-17 12:38:14,592: INFO: main: Training : batch 502 Loss: 0.008006037221029954]
[2024-04-17 12:38:15,225: INFO: main: Training : batch 503 Loss: 0.021646779207631823]
[2024-04-17 12:38:15,857: INFO: main: Training : batch 504 Loss: 0.029582661572051513]
[2024-04-17 12:38:16,489: INFO: main: Training : batch 505 Loss: 0.007525398311476331]
[2024-04-17 12:38:17,122: INFO: main: Training : batch 506 Loss: 0.007639532906101498]
[2024-04-17 12:38:17,760: INFO: main: Training : batch 507 Loss: 0.012018351668892324]
[2024-04-17 12:38:18,396: INFO: main: Training : batch 508 Loss: 0.0049210032353973815]
[2024-04-17 12:38:19,034: INFO: main: Training : batch 509 Loss: 0.039477088303470256]
[2024-04-17 12:38:19,670: INFO: main: Training : batch 510 Loss: 0.012119956818562348]
[2024-04-17 12:38:20,310: INFO: main: Training : batch 511 Loss: 0.022186375332035678]
[2024-04-17 12:38:20,944: INFO: main: Training : batch 512 Loss: 0.005814758370015909]
[2024-04-17 12:38:21,577: INFO: main: Training : batch 513 Loss: 0.004181637974521293]
[2024-04-17 12:38:22,209: INFO: main: Training : batch 514 Loss: 0.007336528504158432]
[2024-04-17 12:38:22,837: INFO: main: Training : batch 515 Loss: 0.0022847471764765137]
[2024-04-17 12:38:23,469: INFO: main: Training : batch 516 Loss: 0.021637285735308463]
[2024-04-17 12:38:24,099: INFO: main: Training : batch 517 Loss: 0.011080173928234988]
[2024-04-17 12:38:24,731: INFO: main: Training : batch 518 Loss: 0.00844506430730656]
[2024-04-17 12:38:25,364: INFO: main: Training : batch 519 Loss: 0.0105439856809143]
[2024-04-17 12:38:25,992: INFO: main: Training : batch 520 Loss: 0.014367725932879024]
[2024-04-17 12:38:26,622: INFO: main: Training : batch 521 Loss: 0.011123015049306061]
[2024-04-17 12:38:27,255: INFO: main: Training : batch 522 Loss: 0.0058030052185197535]
[2024-04-17 12:38:27,887: INFO: main: Training : batch 523 Loss: 0.009388267419639466]
[2024-04-17 12:38:28,519: INFO: main: Training : batch 524 Loss: 0.018133660126471694]
[2024-04-17 12:38:29,153: INFO: main: Training : batch 525 Loss: 0.005833783557040916]
[2024-04-17 12:38:29,782: INFO: main: Training : batch 526 Loss: 0.006951935732237876]
[2024-04-17 12:38:30,411: INFO: main: Training : batch 527 Loss: 0.0073550229254193454]
[2024-04-17 12:38:31,042: INFO: main: Training : batch 528 Loss: 0.009239204611339783]
[2024-04-17 12:38:31,686: INFO: main: Training : batch 529 Loss: 0.014052719580384516]
[2024-04-17 12:38:32,329: INFO: main: Training : batch 530 Loss: 0.001924768741960804]
[2024-04-17 12:38:32,974: INFO: main: Training : batch 531 Loss: 0.014435180674682937]
[2024-04-17 12:38:33,612: INFO: main: Training : batch 532 Loss: 0.006911271947733344]
[2024-04-17 12:38:34,249: INFO: main: Training : batch 533 Loss: 0.004961040044249316]
[2024-04-17 12:38:34,881: INFO: main: Training : batch 534 Loss: 0.005619224273317972]
[2024-04-17 12:38:35,515: INFO: main: Training : batch 535 Loss: 0.005538380016829149]
[2024-04-17 12:38:36,148: INFO: main: Training : batch 536 Loss: 0.005026963868120235]
[2024-04-17 12:38:36,776: INFO: main: Training : batch 537 Loss: 0.014084619497152721]
[2024-04-17 12:38:37,405: INFO: main: Training : batch 538 Loss: 0.006113029462307015]
[2024-04-17 12:38:38,034: INFO: main: Training : batch 539 Loss: 0.0037000688312338516]
[2024-04-17 12:38:38,660: INFO: main: Training : batch 540 Loss: 0.00842609846399572]
[2024-04-17 12:38:39,293: INFO: main: Training : batch 541 Loss: 0.010921868178113083]
[2024-04-17 12:38:39,922: INFO: main: Training : batch 542 Loss: 0.0037053762524927505]
[2024-04-17 12:38:40,561: INFO: main: Training : batch 543 Loss: 0.013526190053259743]
[2024-04-17 12:38:41,193: INFO: main: Training : batch 544 Loss: 0.004734531353565352]
[2024-04-17 12:38:41,824: INFO: main: Training : batch 545 Loss: 0.013126438848176889]
[2024-04-17 12:38:42,456: INFO: main: Training : batch 546 Loss: 0.0033697877931559805]
[2024-04-17 12:38:43,086: INFO: main: Training : batch 547 Loss: 0.023845140053701266]
[2024-04-17 12:38:43,719: INFO: main: Training : batch 548 Loss: 0.017136581371538378]
[2024-04-17 12:38:44,348: INFO: main: Training : batch 549 Loss: 0.01419341424620888]
[2024-04-17 12:38:44,987: INFO: main: Training : batch 550 Loss: 0.013970652428291501]
[2024-04-17 12:38:45,624: INFO: main: Training : batch 551 Loss: 0.006643555137563392]
[2024-04-17 12:38:46,258: INFO: main: Training : batch 552 Loss: 0.01796263512889894]
[2024-04-17 12:38:46,899: INFO: main: Training : batch 553 Loss: 0.03226821038130005]
[2024-04-17 12:38:47,541: INFO: main: Training : batch 554 Loss: 0.023991357192957044]
[2024-04-17 12:38:48,172: INFO: main: Training : batch 555 Loss: 0.025864625171767172]
[2024-04-17 12:38:48,804: INFO: main: Training : batch 556 Loss: 0.022870730870937085]
[2024-04-17 12:38:49,438: INFO: main: Training : batch 557 Loss: 0.002285069287341342]
[2024-04-17 12:38:50,077: INFO: main: Training : batch 558 Loss: 0.010588948334807108]
[2024-04-17 12:38:50,712: INFO: main: Training : batch 559 Loss: 0.023899245703683163]
[2024-04-17 12:38:51,340: INFO: main: Training : batch 560 Loss: 0.01419987144978483]
[2024-04-17 12:38:51,972: INFO: main: Training : batch 561 Loss: 0.026969872639235393]
[2024-04-17 12:38:52,603: INFO: main: Training : batch 562 Loss: 0.011173399855852266]
[2024-04-17 12:38:53,237: INFO: main: Training : batch 563 Loss: 0.011380360542840376]
[2024-04-17 12:38:53,871: INFO: main: Training : batch 564 Loss: 0.008274784670453401]
[2024-04-17 12:38:54,504: INFO: main: Training : batch 565 Loss: 0.004725867584948609]
[2024-04-17 12:38:55,138: INFO: main: Training : batch 566 Loss: 0.01882053771306601]
[2024-04-17 12:38:55,773: INFO: main: Training : batch 567 Loss: 0.002559018853713795]
[2024-04-17 12:38:56,404: INFO: main: Training : batch 568 Loss: 0.006675312773575979]
[2024-04-17 12:38:57,032: INFO: main: Training : batch 569 Loss: 0.005043397790381604]
[2024-04-17 12:38:57,660: INFO: main: Training : batch 570 Loss: 0.00429684695716195]
[2024-04-17 12:38:58,298: INFO: main: Training : batch 571 Loss: 0.012447239687577485]
[2024-04-17 12:38:58,937: INFO: main: Training : batch 572 Loss: 0.005121715059459692]
[2024-04-17 12:38:59,571: INFO: main: Training : batch 573 Loss: 0.0018253032515161286]
[2024-04-17 12:39:00,224: INFO: main: Training : batch 574 Loss: 0.003300304329603017]
[2024-04-17 12:39:00,866: INFO: main: Training : batch 575 Loss: 0.010676371822154132]
[2024-04-17 12:39:01,496: INFO: main: Training : batch 576 Loss: 0.015334283245041564]
[2024-04-17 12:39:02,129: INFO: main: Training : batch 577 Loss: 0.007092605861770187]
[2024-04-17 12:39:02,760: INFO: main: Training : batch 578 Loss: 0.009519079091063204]
[2024-04-17 12:39:03,390: INFO: main: Training : batch 579 Loss: 0.003982680451913732]
[2024-04-17 12:39:04,023: INFO: main: Training : batch 580 Loss: 0.005483051416191304]
[2024-04-17 12:39:04,654: INFO: main: Training : batch 581 Loss: 0.020305258377780425]
[2024-04-17 12:39:05,290: INFO: main: Training : batch 582 Loss: 0.0038110493410944807]
[2024-04-17 12:39:05,922: INFO: main: Training : batch 583 Loss: 0.0005490950027192434]
[2024-04-17 12:39:06,558: INFO: main: Training : batch 584 Loss: 0.01791492444061291]
[2024-04-17 12:39:07,189: INFO: main: Training : batch 585 Loss: 0.005390606736961004]
[2024-04-17 12:39:07,822: INFO: main: Training : batch 586 Loss: 0.0004291303862307041]
[2024-04-17 12:39:08,459: INFO: main: Training : batch 587 Loss: 0.006057273454610058]
[2024-04-17 12:39:09,090: INFO: main: Training : batch 588 Loss: 0.00237841944694992]
[2024-04-17 12:39:09,723: INFO: main: Training : batch 589 Loss: 0.006309449106143753]
[2024-04-17 12:39:10,360: INFO: main: Training : batch 590 Loss: 0.014420695697077145]
[2024-04-17 12:39:10,989: INFO: main: Training : batch 591 Loss: 0.011352567441087138]
[2024-04-17 12:39:11,634: INFO: main: Training : batch 592 Loss: 0.02116206672985572]
[2024-04-17 12:39:12,273: INFO: main: Training : batch 593 Loss: 0.004331623950923062]
[2024-04-17 12:39:12,911: INFO: main: Training : batch 594 Loss: 0.004680603465575099]
[2024-04-17 12:39:13,546: INFO: main: Training : batch 595 Loss: 0.0055837211620279725]
[2024-04-17 12:39:14,179: INFO: main: Training : batch 596 Loss: 0.0035662897968107184]
[2024-04-17 12:39:14,819: INFO: main: Training : batch 597 Loss: 0.0049079403070966116]
[2024-04-17 12:39:15,450: INFO: main: Training : batch 598 Loss: 0.0031524591238863953]
[2024-04-17 12:39:16,082: INFO: main: Training : batch 599 Loss: 0.009515032704320674]
[2024-04-17 12:39:16,714: INFO: main: Training : batch 600 Loss: 0.016030139229626223]
[2024-04-17 12:39:17,350: INFO: main: Training : batch 601 Loss: 0.007825097743073288]
[2024-04-17 12:39:17,983: INFO: main: Training : batch 602 Loss: 0.02266441748019686]
[2024-04-17 12:39:18,609: INFO: main: Training : batch 603 Loss: 0.0003383638015466067]
[2024-04-17 12:39:19,237: INFO: main: Training : batch 604 Loss: 0.0026167810755597792]
[2024-04-17 12:39:19,869: INFO: main: Training : batch 605 Loss: 0.01573815835629775]
[2024-04-17 12:39:20,501: INFO: main: Training : batch 606 Loss: 0.005872518942934138]
[2024-04-17 12:39:21,138: INFO: main: Training : batch 607 Loss: 0.00332159241936115]
[2024-04-17 12:39:21,766: INFO: main: Training : batch 608 Loss: 0.007772061716791872]
[2024-04-17 12:39:22,404: INFO: main: Training : batch 609 Loss: 0.002194707081073766]
[2024-04-17 12:39:23,038: INFO: main: Training : batch 610 Loss: 0.011284582599235026]
[2024-04-17 12:39:23,673: INFO: main: Training : batch 611 Loss: 0.017175210068511786]
[2024-04-17 12:39:24,308: INFO: main: Training : batch 612 Loss: 0.004233837905509899]
[2024-04-17 12:39:24,939: INFO: main: Training : batch 613 Loss: 0.015005277662485767]
[2024-04-17 12:39:25,574: INFO: main: Training : batch 614 Loss: 0.0018753676089184959]
[2024-04-17 12:39:26,215: INFO: main: Training : batch 615 Loss: 0.005542370462855165]
[2024-04-17 12:39:26,847: INFO: main: Training : batch 616 Loss: 0.00739471763167912]
[2024-04-17 12:39:27,482: INFO: main: Training : batch 617 Loss: 0.002408300309994472]
[2024-04-17 12:39:28,123: INFO: main: Training : batch 618 Loss: 0.01048041813620309]
[2024-04-17 12:39:28,757: INFO: main: Training : batch 619 Loss: 0.010523211936627475]
[2024-04-17 12:39:29,392: INFO: main: Training : batch 620 Loss: 0.020367321737525854]
[2024-04-17 12:39:30,023: INFO: main: Training : batch 621 Loss: 0.0045048590885085615]
[2024-04-17 12:39:30,660: INFO: main: Training : batch 622 Loss: 0.011519293079261606]
[2024-04-17 12:39:31,290: INFO: main: Training : batch 623 Loss: 0.02094427376448774]
[2024-04-17 12:39:31,926: INFO: main: Training : batch 624 Loss: 0.026236915842288048]
[2024-04-17 12:39:32,553: INFO: main: Training : batch 625 Loss: 0.006814693142044537]
[2024-04-17 12:39:33,186: INFO: main: Training : batch 626 Loss: 0.005080016175812815]
[2024-04-17 12:39:33,816: INFO: main: Training : batch 627 Loss: 0.0009304220888866476]
[2024-04-17 12:39:34,450: INFO: main: Training : batch 628 Loss: 0.00231425599639733]
[2024-04-17 12:39:35,081: INFO: main: Training : batch 629 Loss: 0.012417730171642955]
[2024-04-17 12:39:35,717: INFO: main: Training : batch 630 Loss: 0.0221032670093856]
[2024-04-17 12:39:36,348: INFO: main: Training : batch 631 Loss: 0.0031511643797239646]
[2024-04-17 12:39:36,981: INFO: main: Training : batch 632 Loss: 0.010028570653215035]
[2024-04-17 12:39:37,611: INFO: main: Training : batch 633 Loss: 0.01437395316976814]
[2024-04-17 12:39:38,242: INFO: main: Training : batch 634 Loss: 0.026367676863917656]
[2024-04-17 12:39:38,870: INFO: main: Training : batch 635 Loss: 0.003606353245558976]
[2024-04-17 12:39:39,511: INFO: main: Training : batch 636 Loss: 0.010765792116350719]
[2024-04-17 12:39:40,148: INFO: main: Training : batch 637 Loss: 0.02190960089083678]
[2024-04-17 12:39:40,783: INFO: main: Training : batch 638 Loss: 0.003330777349016525]
[2024-04-17 12:39:41,417: INFO: main: Training : batch 639 Loss: 0.0100418262583325]
[2024-04-17 12:39:42,052: INFO: main: Training : batch 640 Loss: 0.002106625041633648]
[2024-04-17 12:39:42,682: INFO: main: Training : batch 641 Loss: 0.006786614266967942]
[2024-04-17 12:39:43,313: INFO: main: Training : batch 642 Loss: 0.010726147977868002]
[2024-04-17 12:39:43,946: INFO: main: Training : batch 643 Loss: 0.012072803446589625]
[2024-04-17 12:39:44,571: INFO: main: Training : batch 644 Loss: 0.010597075881383793]
[2024-04-17 12:39:45,199: INFO: main: Training : batch 645 Loss: 0.007466475009580588]
[2024-04-17 12:39:45,828: INFO: main: Training : batch 646 Loss: 0.0007537465394926057]
[2024-04-17 12:39:46,461: INFO: main: Training : batch 647 Loss: 0.01314432668142908]
[2024-04-17 12:39:47,098: INFO: main: Training : batch 648 Loss: 0.006572883565059951]
[2024-04-17 12:39:47,733: INFO: main: Training : batch 649 Loss: 0.01695333800443985]
[2024-04-17 12:39:48,368: INFO: main: Training : batch 650 Loss: 0.0027294896124493728]
[2024-04-17 12:39:48,997: INFO: main: Training : batch 651 Loss: 0.003634431883824272]
[2024-04-17 12:39:49,629: INFO: main: Training : batch 652 Loss: 0.007984173827058598]
[2024-04-17 12:39:50,258: INFO: main: Training : batch 653 Loss: 0.006812302152115026]
[2024-04-17 12:39:50,887: INFO: main: Training : batch 654 Loss: 0.0102172843826099]
[2024-04-17 12:39:51,519: INFO: main: Training : batch 655 Loss: 0.014903513916563435]
[2024-04-17 12:39:52,155: INFO: main: Training : batch 656 Loss: 0.008839711546648756]
[2024-04-17 12:39:52,786: INFO: main: Training : batch 657 Loss: 0.01163706105890153]
[2024-04-17 12:39:53,425: INFO: main: Training : batch 658 Loss: 0.01799325933236257]
[2024-04-17 12:39:54,058: INFO: main: Training : batch 659 Loss: 0.01299489183410033]
[2024-04-17 12:39:54,698: INFO: main: Training : batch 660 Loss: 0.0034931500000661407]
[2024-04-17 12:39:55,336: INFO: main: Training : batch 661 Loss: 0.007553282045630573]
[2024-04-17 12:39:55,964: INFO: main: Training : batch 662 Loss: 0.011949118264081937]
[2024-04-17 12:39:56,597: INFO: main: Training : batch 663 Loss: 0.013138013167063973]
[2024-04-17 12:39:57,230: INFO: main: Training : batch 664 Loss: 0.0011582069405449147]
[2024-04-17 12:39:57,862: INFO: main: Training : batch 665 Loss: 0.000848411241133868]
[2024-04-17 12:39:58,488: INFO: main: Training : batch 666 Loss: 0.00307870134844451]
[2024-04-17 12:39:59,124: INFO: main: Training : batch 667 Loss: 0.005631423884391329]
[2024-04-17 12:39:59,756: INFO: main: Training : batch 668 Loss: 0.005277508019971554]
[2024-04-17 12:40:00,385: INFO: main: Training : batch 669 Loss: 0.009343538563771408]
[2024-04-17 12:40:01,020: INFO: main: Training : batch 670 Loss: 0.006794348256437167]
[2024-04-17 12:40:01,653: INFO: main: Training : batch 671 Loss: 0.009104915984695578]
[2024-04-17 12:40:02,283: INFO: main: Training : batch 672 Loss: 0.002111037695711964]
[2024-04-17 12:40:02,915: INFO: main: Training : batch 673 Loss: 0.03440251290846474]
[2024-04-17 12:40:03,549: INFO: main: Training : batch 674 Loss: 0.014988018668271796]
[2024-04-17 12:40:04,184: INFO: main: Training : batch 675 Loss: 0.017345795136220806]
[2024-04-17 12:40:04,812: INFO: main: Training : batch 676 Loss: 0.0013447421114688273]
[2024-04-17 12:40:05,451: INFO: main: Training : batch 677 Loss: 0.014360053454578265]
[2024-04-17 12:40:06,088: INFO: main: Training : batch 678 Loss: 0.00801521098063505]
[2024-04-17 12:40:06,726: INFO: main: Training : batch 679 Loss: 0.008893259964434932]
[2024-04-17 12:40:07,364: INFO: main: Training : batch 680 Loss: 0.00677456697129734]
[2024-04-17 12:40:08,001: INFO: main: Training : batch 681 Loss: 0.011696087210846738]
[2024-04-17 12:40:08,637: INFO: main: Training : batch 682 Loss: 0.006456771830926563]
[2024-04-17 12:40:09,272: INFO: main: Training : batch 683 Loss: 0.002286251636294571]
[2024-04-17 12:40:09,902: INFO: main: Training : batch 684 Loss: 0.012272317285699481]
[2024-04-17 12:40:10,535: INFO: main: Training : batch 685 Loss: 0.018004671503984063]
[2024-04-17 12:40:11,161: INFO: main: Training : batch 686 Loss: 0.02954176645711055]
[2024-04-17 12:40:11,794: INFO: main: Training : batch 687 Loss: 0.024640898733814042]
[2024-04-17 12:40:12,425: INFO: main: Training : batch 688 Loss: 0.0020973263689067416]
[2024-04-17 12:40:13,054: INFO: main: Training : batch 689 Loss: 0.003442922993618227]
[2024-04-17 12:40:13,685: INFO: main: Training : batch 690 Loss: 0.002266626877956786]
[2024-04-17 12:40:14,320: INFO: main: Training : batch 691 Loss: 0.010149328026499324]
[2024-04-17 12:40:14,953: INFO: main: Training : batch 692 Loss: 0.005367982083999125]
[2024-04-17 12:40:15,592: INFO: main: Training : batch 693 Loss: 0.007226884487414304]
[2024-04-17 12:40:16,228: INFO: main: Training : batch 694 Loss: 0.007737373360577428]
[2024-04-17 12:40:16,858: INFO: main: Training : batch 695 Loss: 0.011255498609249816]
[2024-04-17 12:40:17,491: INFO: main: Training : batch 696 Loss: 0.0026318974788762787]
[2024-04-17 12:40:18,121: INFO: main: Training : batch 697 Loss: 0.024005586022509497]
[2024-04-17 12:40:18,751: INFO: main: Training : batch 698 Loss: 0.011052168343414439]
[2024-04-17 12:40:19,391: INFO: main: Training : batch 699 Loss: 0.020577902498189637]
[2024-04-17 12:40:20,031: INFO: main: Training : batch 700 Loss: 0.01896552934410931]
[2024-04-17 12:40:20,669: INFO: main: Training : batch 701 Loss: 0.0026883267449152115]
[2024-04-17 12:40:21,318: INFO: main: Training : batch 702 Loss: 0.010220715704588398]
[2024-04-17 12:40:21,955: INFO: main: Training : batch 703 Loss: 0.004773702362277283]
[2024-04-17 12:40:22,587: INFO: main: Training : batch 704 Loss: 0.010380759198148409]
[2024-04-17 12:40:23,218: INFO: main: Training : batch 705 Loss: 0.013961142073029616]
[2024-04-17 12:40:23,860: INFO: main: Training : batch 706 Loss: 0.009559142859280164]
[2024-04-17 12:40:24,490: INFO: main: Training : batch 707 Loss: 0.012090479573205033]
[2024-04-17 12:40:25,119: INFO: main: Training : batch 708 Loss: 0.007476170457535876]
[2024-04-17 12:40:25,755: INFO: main: Training : batch 709 Loss: 0.0015319604608799902]
[2024-04-17 12:40:26,389: INFO: main: Training : batch 710 Loss: 0.003109037151433845]
[2024-04-17 12:40:27,012: INFO: main: Training : batch 711 Loss: 0.02148157991081844]
[2024-04-17 12:40:27,646: INFO: main: Training : batch 712 Loss: 0.0023162722384305746]
[2024-04-17 12:40:28,277: INFO: main: Training : batch 713 Loss: 0.009558638226382445]
[2024-04-17 12:40:28,911: INFO: main: Training : batch 714 Loss: 0.02283783973210444]
[2024-04-17 12:40:29,543: INFO: main: Training : batch 715 Loss: 0.01065083087973327]
[2024-04-17 12:40:30,171: INFO: main: Training : batch 716 Loss: 0.008790755227795903]
[2024-04-17 12:40:30,802: INFO: main: Training : batch 717 Loss: 0.006951560553798675]
[2024-04-17 12:40:31,436: INFO: main: Training : batch 718 Loss: 0.025030102929467286]
[2024-04-17 12:40:32,067: INFO: main: Training : batch 719 Loss: 0.007078764325288678]
[2024-04-17 12:40:32,702: INFO: main: Training : batch 720 Loss: 0.003963840135549665]
[2024-04-17 12:40:33,340: INFO: main: Training : batch 721 Loss: 0.0064165975344618355]
[2024-04-17 12:40:33,971: INFO: main: Training : batch 722 Loss: 0.02083563269632824]
[2024-04-17 12:40:34,605: INFO: main: Training : batch 723 Loss: 0.010090541231982319]
[2024-04-17 12:40:35,246: INFO: main: Training : batch 724 Loss: 0.005089930713469154]
[2024-04-17 12:40:35,879: INFO: main: Training : batch 725 Loss: 0.005236546967967156]
[2024-04-17 12:40:36,509: INFO: main: Training : batch 726 Loss: 0.017803825846397205]
[2024-04-17 12:40:37,139: INFO: main: Training : batch 727 Loss: 0.00590480412656375]
[2024-04-17 12:40:37,769: INFO: main: Training : batch 728 Loss: 0.016589255760806384]
[2024-04-17 12:40:38,397: INFO: main: Training : batch 729 Loss: 0.005573894656487335]
[2024-04-17 12:40:39,029: INFO: main: Training : batch 730 Loss: 0.002443012543175696]
[2024-04-17 12:40:39,660: INFO: main: Training : batch 731 Loss: 0.005490037160678522]
[2024-04-17 12:40:40,291: INFO: main: Training : batch 732 Loss: 0.011279081962665548]
[2024-04-17 12:40:40,921: INFO: main: Training : batch 733 Loss: 0.016799949032994022]
[2024-04-17 12:40:41,550: INFO: main: Training : batch 734 Loss: 0.006633732372656871]
[2024-04-17 12:40:42,179: INFO: main: Training : batch 735 Loss: 0.003896446973772114]
[2024-04-17 12:40:42,807: INFO: main: Training : batch 736 Loss: 0.01883782041650784]
[2024-04-17 12:40:43,436: INFO: main: Training : batch 737 Loss: 0.004820161149689469]
[2024-04-17 12:40:44,072: INFO: main: Training : batch 738 Loss: 0.00625821922343489]
[2024-04-17 12:40:44,703: INFO: main: Training : batch 739 Loss: 0.007703980738814679]
[2024-04-17 12:40:45,336: INFO: main: Training : batch 740 Loss: 0.01076401059050106]
[2024-04-17 12:40:45,967: INFO: main: Training : batch 741 Loss: 0.008517569683909262]
[2024-04-17 12:40:46,608: INFO: main: Training : batch 742 Loss: 0.011166271760586226]
[2024-04-17 12:40:47,257: INFO: main: Training : batch 743 Loss: 0.00218763995330359]
[2024-04-17 12:40:47,893: INFO: main: Training : batch 744 Loss: 0.007955760714380707]
[2024-04-17 12:40:48,533: INFO: main: Training : batch 745 Loss: 0.024658193885377294]
[2024-04-17 12:40:49,164: INFO: main: Training : batch 746 Loss: 0.009647817923068876]
[2024-04-17 12:40:49,791: INFO: main: Training : batch 747 Loss: 0.006554492136686905]
[2024-04-17 12:40:50,427: INFO: main: Training : batch 748 Loss: 0.005905207583904096]
[2024-04-17 12:40:51,059: INFO: main: Training : batch 749 Loss: 0.016446751493116103]
[2024-04-17 12:40:51,689: INFO: main: Training : batch 750 Loss: 0.011000802652586138]
[2024-04-17 12:40:52,320: INFO: main: Training : batch 751 Loss: 0.006558926703967578]
[2024-04-17 12:40:52,948: INFO: main: Training : batch 752 Loss: 0.006518833377842976]
[2024-04-17 12:40:53,578: INFO: main: Training : batch 753 Loss: 0.01201740706900278]
[2024-04-17 12:40:54,209: INFO: main: Training : batch 754 Loss: 0.013474035981399936]
[2024-04-17 12:40:54,837: INFO: main: Training : batch 755 Loss: 0.0022596633887281476]
[2024-04-17 12:40:55,475: INFO: main: Training : batch 756 Loss: 0.05008497465632606]
[2024-04-17 12:40:56,104: INFO: main: Training : batch 757 Loss: 0.005487988321104337]
[2024-04-17 12:40:56,734: INFO: main: Training : batch 758 Loss: 0.0022263112369070815]
[2024-04-17 12:40:57,364: INFO: main: Training : batch 759 Loss: 0.00791971460548193]
[2024-04-17 12:40:57,996: INFO: main: Training : batch 760 Loss: 0.004906146105820892]
[2024-04-17 12:40:58,624: INFO: main: Training : batch 761 Loss: 0.007538466751991122]
[2024-04-17 12:40:59,261: INFO: main: Training : batch 762 Loss: 0.016822317531269303]
[2024-04-17 12:40:59,910: INFO: main: Training : batch 763 Loss: 0.03660529336242834]
[2024-04-17 12:41:00,548: INFO: main: Training : batch 764 Loss: 0.011755561392916985]
[2024-04-17 12:41:01,186: INFO: main: Training : batch 765 Loss: 0.008520592246471436]
[2024-04-17 12:41:01,828: INFO: main: Training : batch 766 Loss: 0.0073879440381672025]
[2024-04-17 12:41:02,468: INFO: main: Training : batch 767 Loss: 0.004475510285016151]
[2024-04-17 12:41:03,098: INFO: main: Training : batch 768 Loss: 0.009670379287070117]
[2024-04-17 12:41:03,726: INFO: main: Training : batch 769 Loss: 0.007402406946098862]
[2024-04-17 12:41:04,357: INFO: main: Training : batch 770 Loss: 0.009798675967299707]
[2024-04-17 12:41:04,991: INFO: main: Training : batch 771 Loss: 0.01618998487413104]
[2024-04-17 12:41:05,621: INFO: main: Training : batch 772 Loss: 0.004982039973481666]
[2024-04-17 12:41:06,250: INFO: main: Training : batch 773 Loss: 0.0028702508752815542]
[2024-04-17 12:41:06,882: INFO: main: Training : batch 774 Loss: 0.003486098258893432]
[2024-04-17 12:41:07,518: INFO: main: Training : batch 775 Loss: 0.013569147003947058]
[2024-04-17 12:41:08,150: INFO: main: Training : batch 776 Loss: 0.010268760411453066]
[2024-04-17 12:41:08,782: INFO: main: Training : batch 777 Loss: 0.009507900411119931]
[2024-04-17 12:41:09,413: INFO: main: Training : batch 778 Loss: 0.014015693756924292]
[2024-04-17 12:41:10,047: INFO: main: Training : batch 779 Loss: 0.01824731024404828]
[2024-04-17 12:41:10,680: INFO: main: Training : batch 780 Loss: 0.010300498826015055]
[2024-04-17 12:41:11,312: INFO: main: Training : batch 781 Loss: 0.010382325462076049]
[2024-04-17 12:41:11,943: INFO: main: Training : batch 782 Loss: 0.002400420245126959]
[2024-04-17 12:41:12,575: INFO: main: Training : batch 783 Loss: 0.006636420103525159]
[2024-04-17 12:41:13,214: INFO: main: Training : batch 784 Loss: 0.010341186458333936]
[2024-04-17 12:41:13,852: INFO: main: Training : batch 785 Loss: 0.011226469667175835]
[2024-04-17 12:41:14,487: INFO: main: Training : batch 786 Loss: 0.01270450653856939]
[2024-04-17 12:41:15,128: INFO: main: Training : batch 787 Loss: 0.007686541906854415]
[2024-04-17 12:41:15,770: INFO: main: Training : batch 788 Loss: 0.008159509739233088]
[2024-04-17 12:41:16,402: INFO: main: Training : batch 789 Loss: 0.00764321411721342]
[2024-04-17 12:41:17,034: INFO: main: Training : batch 790 Loss: 0.007102780617077545]
[2024-04-17 12:41:17,669: INFO: main: Training : batch 791 Loss: 0.009997410627048333]
[2024-04-17 12:41:18,297: INFO: main: Training : batch 792 Loss: 0.004040580893832467]
[2024-04-17 12:41:18,929: INFO: main: Training : batch 793 Loss: 0.008922201312457495]
[2024-04-17 12:41:19,560: INFO: main: Training : batch 794 Loss: 0.003238379067457438]
[2024-04-17 12:41:20,188: INFO: main: Training : batch 795 Loss: 0.008364717733915632]
[2024-04-17 12:41:20,820: INFO: main: Training : batch 796 Loss: 0.009475457039600007]
[2024-04-17 12:41:21,456: INFO: main: Training : batch 797 Loss: 0.018705947164613258]
[2024-04-17 12:41:22,089: INFO: main: Training : batch 798 Loss: 0.009210968731049263]
[2024-04-17 12:41:22,722: INFO: main: Training : batch 799 Loss: 0.00030001998238787477]
[2024-04-17 12:41:23,352: INFO: main: Training : batch 800 Loss: 0.0029690975518864285]
[2024-04-17 12:41:23,985: INFO: main: Training : batch 801 Loss: 0.025326005558345646]
[2024-04-17 12:41:24,614: INFO: main: Training : batch 802 Loss: 0.0021625646924964357]
[2024-04-17 12:41:25,244: INFO: main: Training : batch 803 Loss: 0.0037495929612532176]
[2024-04-17 12:41:25,878: INFO: main: Training : batch 804 Loss: 0.0037181740413824603]
[2024-04-17 12:41:26,517: INFO: main: Training : batch 805 Loss: 0.02189576726417543]
[2024-04-17 12:41:27,155: INFO: main: Training : batch 806 Loss: 0.006213111704334704]
[2024-04-17 12:41:27,802: INFO: main: Training : batch 807 Loss: 0.013485685558286466]
[2024-04-17 12:41:28,439: INFO: main: Training : batch 808 Loss: 0.004755888972717027]
[2024-04-17 12:41:29,072: INFO: main: Training : batch 809 Loss: 0.005280142331634495]
[2024-04-17 12:41:29,704: INFO: main: Training : batch 810 Loss: 0.00910773805254785]
[2024-04-17 12:41:30,337: INFO: main: Training : batch 811 Loss: 0.01508623038984336]
[2024-04-17 12:41:30,964: INFO: main: Training : batch 812 Loss: 0.010423810408610191]
[2024-04-17 12:41:31,594: INFO: main: Training : batch 813 Loss: 0.008926829450321624]
[2024-04-17 12:41:32,228: INFO: main: Training : batch 814 Loss: 0.009986893868253442]
[2024-04-17 12:41:32,858: INFO: main: Training : batch 815 Loss: 0.005952415977591229]
[2024-04-17 12:41:33,489: INFO: main: Training : batch 816 Loss: 0.02877783809219893]
[2024-04-17 12:41:34,129: INFO: main: Training : batch 817 Loss: 0.008628231297816688]
[2024-04-17 12:41:34,764: INFO: main: Training : batch 818 Loss: 0.015466250920501115]
[2024-04-17 12:41:35,396: INFO: main: Training : batch 819 Loss: 0.01145645255755523]
[2024-04-17 12:41:36,024: INFO: main: Training : batch 820 Loss: 0.002200712073781822]
[2024-04-17 12:41:36,655: INFO: main: Training : batch 821 Loss: 0.0030789631700552295]
[2024-04-17 12:41:37,291: INFO: main: Training : batch 822 Loss: 0.006293696730771325]
[2024-04-17 12:41:37,927: INFO: main: Training : batch 823 Loss: 0.0026227057600562973]
[2024-04-17 12:41:38,558: INFO: main: Training : batch 824 Loss: 0.0029713224959152704]
[2024-04-17 12:41:39,193: INFO: main: Training : batch 825 Loss: 0.007977436733035677]
[2024-04-17 12:41:39,837: INFO: main: Training : batch 826 Loss: 0.0007819147769726889]
[2024-04-17 12:41:40,474: INFO: main: Training : batch 827 Loss: 0.006392553812089914]
[2024-04-17 12:41:41,111: INFO: main: Training : batch 828 Loss: 0.00791826430934154]
[2024-04-17 12:41:41,747: INFO: main: Training : batch 829 Loss: 0.004590533272874071]
[2024-04-17 12:41:42,385: INFO: main: Training : batch 830 Loss: 0.005793304725880098]
[2024-04-17 12:41:43,021: INFO: main: Training : batch 831 Loss: 0.001896284778431224]
[2024-04-17 12:41:43,654: INFO: main: Training : batch 832 Loss: 0.01591346953061812]
[2024-04-17 12:41:44,284: INFO: main: Training : batch 833 Loss: 0.01765217375891677]
[2024-04-17 12:41:44,914: INFO: main: Training : batch 834 Loss: 0.005723547811880144]
[2024-04-17 12:41:45,552: INFO: main: Training : batch 835 Loss: 0.00873887702657723]
[2024-04-17 12:41:46,182: INFO: main: Training : batch 836 Loss: 0.00899049918679789]
[2024-04-17 12:41:46,815: INFO: main: Training : batch 837 Loss: 0.0026168474529386886]
[2024-04-17 12:41:47,445: INFO: main: Training : batch 838 Loss: 0.005062118610947093]
[2024-04-17 12:41:48,078: INFO: main: Training : batch 839 Loss: 0.002353469106550792]
[2024-04-17 12:41:48,711: INFO: main: Training : batch 840 Loss: 0.006146998502032821]
[2024-04-17 12:41:49,347: INFO: main: Training : batch 841 Loss: 0.0012830309924113372]
[2024-04-17 12:41:49,976: INFO: main: Training : batch 842 Loss: 0.009371078204985114]
[2024-04-17 12:41:50,607: INFO: main: Training : batch 843 Loss: 0.012087459776778074]
[2024-04-17 12:41:51,242: INFO: main: Training : batch 844 Loss: 0.004639732158027599]
[2024-04-17 12:41:51,870: INFO: main: Training : batch 845 Loss: 0.011618347987597819]
[2024-04-17 12:41:52,503: INFO: main: Training : batch 846 Loss: 0.006935155631544235]
[2024-04-17 12:41:53,139: INFO: main: Training : batch 847 Loss: 0.028130487599200886]
[2024-04-17 12:41:53,792: INFO: main: Training : batch 848 Loss: 0.015145164788073479]
[2024-04-17 12:41:54,430: INFO: main: Training : batch 849 Loss: 0.019462488376124895]
[2024-04-17 12:41:55,065: INFO: main: Training : batch 850 Loss: 0.0028697043710135322]
[2024-04-17 12:41:55,713: INFO: main: Training : batch 851 Loss: 0.02200041860576393]
[2024-04-17 12:41:56,345: INFO: main: Training : batch 852 Loss: 0.009441328584614276]
[2024-04-17 12:41:56,973: INFO: main: Training : batch 853 Loss: 0.0032381026297365885]
[2024-04-17 12:41:57,603: INFO: main: Training : batch 854 Loss: 0.010518103567762888]
[2024-04-17 12:41:58,236: INFO: main: Training : batch 855 Loss: 0.007291858085795751]
[2024-04-17 12:41:58,868: INFO: main: Training : batch 856 Loss: 0.00957066183401921]
[2024-04-17 12:41:59,501: INFO: main: Training : batch 857 Loss: 0.009079928969524162]
[2024-04-17 12:42:00,137: INFO: main: Training : batch 858 Loss: 0.010291485534697189]
[2024-04-17 12:42:00,767: INFO: main: Training : batch 859 Loss: 0.024256602785248466]
[2024-04-17 12:42:01,401: INFO: main: Training : batch 860 Loss: 0.001090193760022093]
[2024-04-17 12:42:02,035: INFO: main: Training : batch 861 Loss: 0.0036858513499761834]
[2024-04-17 12:42:02,666: INFO: main: Training : batch 862 Loss: 0.0034804297013721784]
[2024-04-17 12:42:03,288: INFO: main: Training : batch 863 Loss: 0.0019930944543312566]
[2024-04-17 12:42:03,916: INFO: main: Training : batch 864 Loss: 0.006045037710824998]
[2024-04-17 12:42:04,549: INFO: main: Training : batch 865 Loss: 0.005374203983462415]
[2024-04-17 12:42:05,181: INFO: main: Training : batch 866 Loss: 0.005656406336789318]
[2024-04-17 12:42:05,816: INFO: main: Training : batch 867 Loss: 0.004869751809726832]
[2024-04-17 12:42:06,460: INFO: main: Training : batch 868 Loss: 0.0017630015340946402]
[2024-04-17 12:42:07,097: INFO: main: Training : batch 869 Loss: 0.008386080625158181]
[2024-04-17 12:42:07,740: INFO: main: Training : batch 870 Loss: 0.0149385004407171]
[2024-04-17 12:42:08,383: INFO: main: Training : batch 871 Loss: 0.00932229418540785]
[2024-04-17 12:42:09,021: INFO: main: Training : batch 872 Loss: 0.002775000431738229]
[2024-04-17 12:42:09,655: INFO: main: Training : batch 873 Loss: 0.004932559998226465]
[2024-04-17 12:42:10,285: INFO: main: Training : batch 874 Loss: 0.008816054115566987]
[2024-04-17 12:42:10,915: INFO: main: Training : batch 875 Loss: 0.01923591120409679]
[2024-04-17 12:42:11,549: INFO: main: Training : batch 876 Loss: 0.010675693356003643]
[2024-04-17 12:42:12,179: INFO: main: Training : batch 877 Loss: 0.009339827354318327]
[2024-04-17 12:42:12,811: INFO: main: Training : batch 878 Loss: 0.00720248229360393]
[2024-04-17 12:42:13,434: INFO: main: Training : batch 879 Loss: 0.012918534075480286]
[2024-04-17 12:42:14,068: INFO: main: Training : batch 880 Loss: 0.013389895653715496]
[2024-04-17 12:42:14,695: INFO: main: Training : batch 881 Loss: 0.006660403566715792]
[2024-04-17 12:42:15,325: INFO: main: Training : batch 882 Loss: 0.021234971460344882]
[2024-04-17 12:42:15,953: INFO: main: Training : batch 883 Loss: 0.002251359392454496]
[2024-04-17 12:42:16,586: INFO: main: Training : batch 884 Loss: 0.007651890438814996]
[2024-04-17 12:42:17,220: INFO: main: Training : batch 885 Loss: 0.011500277165500778]
[2024-04-17 12:42:17,854: INFO: main: Training : batch 886 Loss: 0.020625530669190848]
[2024-04-17 12:42:18,489: INFO: main: Training : batch 887 Loss: 0.011523888824965347]
[2024-04-17 12:42:19,126: INFO: main: Training : batch 888 Loss: 0.01968476434006008]
[2024-04-17 12:42:19,758: INFO: main: Training : batch 889 Loss: 0.011237656046959373]
[2024-04-17 12:42:20,403: INFO: main: Training : batch 890 Loss: 0.0016711768943706248]
[2024-04-17 12:42:21,038: INFO: main: Training : batch 891 Loss: 0.01638031187235791]
[2024-04-17 12:42:21,676: INFO: main: Training : batch 892 Loss: 0.010596527604162758]
[2024-04-17 12:42:22,312: INFO: main: Training : batch 893 Loss: 0.02059857488864275]
[2024-04-17 12:42:22,958: INFO: main: Training : batch 894 Loss: 0.0037203425696702218]
[2024-04-17 12:42:23,591: INFO: main: Training : batch 895 Loss: 0.006583043367116994]
[2024-04-17 12:42:24,222: INFO: main: Training : batch 896 Loss: 0.0016805431503956394]
[2024-04-17 12:42:24,859: INFO: main: Training : batch 897 Loss: 0.005111802990454175]
[2024-04-17 12:42:25,491: INFO: main: Training : batch 898 Loss: 0.004643779601781815]
[2024-04-17 12:42:26,119: INFO: main: Training : batch 899 Loss: 0.02118249917744343]
[2024-04-17 12:42:26,746: INFO: main: Training : batch 900 Loss: 0.004710419757713234]
[2024-04-17 12:42:27,375: INFO: main: Training : batch 901 Loss: 0.02895533782429598]
[2024-04-17 12:42:28,008: INFO: main: Training : batch 902 Loss: 0.01066005934472231]
[2024-04-17 12:42:28,640: INFO: main: Training : batch 903 Loss: 0.00330081743647698]
[2024-04-17 12:42:29,273: INFO: main: Training : batch 904 Loss: 0.009085439313900571]
[2024-04-17 12:42:29,904: INFO: main: Training : batch 905 Loss: 0.0015814275929632582]
[2024-04-17 12:42:30,536: INFO: main: Training : batch 906 Loss: 0.0193663544768476]
[2024-04-17 12:42:31,165: INFO: main: Training : batch 907 Loss: 0.006537024415438961]
[2024-04-17 12:42:31,797: INFO: main: Training : batch 908 Loss: 0.01357213594122215]
[2024-04-17 12:42:32,431: INFO: main: Training : batch 909 Loss: 0.02630261400397652]
[2024-04-17 12:42:33,069: INFO: main: Training : batch 910 Loss: 0.0046073251242031545]
[2024-04-17 12:42:33,716: INFO: main: Training : batch 911 Loss: 0.01511331054838003]
[2024-04-17 12:42:34,351: INFO: main: Training : batch 912 Loss: 0.016491871933583452]
[2024-04-17 12:42:34,985: INFO: main: Training : batch 913 Loss: 0.012436926445224492]
[2024-04-17 12:42:35,622: INFO: main: Training : batch 914 Loss: 0.010977617668051153]
[2024-04-17 12:42:36,259: INFO: main: Training : batch 915 Loss: 0.011535412506006591]
[2024-04-17 12:42:36,890: INFO: main: Training : batch 916 Loss: 0.007617419090262798]
[2024-04-17 12:42:37,517: INFO: main: Training : batch 917 Loss: 0.006585232485190524]
[2024-04-17 12:42:38,153: INFO: main: Training : batch 918 Loss: 0.003923690906503907]
[2024-04-17 12:42:38,785: INFO: main: Training : batch 919 Loss: 0.026726037111104255]
[2024-04-17 12:42:39,413: INFO: main: Training : batch 920 Loss: 0.009128204765503266]
[2024-04-17 12:42:40,043: INFO: main: Training : batch 921 Loss: 0.0042702020880844216]
[2024-04-17 12:42:40,674: INFO: main: Training : batch 922 Loss: 0.004449837123904669]
[2024-04-17 12:42:41,302: INFO: main: Training : batch 923 Loss: 0.017084116326152154]
[2024-04-17 12:42:41,931: INFO: main: Training : batch 924 Loss: 0.015620683680667189]
[2024-04-17 12:42:42,564: INFO: main: Training : batch 925 Loss: 0.04499625008596981]
[2024-04-17 12:42:43,194: INFO: main: Training : batch 926 Loss: 0.013653535451547088]
[2024-04-17 12:42:43,826: INFO: main: Training : batch 927 Loss: 0.01388800051828876]
[2024-04-17 12:42:44,457: INFO: main: Training : batch 928 Loss: 0.01125218304030442]
[2024-04-17 12:42:45,086: INFO: main: Training : batch 929 Loss: 0.01705599084229591]
[2024-04-17 12:42:45,716: INFO: main: Training : batch 930 Loss: 0.009130255281868411]
[2024-04-17 12:42:46,347: INFO: main: Training : batch 931 Loss: 0.01053220542048321]
[2024-04-17 12:42:46,986: INFO: main: Training : batch 932 Loss: 0.008285139741107424]
[2024-04-17 12:42:47,629: INFO: main: Training : batch 933 Loss: 0.005102258634914795]
[2024-04-17 12:42:48,266: INFO: main: Training : batch 934 Loss: 0.00698313291178685]
[2024-04-17 12:42:48,903: INFO: main: Training : batch 935 Loss: 0.004616518178960571]
[2024-04-17 12:42:49,540: INFO: main: Training : batch 936 Loss: 0.017194520051414315]
[2024-04-17 12:42:50,174: INFO: main: Training : batch 937 Loss: 0.00747118686010979]
[2024-04-17 12:42:50,806: INFO: main: Training : batch 938 Loss: 0.005551505737260717]
[2024-04-17 12:42:51,438: INFO: main: Training : batch 939 Loss: 0.0017197383718572563]
[2024-04-17 12:42:52,070: INFO: main: Training : batch 940 Loss: 0.01515305253783796]
[2024-04-17 12:42:52,707: INFO: main: Training : batch 941 Loss: 0.004852669263858682]
[2024-04-17 12:42:53,338: INFO: main: Training : batch 942 Loss: 0.002392234879962054]
[2024-04-17 12:42:53,970: INFO: main: Training : batch 943 Loss: 0.006601818166863527]
[2024-04-17 12:42:54,600: INFO: main: Training : batch 944 Loss: 0.0032967560651418586]
[2024-04-17 12:42:55,229: INFO: main: Training : batch 945 Loss: 0.004173724543255566]
[2024-04-17 12:42:55,860: INFO: main: Training : batch 946 Loss: 0.01342561007648758]
[2024-04-17 12:42:56,490: INFO: main: Training : batch 947 Loss: 0.011913417386987507]
[2024-04-17 12:42:57,119: INFO: main: Training : batch 948 Loss: 0.016885481780899294]
[2024-04-17 12:42:57,749: INFO: main: Training : batch 949 Loss: 0.011882226768251911]
[2024-04-17 12:42:58,378: INFO: main: Training : batch 950 Loss: 0.014797614177640585]
[2024-04-17 12:42:59,009: INFO: main: Training : batch 951 Loss: 0.013161674834360565]
[2024-04-17 12:42:59,638: INFO: main: Training : batch 952 Loss: 0.017026868091785996]
[2024-04-17 12:43:00,274: INFO: main: Training : batch 953 Loss: 0.018020633308897693]
[2024-04-17 12:43:00,909: INFO: main: Training : batch 954 Loss: 0.01941352073215572]
[2024-04-17 12:43:01,553: INFO: main: Training : batch 955 Loss: 0.006675971940808581]
[2024-04-17 12:43:02,191: INFO: main: Training : batch 956 Loss: 0.025505257162433615]
[2024-04-17 12:43:02,829: INFO: main: Training : batch 957 Loss: 0.003451796293344433]
[2024-04-17 12:43:03,462: INFO: main: Training : batch 958 Loss: 0.015373295247203706]
[2024-04-17 12:43:04,097: INFO: main: Training : batch 959 Loss: 0.014065342201138576]
[2024-04-17 12:43:04,724: INFO: main: Training : batch 960 Loss: 0.012143540421795405]
[2024-04-17 12:43:05,358: INFO: main: Training : batch 961 Loss: 0.023539593903542204]
[2024-04-17 12:43:05,988: INFO: main: Training : batch 962 Loss: 0.025601914357059254]
[2024-04-17 12:43:06,618: INFO: main: Training : batch 963 Loss: 0.011665780110644081]
[2024-04-17 12:43:07,252: INFO: main: Training : batch 964 Loss: 0.009355873321028666]
[2024-04-17 12:43:07,884: INFO: main: Training : batch 965 Loss: 0.003428937990031185]
[2024-04-17 12:43:08,505: INFO: main: Training : batch 966 Loss: 0.003663269730610834]
[2024-04-17 12:43:09,137: INFO: main: Training : batch 967 Loss: 0.003376115499903666]
[2024-04-17 12:43:09,767: INFO: main: Training : batch 968 Loss: 0.009552412063937612]
[2024-04-17 12:43:10,394: INFO: main: Training : batch 969 Loss: 0.006675719876891825]
[2024-04-17 12:43:11,022: INFO: main: Training : batch 970 Loss: 0.008931594448485507]
[2024-04-17 12:43:11,652: INFO: main: Training : batch 971 Loss: 0.0037211899856612906]
[2024-04-17 12:43:12,283: INFO: main: Training : batch 972 Loss: 0.015721238237327045]
[2024-04-17 12:43:12,919: INFO: main: Training : batch 973 Loss: 0.006985823904798373]
[2024-04-17 12:43:13,555: INFO: main: Training : batch 974 Loss: 0.002439061867061632]
[2024-04-17 12:43:14,192: INFO: main: Training : batch 975 Loss: 0.006856064910933335]
[2024-04-17 12:43:14,838: INFO: main: Training : batch 976 Loss: 0.0071702737696317164]
[2024-04-17 12:43:15,471: INFO: main: Training : batch 977 Loss: 0.0034345983826441105]
[2024-04-17 12:43:16,113: INFO: main: Training : batch 978 Loss: 0.0036952514740089455]
[2024-04-17 12:43:16,743: INFO: main: Training : batch 979 Loss: 0.004965036223937928]
[2024-04-17 12:43:17,374: INFO: main: Training : batch 980 Loss: 0.018297807564970264]
[2024-04-17 12:43:18,005: INFO: main: Training : batch 981 Loss: 0.005507613990694164]
[2024-04-17 12:43:18,631: INFO: main: Training : batch 982 Loss: 0.0016047861635829729]
[2024-04-17 12:43:19,260: INFO: main: Training : batch 983 Loss: 0.00254244238537228]
[2024-04-17 12:43:19,885: INFO: main: Training : batch 984 Loss: 0.010811896881270839]
[2024-04-17 12:43:20,510: INFO: main: Training : batch 985 Loss: 0.025497420312854447]
[2024-04-17 12:43:21,138: INFO: main: Training : batch 986 Loss: 0.01574002837024141]
[2024-04-17 12:43:21,768: INFO: main: Training : batch 987 Loss: 0.010909307283342622]
[2024-04-17 12:43:22,402: INFO: main: Training : batch 988 Loss: 0.004182889412660423]
[2024-04-17 12:43:23,032: INFO: main: Training : batch 989 Loss: 0.0018268951647921448]
[2024-04-17 12:43:23,660: INFO: main: Training : batch 990 Loss: 0.037754565183033315]
[2024-04-17 12:43:24,289: INFO: main: Training : batch 991 Loss: 0.004493232083755035]
[2024-04-17 12:43:24,917: INFO: main: Training : batch 992 Loss: 0.013826174826553989]
[2024-04-17 12:43:25,546: INFO: main: Training : batch 993 Loss: 0.003670610829520248]
[2024-04-17 12:43:26,175: INFO: main: Training : batch 994 Loss: 0.007714862239625112]
[2024-04-17 12:43:26,813: INFO: main: Training : batch 995 Loss: 0.007447766164563834]
[2024-04-17 12:43:27,450: INFO: main: Training : batch 996 Loss: 0.006033135446873401]
[2024-04-17 12:43:28,084: INFO: main: Training : batch 997 Loss: 0.00873285488196845]
[2024-04-17 12:43:28,718: INFO: main: Training : batch 998 Loss: 0.0013918822893321241]
[2024-04-17 12:43:29,363: INFO: main: Training : batch 999 Loss: 0.0025506252872839726]
[2024-04-17 12:43:29,994: INFO: main: Training : batch 1000 Loss: 0.009458736637794774]
[2024-04-17 12:43:30,624: INFO: main: Training : batch 1001 Loss: 0.0076999461824764]
[2024-04-17 12:43:31,254: INFO: main: Training : batch 1002 Loss: 0.01203452976609837]
[2024-04-17 12:43:31,889: INFO: main: Training : batch 1003 Loss: 0.009300299758023243]
[2024-04-17 12:43:32,521: INFO: main: Training : batch 1004 Loss: 0.0010691402776513265]
[2024-04-17 12:43:33,150: INFO: main: Training : batch 1005 Loss: 0.009262837820755813]
[2024-04-17 12:43:33,780: INFO: main: Training : batch 1006 Loss: 0.0034216205923891835]
[2024-04-17 12:43:34,413: INFO: main: Training : batch 1007 Loss: 0.007779561116909422]
[2024-04-17 12:43:35,044: INFO: main: Training : batch 1008 Loss: 0.006094189651629339]
[2024-04-17 12:43:35,680: INFO: main: Training : batch 1009 Loss: 0.006267719139526196]
[2024-04-17 12:43:36,312: INFO: main: Training : batch 1010 Loss: 0.025562083741532654]
[2024-04-17 12:43:36,942: INFO: main: Training : batch 1011 Loss: 0.003314955752303125]
[2024-04-17 12:43:37,571: INFO: main: Training : batch 1012 Loss: 0.014347477527059355]
[2024-04-17 12:43:38,201: INFO: main: Training : batch 1013 Loss: 0.02224322266440743]
[2024-04-17 12:43:38,829: INFO: main: Training : batch 1014 Loss: 0.007116170037348653]
[2024-04-17 12:43:39,460: INFO: main: Training : batch 1015 Loss: 0.013297958246710737]
[2024-04-17 12:43:40,098: INFO: main: Training : batch 1016 Loss: 0.002631774652378557]
[2024-04-17 12:43:40,737: INFO: main: Training : batch 1017 Loss: 0.015424874102963905]
[2024-04-17 12:43:41,377: INFO: main: Training : batch 1018 Loss: 0.004399596045885185]
[2024-04-17 12:43:42,013: INFO: main: Training : batch 1019 Loss: 0.009918862526538444]
[2024-04-17 12:43:42,651: INFO: main: Training : batch 1020 Loss: 0.013029601623787]
[2024-04-17 12:43:43,282: INFO: main: Training : batch 1021 Loss: 0.003765210971364584]
[2024-04-17 12:43:43,915: INFO: main: Training : batch 1022 Loss: 0.009520878827982745]
[2024-04-17 12:43:44,544: INFO: main: Training : batch 1023 Loss: 0.008151069014024572]
[2024-04-17 12:43:45,180: INFO: main: Training : batch 1024 Loss: 0.006753106174931287]
[2024-04-17 12:43:45,813: INFO: main: Training : batch 1025 Loss: 0.026225796161716716]
[2024-04-17 12:43:46,451: INFO: main: Training : batch 1026 Loss: 0.02145266395258822]
[2024-04-17 12:43:47,083: INFO: main: Training : batch 1027 Loss: 0.012827177606558379]
[2024-04-17 12:43:47,717: INFO: main: Training : batch 1028 Loss: 0.025494516895211084]
[2024-04-17 12:43:48,347: INFO: main: Training : batch 1029 Loss: 0.005394859659813533]
[2024-04-17 12:43:48,974: INFO: main: Training : batch 1030 Loss: 0.011143785468909493]
[2024-04-17 12:43:49,604: INFO: main: Training : batch 1031 Loss: 0.0023886682562329685]
[2024-04-17 12:43:50,232: INFO: main: Training : batch 1032 Loss: 0.002315796682655781]
[2024-04-17 12:43:50,863: INFO: main: Training : batch 1033 Loss: 0.022822657343412957]
[2024-04-17 12:43:51,497: INFO: main: Training : batch 1034 Loss: 0.0035945849810915488]
[2024-04-17 12:43:52,130: INFO: main: Training : batch 1035 Loss: 0.01478363221607244]
[2024-04-17 12:43:52,757: INFO: main: Training : batch 1036 Loss: 0.006870984217265204]
[2024-04-17 12:43:53,393: INFO: main: Training : batch 1037 Loss: 0.009160328837910264]
[2024-04-17 12:43:54,031: INFO: main: Training : batch 1038 Loss: 0.00434100494192001]
[2024-04-17 12:43:54,666: INFO: main: Training : batch 1039 Loss: 0.0030458369127734208]
[2024-04-17 12:43:55,301: INFO: main: Training : batch 1040 Loss: 0.031062141095747127]
[2024-04-17 12:43:55,936: INFO: main: Training : batch 1041 Loss: 0.004264267169071009]
[2024-04-17 12:43:56,575: INFO: main: Training : batch 1042 Loss: 0.009596788254355807]
[2024-04-17 12:43:57,205: INFO: main: Training : batch 1043 Loss: 0.010030411520819365]
[2024-04-17 12:43:57,832: INFO: main: Training : batch 1044 Loss: 0.021754784254848485]
[2024-04-17 12:43:58,460: INFO: main: Training : batch 1045 Loss: 0.0010541729716197342]
[2024-04-17 12:43:59,088: INFO: main: Training : batch 1046 Loss: 0.010661268099126285]
[2024-04-17 12:43:59,717: INFO: main: Training : batch 1047 Loss: 0.02285663066980673]
[2024-04-17 12:44:00,343: INFO: main: Training : batch 1048 Loss: 0.01877260408928232]
[2024-04-17 12:44:00,972: INFO: main: Training : batch 1049 Loss: 0.008791122149895901]
[2024-04-17 12:44:01,605: INFO: main: Training : batch 1050 Loss: 0.011867641050454344]
[2024-04-17 12:44:02,234: INFO: main: Training : batch 1051 Loss: 0.03056266334034451]
[2024-04-17 12:44:02,870: INFO: main: Training : batch 1052 Loss: 0.007716495744411187]
[2024-04-17 12:44:03,501: INFO: main: Training : batch 1053 Loss: 0.010307751456593047]
[2024-04-17 12:44:04,132: INFO: main: Training : batch 1054 Loss: 0.014282751943497303]
[2024-04-17 12:44:04,757: INFO: main: Training : batch 1055 Loss: 0.006315530812163515]
[2024-04-17 12:44:05,393: INFO: main: Training : batch 1056 Loss: 0.020496206644365907]
[2024-04-17 12:44:06,028: INFO: main: Training : batch 1057 Loss: 0.003881391556931021]
[2024-04-17 12:44:06,657: INFO: main: Training : batch 1058 Loss: 0.00353143367430196]
[2024-04-17 12:44:07,288: INFO: main: Training : batch 1059 Loss: 0.008549998160273057]
[2024-04-17 12:44:07,924: INFO: main: Training : batch 1060 Loss: 0.0530132111260976]
[2024-04-17 12:44:08,561: INFO: main: Training : batch 1061 Loss: 0.002895176517390531]
[2024-04-17 12:44:09,192: INFO: main: Training : batch 1062 Loss: 0.003429269321859848]
[2024-04-17 12:44:09,828: INFO: main: Training : batch 1063 Loss: 0.0061919938350147915]
[2024-04-17 12:44:10,460: INFO: main: Training : batch 1064 Loss: 0.024360313990035503]
[2024-04-17 12:44:11,090: INFO: main: Training : batch 1065 Loss: 0.01930068340958825]
[2024-04-17 12:44:11,720: INFO: main: Training : batch 1066 Loss: 0.013617907688802692]
[2024-04-17 12:44:12,346: INFO: main: Training : batch 1067 Loss: 0.008540071900870678]
[2024-04-17 12:44:12,975: INFO: main: Training : batch 1068 Loss: 0.004306068393656666]
[2024-04-17 12:44:13,601: INFO: main: Training : batch 1069 Loss: 0.009751135486609771]
[2024-04-17 12:44:14,230: INFO: main: Training : batch 1070 Loss: 0.0031095718751903206]
[2024-04-17 12:44:14,862: INFO: main: Training : batch 1071 Loss: 0.021386420870188473]
[2024-04-17 12:44:15,490: INFO: main: Training : batch 1072 Loss: 0.01789130882086094]
[2024-04-17 12:44:16,120: INFO: main: Training : batch 1073 Loss: 0.01810250717549113]
[2024-04-17 12:44:16,748: INFO: main: Training : batch 1074 Loss: 0.026590763536179705]
[2024-04-17 12:44:17,376: INFO: main: Training : batch 1075 Loss: 0.009822327132889903]
[2024-04-17 12:44:18,003: INFO: main: Training : batch 1076 Loss: 0.019598566030724632]
[2024-04-17 12:44:18,632: INFO: main: Training : batch 1077 Loss: 0.008328630236014375]
[2024-04-17 12:44:19,263: INFO: main: Training : batch 1078 Loss: 0.004287406777990974]
[2024-04-17 12:44:19,889: INFO: main: Training : batch 1079 Loss: 0.012461361912318458]
[2024-04-17 12:44:20,525: INFO: main: Training : batch 1080 Loss: 0.011132457773254431]
[2024-04-17 12:44:21,166: INFO: main: Training : batch 1081 Loss: 0.022908773100165283]
[2024-04-17 12:44:21,808: INFO: main: Training : batch 1082 Loss: 0.008159646627554556]
[2024-04-17 12:44:22,441: INFO: main: Training : batch 1083 Loss: 0.011988485404541295]
[2024-04-17 12:44:23,088: INFO: main: Training : batch 1084 Loss: 0.0019148902511070729]
[2024-04-17 12:44:23,722: INFO: main: Training : batch 1085 Loss: 0.004612172298923871]
[2024-04-17 12:44:24,353: INFO: main: Training : batch 1086 Loss: 0.0045369962307716645]
[2024-04-17 12:44:24,983: INFO: main: Training : batch 1087 Loss: 0.009029524904823989]
[2024-04-17 12:44:25,612: INFO: main: Training : batch 1088 Loss: 0.009910272547507504]
[2024-04-17 12:44:26,241: INFO: main: Training : batch 1089 Loss: 0.0026713935004118224]
[2024-04-17 12:44:26,870: INFO: main: Training : batch 1090 Loss: 0.005921203558491606]
[2024-04-17 12:44:27,504: INFO: main: Training : batch 1091 Loss: 0.010290703187147521]
[2024-04-17 12:44:28,134: INFO: main: Training : batch 1092 Loss: 0.007417007878498188]
[2024-04-17 12:44:28,766: INFO: main: Training : batch 1093 Loss: 0.051736135723361096]
[2024-04-17 12:44:29,395: INFO: main: Training : batch 1094 Loss: 0.007214918571123718]
[2024-04-17 12:44:30,030: INFO: main: Training : batch 1095 Loss: 0.013271268072777573]
[2024-04-17 12:44:30,661: INFO: main: Training : batch 1096 Loss: 0.0069358907914593175]
[2024-04-17 12:44:31,290: INFO: main: Training : batch 1097 Loss: 0.011593338856789108]
[2024-04-17 12:44:31,923: INFO: main: Training : batch 1098 Loss: 0.0010330626811592748]
[2024-04-17 12:44:32,556: INFO: main: Training : batch 1099 Loss: 0.006164100319484577]
[2024-04-17 12:44:33,192: INFO: main: Training : batch 1100 Loss: 0.008022076943337134]
[2024-04-17 12:44:33,834: INFO: main: Training : batch 1101 Loss: 0.009165770060517985]
[2024-04-17 12:44:34,478: INFO: main: Training : batch 1102 Loss: 0.006037892417995113]
[2024-04-17 12:44:35,120: INFO: main: Training : batch 1103 Loss: 0.018531662875006]
[2024-04-17 12:44:35,756: INFO: main: Training : batch 1104 Loss: 0.08947311245317151]
[2024-04-17 12:44:36,397: INFO: main: Training : batch 1105 Loss: 0.00419434806270314]
[2024-04-17 12:44:37,032: INFO: main: Training : batch 1106 Loss: 0.008698189776912156]
[2024-04-17 12:44:37,663: INFO: main: Training : batch 1107 Loss: 0.008885608285300434]
[2024-04-17 12:44:38,295: INFO: main: Training : batch 1108 Loss: 0.006369572898484768]
[2024-04-17 12:44:38,932: INFO: main: Training : batch 1109 Loss: 0.025676142490689037]
[2024-04-17 12:44:39,564: INFO: main: Training : batch 1110 Loss: 0.006308821802513507]
[2024-04-17 12:44:40,198: INFO: main: Training : batch 1111 Loss: 0.0022266429206592602]
[2024-04-17 12:44:40,824: INFO: main: Training : batch 1112 Loss: 0.006622224084655867]
[2024-04-17 12:44:41,459: INFO: main: Training : batch 1113 Loss: 0.014021101306006329]
[2024-04-17 12:44:42,091: INFO: main: Training : batch 1114 Loss: 0.013791576674256269]
[2024-04-17 12:44:42,720: INFO: main: Training : batch 1115 Loss: 0.0027462140248946353]
[2024-04-17 12:44:43,353: INFO: main: Training : batch 1116 Loss: 0.021128258279271853]
[2024-04-17 12:44:43,987: INFO: main: Training : batch 1117 Loss: 0.011266108576621012]
[2024-04-17 12:44:44,622: INFO: main: Training : batch 1118 Loss: 0.011908039352252016]
[2024-04-17 12:44:45,258: INFO: main: Training : batch 1119 Loss: 0.018593383168223596]
[2024-04-17 12:44:45,892: INFO: main: Training : batch 1120 Loss: 0.02440611034603301]
[2024-04-17 12:44:46,525: INFO: main: Training : batch 1121 Loss: 0.02361355123167992]
[2024-04-17 12:44:47,166: INFO: main: Training : batch 1122 Loss: 0.00951368270940515]
[2024-04-17 12:44:47,812: INFO: main: Training : batch 1123 Loss: 0.010126284505601308]
[2024-04-17 12:44:48,457: INFO: main: Training : batch 1124 Loss: 0.011034238760018118]
[2024-04-17 12:44:49,097: INFO: main: Training : batch 1125 Loss: 0.004086891207743828]
[2024-04-17 12:44:49,731: INFO: main: Training : batch 1126 Loss: 0.0011397203222872825]
[2024-04-17 12:44:50,373: INFO: main: Training : batch 1127 Loss: 0.008165494491851924]
[2024-04-17 12:44:51,006: INFO: main: Training : batch 1128 Loss: 0.008798430068998626]
[2024-04-17 12:44:51,638: INFO: main: Training : batch 1129 Loss: 0.014424742348315563]
[2024-04-17 12:44:52,272: INFO: main: Training : batch 1130 Loss: 0.015056344922545732]
[2024-04-17 12:44:52,908: INFO: main: Training : batch 1131 Loss: 0.003544817014698954]
[2024-04-17 12:44:53,543: INFO: main: Training : batch 1132 Loss: 0.000851761917965525]
[2024-04-17 12:44:54,173: INFO: main: Training : batch 1133 Loss: 0.004134461407995725]
[2024-04-17 12:44:54,810: INFO: main: Training : batch 1134 Loss: 0.007632529281620774]
[2024-04-17 12:44:55,440: INFO: main: Training : batch 1135 Loss: 0.0007329111295974882]
[2024-04-17 12:44:56,074: INFO: main: Training : batch 1136 Loss: 0.00769845161032122]
[2024-04-17 12:44:56,709: INFO: main: Training : batch 1137 Loss: 0.013996334002775607]
[2024-04-17 12:44:57,343: INFO: main: Training : batch 1138 Loss: 0.016157117368697103]
[2024-04-17 12:44:57,980: INFO: main: Training : batch 1139 Loss: 0.004807094390795584]
[2024-04-17 12:44:58,619: INFO: main: Training : batch 1140 Loss: 0.01205189196122614]
[2024-04-17 12:44:59,254: INFO: main: Training : batch 1141 Loss: 0.014434704327110168]
[2024-04-17 12:44:59,884: INFO: main: Training : batch 1142 Loss: 0.013387816077010195]
[2024-04-17 12:45:00,525: INFO: main: Training : batch 1143 Loss: 0.00477422740935675]
[2024-04-17 12:45:01,168: INFO: main: Training : batch 1144 Loss: 0.004753886818846157]
[2024-04-17 12:45:01,815: INFO: main: Training : batch 1145 Loss: 0.01028794812029627]
[2024-04-17 12:45:02,451: INFO: main: Training : batch 1146 Loss: 0.0006405425787259323]
[2024-04-17 12:45:03,097: INFO: main: Training : batch 1147 Loss: 0.01669946809993265]
[2024-04-17 12:45:03,738: INFO: main: Training : batch 1148 Loss: 0.016530715622963342]
[2024-04-17 12:45:04,375: INFO: main: Training : batch 1149 Loss: 0.0014948102860668471]
[2024-04-17 12:45:05,015: INFO: main: Training : batch 1150 Loss: 0.003982886380877082]
[2024-04-17 12:45:05,647: INFO: main: Training : batch 1151 Loss: 0.004375731136942464]
[2024-04-17 12:45:06,285: INFO: main: Training : batch 1152 Loss: 0.006813260011650395]
[2024-04-17 12:45:06,915: INFO: main: Training : batch 1153 Loss: 0.0034172166815664776]
[2024-04-17 12:45:07,547: INFO: main: Training : batch 1154 Loss: 0.004490776316676425]
[2024-04-17 12:45:08,183: INFO: main: Training : batch 1155 Loss: 0.009270750904277852]
[2024-04-17 12:45:08,816: INFO: main: Training : batch 1156 Loss: 0.013181067750774404]
[2024-04-17 12:45:09,450: INFO: main: Training : batch 1157 Loss: 0.010548924671676362]
[2024-04-17 12:45:10,087: INFO: main: Training : batch 1158 Loss: 0.004791690271000006]
[2024-04-17 12:45:10,721: INFO: main: Training : batch 1159 Loss: 0.011619423504375009]
[2024-04-17 12:45:11,353: INFO: main: Training : batch 1160 Loss: 0.0037366401646457756]
[2024-04-17 12:45:11,991: INFO: main: Training : batch 1161 Loss: 0.0018315225607276576]
[2024-04-17 12:45:12,623: INFO: main: Training : batch 1162 Loss: 0.0013976334917957386]
[2024-04-17 12:45:13,267: INFO: main: Training : batch 1163 Loss: 0.0065828572024308925]
[2024-04-17 12:45:13,906: INFO: main: Training : batch 1164 Loss: 0.001975279439281659]
[2024-04-17 12:45:14,538: INFO: main: Training : batch 1165 Loss: 0.0095008680446041]
[2024-04-17 12:45:15,178: INFO: main: Training : batch 1166 Loss: 0.021717753395159606]
[2024-04-17 12:45:15,820: INFO: main: Training : batch 1167 Loss: 0.010337290927020275]
[2024-04-17 12:45:16,459: INFO: main: Training : batch 1168 Loss: 0.002806124387084341]
[2024-04-17 12:45:17,094: INFO: main: Training : batch 1169 Loss: 0.017565435025179554]
[2024-04-17 12:45:17,727: INFO: main: Training : batch 1170 Loss: 0.00073649918144773]
[2024-04-17 12:45:18,362: INFO: main: Training : batch 1171 Loss: 0.018113668412088647]
[2024-04-17 12:45:18,993: INFO: main: Training : batch 1172 Loss: 0.0031742800333614644]
[2024-04-17 12:45:19,621: INFO: main: Training : batch 1173 Loss: 0.021808590822124326]
[2024-04-17 12:45:20,250: INFO: main: Training : batch 1174 Loss: 0.0256199865537751]
[2024-04-17 12:45:20,885: INFO: main: Training : batch 1175 Loss: 0.0013049253003060387]
[2024-04-17 12:45:21,518: INFO: main: Training : batch 1176 Loss: 0.006437797462497115]
[2024-04-17 12:45:22,147: INFO: main: Training : batch 1177 Loss: 0.011094162691634457]
[2024-04-17 12:45:22,775: INFO: main: Training : batch 1178 Loss: 0.004497493300643404]
[2024-04-17 12:45:23,406: INFO: main: Training : batch 1179 Loss: 0.014375091920728441]
[2024-04-17 12:45:24,034: INFO: main: Training : batch 1180 Loss: 0.0034546081101885576]
[2024-04-17 12:45:24,665: INFO: main: Training : batch 1181 Loss: 0.011610403849810328]
[2024-04-17 12:45:25,290: INFO: main: Training : batch 1182 Loss: 0.05453537076946397]
[2024-04-17 12:45:25,922: INFO: main: Training : batch 1183 Loss: 0.0061360677863856955]
[2024-04-17 12:45:26,550: INFO: main: Training : batch 1184 Loss: 0.032253397703080236]
[2024-04-17 12:45:27,175: INFO: main: Training : batch 1185 Loss: 0.011230528141306189]
[2024-04-17 12:45:27,812: INFO: main: Training : batch 1186 Loss: 0.011129411366026375]
[2024-04-17 12:45:28,446: INFO: main: Training : batch 1187 Loss: 0.009065613380065783]
[2024-04-17 12:45:29,082: INFO: main: Training : batch 1188 Loss: 0.00713399111515246]
[2024-04-17 12:45:29,716: INFO: main: Training : batch 1189 Loss: 0.002303826757925693]
[2024-04-17 12:45:30,355: INFO: main: Training : batch 1190 Loss: 0.009469668895651985]
[2024-04-17 12:45:30,986: INFO: main: Training : batch 1191 Loss: 0.003946339469235638]
[2024-04-17 12:45:31,613: INFO: main: Training : batch 1192 Loss: 0.0021536198037658692]
[2024-04-17 12:45:32,246: INFO: main: Training : batch 1193 Loss: 0.0031100715153125967]
[2024-04-17 12:45:32,870: INFO: main: Training : batch 1194 Loss: 0.01311096363277068]
[2024-04-17 12:45:33,501: INFO: main: Training : batch 1195 Loss: 0.027745263984817452]
[2024-04-17 12:45:34,129: INFO: main: Training : batch 1196 Loss: 0.011152354781939287]
[2024-04-17 12:45:34,758: INFO: main: Training : batch 1197 Loss: 0.011536372307761176]
[2024-04-17 12:45:35,387: INFO: main: Training : batch 1198 Loss: 0.007738847906930023]
[2024-04-17 12:45:36,015: INFO: main: Training : batch 1199 Loss: 0.00841546673683062]
[2024-04-17 12:45:36,640: INFO: main: Training : batch 1200 Loss: 0.018077036579429762]
[2024-04-17 12:45:37,266: INFO: main: Training : batch 1201 Loss: 0.005898460381254934]
[2024-04-17 12:45:37,893: INFO: main: Training : batch 1202 Loss: 0.01825305576565625]
[2024-04-17 12:45:38,518: INFO: main: Training : batch 1203 Loss: 0.00876315755193869]
[2024-04-17 12:45:39,160: INFO: main: Training : batch 1204 Loss: 0.0036867834819373072]
[2024-04-17 12:45:39,786: INFO: main: Training : batch 1205 Loss: 0.002456520521265534]
[2024-04-17 12:45:40,417: INFO: main: Training : batch 1206 Loss: 0.006194154648941336]
[2024-04-17 12:45:41,049: INFO: main: Training : batch 1207 Loss: 0.0027493860381115017]
[2024-04-17 12:45:41,692: INFO: main: Training : batch 1208 Loss: 0.003722125505856872]
[2024-04-17 12:45:42,324: INFO: main: Training : batch 1209 Loss: 0.0049794240168553345]
[2024-04-17 12:45:42,961: INFO: main: Training : batch 1210 Loss: 0.0027970625183398224]
[2024-04-17 12:45:43,597: INFO: main: Training : batch 1211 Loss: 0.006121747825128008]
[2024-04-17 12:45:44,229: INFO: main: Training : batch 1212 Loss: 0.001842185629186519]
[2024-04-17 12:45:44,858: INFO: main: Training : batch 1213 Loss: 0.005520748009629745]
[2024-04-17 12:45:45,483: INFO: main: Training : batch 1214 Loss: 0.005593508715175275]
[2024-04-17 12:45:46,114: INFO: main: Training : batch 1215 Loss: 0.010428756378304493]
[2024-04-17 12:45:46,742: INFO: main: Training : batch 1216 Loss: 0.004002802077319391]
[2024-04-17 12:45:47,369: INFO: main: Training : batch 1217 Loss: 0.00157365185773531]
[2024-04-17 12:45:47,995: INFO: main: Training : batch 1218 Loss: 0.011720900616049109]
[2024-04-17 12:45:48,625: INFO: main: Training : batch 1219 Loss: 0.011491239797948714]
[2024-04-17 12:45:49,256: INFO: main: Training : batch 1220 Loss: 0.0017325380711883218]
[2024-04-17 12:45:49,891: INFO: main: Training : batch 1221 Loss: 0.012556225296161605]
[2024-04-17 12:45:50,524: INFO: main: Training : batch 1222 Loss: 0.016476640887580222]
[2024-04-17 12:45:51,155: INFO: main: Training : batch 1223 Loss: 0.005707768885334021]
[2024-04-17 12:45:51,785: INFO: main: Training : batch 1224 Loss: 0.003792430541123794]
[2024-04-17 12:45:52,420: INFO: main: Training : batch 1225 Loss: 0.013290218715541402]
[2024-04-17 12:45:53,053: INFO: main: Training : batch 1226 Loss: 0.06576866344697677]
[2024-04-17 12:45:53,683: INFO: main: Training : batch 1227 Loss: 0.025545998376683917]
[2024-04-17 12:45:54,321: INFO: main: Training : batch 1228 Loss: 0.006967877422018071]
[2024-04-17 12:45:54,962: INFO: main: Training : batch 1229 Loss: 0.005317991659577438]
[2024-04-17 12:45:55,602: INFO: main: Training : batch 1230 Loss: 0.00378741146527778]
[2024-04-17 12:45:56,242: INFO: main: Training : batch 1231 Loss: 0.0061743475391945425]
[2024-04-17 12:45:56,883: INFO: main: Training : batch 1232 Loss: 0.02031036700958462]
[2024-04-17 12:45:57,523: INFO: main: Training : batch 1233 Loss: 0.013075273620604]
[2024-04-17 12:45:58,160: INFO: main: Training : batch 1234 Loss: 0.005731612616136615]
[2024-04-17 12:45:58,792: INFO: main: Training : batch 1235 Loss: 0.006210677135006104]
[2024-04-17 12:45:59,425: INFO: main: Training : batch 1236 Loss: 0.008768908858263076]
[2024-04-17 12:46:00,062: INFO: main: Training : batch 1237 Loss: 0.006512552959027816]
[2024-04-17 12:46:00,694: INFO: main: Training : batch 1238 Loss: 0.004625095373815865]
[2024-04-17 12:46:01,326: INFO: main: Training : batch 1239 Loss: 0.007125591593669886]
[2024-04-17 12:46:01,963: INFO: main: Training : batch 1240 Loss: 0.0012292013222941023]
[2024-04-17 12:46:02,600: INFO: main: Training : batch 1241 Loss: 0.008375025441301987]
[2024-04-17 12:46:03,238: INFO: main: Training : batch 1242 Loss: 0.0065620583989759625]
[2024-04-17 12:46:03,874: INFO: main: Training : batch 1243 Loss: 0.007867707350413417]
[2024-04-17 12:46:04,508: INFO: main: Training : batch 1244 Loss: 0.02969544328307273]
[2024-04-17 12:46:05,148: INFO: main: Training : batch 1245 Loss: 0.007753534280449644]
[2024-04-17 12:46:05,779: INFO: main: Training : batch 1246 Loss: 0.03620143346676931]
[2024-04-17 12:46:06,417: INFO: main: Training : batch 1247 Loss: 0.004977970537026453]
[2024-04-17 12:46:07,051: INFO: main: Training : batch 1248 Loss: 0.0065775613234274]
[2024-04-17 12:46:07,696: INFO: main: Training : batch 1249 Loss: 0.010786852751557042]
[2024-04-17 12:46:08,347: INFO: main: Training : batch 1250 Loss: 0.0015830496149807057]
[2024-04-17 12:46:08,992: INFO: main: Training : batch 1251 Loss: 0.001929408651459714]
[2024-04-17 12:46:09,638: INFO: main: Training : batch 1252 Loss: 0.006183349693206505]
[2024-04-17 12:46:10,279: INFO: main: Training : batch 1253 Loss: 0.0015338838323768348]
[2024-04-17 12:46:10,939: INFO: main: Training : batch 1254 Loss: 0.011726455157717407]
[2024-04-17 12:46:11,578: INFO: main: Training : batch 1255 Loss: 0.04124256913160327]
[2024-04-17 12:46:12,213: INFO: main: Training : batch 1256 Loss: 0.00633542111369776]
[2024-04-17 12:46:12,852: INFO: main: Training : batch 1257 Loss: 0.0168951589598741]
[2024-04-17 12:46:13,488: INFO: main: Training : batch 1258 Loss: 0.005249956716596199]
[2024-04-17 12:46:14,127: INFO: main: Training : batch 1259 Loss: 0.0058473232385906505]
[2024-04-17 12:46:14,765: INFO: main: Training : batch 1260 Loss: 0.0049682158507892305]
[2024-04-17 12:46:15,401: INFO: main: Training : batch 1261 Loss: 0.005708825341120417]
[2024-04-17 12:46:16,036: INFO: main: Training : batch 1262 Loss: 0.0011031287076999846]
[2024-04-17 12:46:16,670: INFO: main: Training : batch 1263 Loss: 0.006028685396439638]
[2024-04-17 12:46:17,308: INFO: main: Training : batch 1264 Loss: 0.008036656832175127]
[2024-04-17 12:46:17,942: INFO: main: Training : batch 1265 Loss: 0.023799955166353838]
[2024-04-17 12:46:18,577: INFO: main: Training : batch 1266 Loss: 0.003108433130960463]
[2024-04-17 12:46:19,212: INFO: main: Training : batch 1267 Loss: 0.009937834930091267]
[2024-04-17 12:46:19,845: INFO: main: Training : batch 1268 Loss: 0.008252194114916357]
[2024-04-17 12:46:20,482: INFO: main: Training : batch 1269 Loss: 0.006986172116331911]
[2024-04-17 12:46:21,126: INFO: main: Training : batch 1270 Loss: 0.011597781170067213]
[2024-04-17 12:46:21,769: INFO: main: Training : batch 1271 Loss: 0.02387711783821056]
[2024-04-17 12:46:22,415: INFO: main: Training : batch 1272 Loss: 0.007567096188908039]
[2024-04-17 12:46:23,061: INFO: main: Training : batch 1273 Loss: 0.026534327538846828]
[2024-04-17 12:46:23,709: INFO: main: Training : batch 1274 Loss: 0.005647735007753623]
[2024-04-17 12:46:24,356: INFO: main: Training : batch 1275 Loss: 0.0018895567620651065]
[2024-04-17 12:46:24,992: INFO: main: Training : batch 1276 Loss: 0.0009458671552709471]
[2024-04-17 12:46:25,625: INFO: main: Training : batch 1277 Loss: 0.02510773014423702]
[2024-04-17 12:46:26,261: INFO: main: Training : batch 1278 Loss: 0.002756937895818144]
[2024-04-17 12:46:26,897: INFO: main: Training : batch 1279 Loss: 0.004689989745487312]
[2024-04-17 12:46:27,531: INFO: main: Training : batch 1280 Loss: 0.0035787332077596556]
[2024-04-17 12:46:28,162: INFO: main: Training : batch 1281 Loss: 0.03684419720398641]
[2024-04-17 12:46:28,798: INFO: main: Training : batch 1282 Loss: 0.019545934978691226]
[2024-04-17 12:46:29,428: INFO: main: Training : batch 1283 Loss: 0.006840167421058978]
[2024-04-17 12:46:30,071: INFO: main: Training : batch 1284 Loss: 0.023979685493745843]
[2024-04-17 12:46:30,704: INFO: main: Training : batch 1285 Loss: 0.00711666626281217]
[2024-04-17 12:46:31,339: INFO: main: Training : batch 1286 Loss: 0.016424837784879173]
[2024-04-17 12:46:31,969: INFO: main: Training : batch 1287 Loss: 0.0035919384603695617]
[2024-04-17 12:46:32,605: INFO: main: Training : batch 1288 Loss: 0.0028073735143829765]
[2024-04-17 12:46:33,238: INFO: main: Training : batch 1289 Loss: 0.005671515134459947]
[2024-04-17 12:46:33,866: INFO: main: Training : batch 1290 Loss: 0.009917778707912944]
[2024-04-17 12:46:34,503: INFO: main: Training : batch 1291 Loss: 0.005963434202217696]
[2024-04-17 12:46:35,149: INFO: main: Training : batch 1292 Loss: 0.007885989288618405]
[2024-04-17 12:46:35,788: INFO: main: Training : batch 1293 Loss: 0.004306687823898384]
[2024-04-17 12:46:36,426: INFO: main: Training : batch 1294 Loss: 0.017927749333043603]
[2024-04-17 12:46:37,072: INFO: main: Training : batch 1295 Loss: 0.013596664565892404]
[2024-04-17 12:46:37,721: INFO: main: Training : batch 1296 Loss: 0.013656018202557928]
[2024-04-17 12:46:38,355: INFO: main: Training : batch 1297 Loss: 0.014032061324916715]
[2024-04-17 12:46:38,984: INFO: main: Training : batch 1298 Loss: 0.02402013184667822]
[2024-04-17 12:46:39,612: INFO: main: Training : batch 1299 Loss: 0.015241077138606443]
[2024-04-17 12:46:40,244: INFO: main: Training : batch 1300 Loss: 0.0073432656114931355]
[2024-04-17 12:46:40,876: INFO: main: Training : batch 1301 Loss: 0.00827734803056788]
[2024-04-17 12:46:41,510: INFO: main: Training : batch 1302 Loss: 0.011811784730346915]
[2024-04-17 12:46:42,138: INFO: main: Training : batch 1303 Loss: 0.0037144802048642233]
[2024-04-17 12:46:42,768: INFO: main: Training : batch 1304 Loss: 0.01141028404875261]
[2024-04-17 12:46:43,393: INFO: main: Training : batch 1305 Loss: 0.014585235363052848]
[2024-04-17 12:46:44,023: INFO: main: Training : batch 1306 Loss: 0.003045574534422923]
[2024-04-17 12:46:44,656: INFO: main: Training : batch 1307 Loss: 0.009955009720747818]
[2024-04-17 12:46:45,286: INFO: main: Training : batch 1308 Loss: 0.001337247664316558]
[2024-04-17 12:46:45,915: INFO: main: Training : batch 1309 Loss: 0.024594740832518336]
[2024-04-17 12:46:46,548: INFO: main: Training : batch 1310 Loss: 0.004638194455395082]
[2024-04-17 12:46:47,181: INFO: main: Training : batch 1311 Loss: 0.016798274168201205]
[2024-04-17 12:46:47,810: INFO: main: Training : batch 1312 Loss: 0.010257447643973542]
[2024-04-17 12:46:48,446: INFO: main: Training : batch 1313 Loss: 0.008671113523627523]
[2024-04-17 12:46:49,085: INFO: main: Training : batch 1314 Loss: 0.007222824535069998]
[2024-04-17 12:46:49,718: INFO: main: Training : batch 1315 Loss: 0.002578316692720546]
[2024-04-17 12:46:50,350: INFO: main: Training : batch 1316 Loss: 0.02355997180182021]
[2024-04-17 12:46:50,990: INFO: main: Training : batch 1317 Loss: 0.002247410005571987]
[2024-04-17 12:46:51,624: INFO: main: Training : batch 1318 Loss: 0.01085266348180776]
[2024-04-17 12:46:52,252: INFO: main: Training : batch 1319 Loss: 0.007530400502567828]
[2024-04-17 12:46:52,878: INFO: main: Training : batch 1320 Loss: 0.00507354476049959]
[2024-04-17 12:46:53,509: INFO: main: Training : batch 1321 Loss: 0.0030352878856417143]
[2024-04-17 12:46:54,139: INFO: main: Training : batch 1322 Loss: 0.0030627930994790538]
[2024-04-17 12:46:54,765: INFO: main: Training : batch 1323 Loss: 0.004485929985690109]
[2024-04-17 12:46:55,393: INFO: main: Training : batch 1324 Loss: 0.016575116854819938]
[2024-04-17 12:46:56,019: INFO: main: Training : batch 1325 Loss: 0.019004537253517233]
[2024-04-17 12:46:56,648: INFO: main: Training : batch 1326 Loss: 0.009167913296847643]
[2024-04-17 12:46:57,275: INFO: main: Training : batch 1327 Loss: 0.007967477820753409]
[2024-04-17 12:46:57,907: INFO: main: Training : batch 1328 Loss: 0.0007743516108861956]
[2024-04-17 12:46:58,537: INFO: main: Training : batch 1329 Loss: 0.010880858597740375]
[2024-04-17 12:46:59,166: INFO: main: Training : batch 1330 Loss: 0.011774644947132847]
[2024-04-17 12:46:59,797: INFO: main: Training : batch 1331 Loss: 0.0028518945712310475]
[2024-04-17 12:47:00,427: INFO: main: Training : batch 1332 Loss: 0.005189401789522685]
[2024-04-17 12:47:01,058: INFO: main: Training : batch 1333 Loss: 0.0032274212198805372]
[2024-04-17 12:47:01,696: INFO: main: Training : batch 1334 Loss: 0.011788864333521642]
[2024-04-17 12:47:02,332: INFO: main: Training : batch 1335 Loss: 0.007884740740487046]
[2024-04-17 12:47:02,964: INFO: main: Training : batch 1336 Loss: 0.009640359284750686]
[2024-04-17 12:47:03,596: INFO: main: Training : batch 1337 Loss: 0.0007287196001038743]
[2024-04-17 12:47:04,242: INFO: main: Training : batch 1338 Loss: 0.013164022042541516]
[2024-04-17 12:47:04,896: INFO: main: Training : batch 1339 Loss: 0.026975561400449134]
[2024-04-17 12:47:05,526: INFO: main: Training : batch 1340 Loss: 0.004690312070873204]
[2024-04-17 12:47:06,154: INFO: main: Training : batch 1341 Loss: 0.028732118342508925]
[2024-04-17 12:47:06,781: INFO: main: Training : batch 1342 Loss: 0.00492253154629946]
[2024-04-17 12:47:07,403: INFO: main: Training : batch 1343 Loss: 0.012412112981992362]
[2024-04-17 12:47:08,035: INFO: main: Training : batch 1344 Loss: 0.003957583534343981]
[2024-04-17 12:47:08,665: INFO: main: Training : batch 1345 Loss: 0.02253254883117228]
[2024-04-17 12:47:09,299: INFO: main: Training : batch 1346 Loss: 0.001046553135918682]
[2024-04-17 12:47:09,933: INFO: main: Training : batch 1347 Loss: 0.018795736553648076]
[2024-04-17 12:47:10,564: INFO: main: Training : batch 1348 Loss: 0.0031563494565821602]
[2024-04-17 12:47:11,195: INFO: main: Training : batch 1349 Loss: 0.00958587735484129]
[2024-04-17 12:47:11,828: INFO: main: Training : batch 1350 Loss: 0.0020617474120752837]
[2024-04-17 12:47:12,461: INFO: main: Training : batch 1351 Loss: 0.012560346445300267]
[2024-04-17 12:47:13,087: INFO: main: Training : batch 1352 Loss: 0.013999779552746948]
[2024-04-17 12:47:13,716: INFO: main: Training : batch 1353 Loss: 0.015092267329953432]
[2024-04-17 12:47:14,345: INFO: main: Training : batch 1354 Loss: 0.004822345966601591]
[2024-04-17 12:47:14,982: INFO: main: Training : batch 1355 Loss: 0.003563154716275959]
[2024-04-17 12:47:15,619: INFO: main: Training : batch 1356 Loss: 0.0065293902448578115]
[2024-04-17 12:47:16,252: INFO: main: Training : batch 1357 Loss: 0.005322104456892602]
[2024-04-17 12:47:16,891: INFO: main: Training : batch 1358 Loss: 0.02286245296845838]
[2024-04-17 12:47:17,535: INFO: main: Training : batch 1359 Loss: 0.0062360439968184135]
[2024-04-17 12:47:18,173: INFO: main: Training : batch 1360 Loss: 0.014161608715123114]
[2024-04-17 12:47:18,806: INFO: main: Training : batch 1361 Loss: 0.027871540002563363]
[2024-04-17 12:47:19,442: INFO: main: Training : batch 1362 Loss: 0.018703581688193024]
[2024-04-17 12:47:20,073: INFO: main: Training : batch 1363 Loss: 0.003532918054933593]
[2024-04-17 12:47:20,704: INFO: main: Training : batch 1364 Loss: 0.00469786831728072]
[2024-04-17 12:47:21,338: INFO: main: Training : batch 1365 Loss: 0.009431711349152931]
[2024-04-17 12:47:21,969: INFO: main: Training : batch 1366 Loss: 0.019634278271595945]
[2024-04-17 12:47:22,599: INFO: main: Training : batch 1367 Loss: 0.013935011329081344]
[2024-04-17 12:47:23,231: INFO: main: Training : batch 1368 Loss: 0.0030891393023046703]
[2024-04-17 12:47:23,866: INFO: main: Training : batch 1369 Loss: 0.010225342435493055]
[2024-04-17 12:47:24,500: INFO: main: Training : batch 1370 Loss: 0.0064696137253406226]
[2024-04-17 12:47:25,132: INFO: main: Training : batch 1371 Loss: 0.0016573661509581372]
[2024-04-17 12:47:25,766: INFO: main: Training : batch 1372 Loss: 0.003911624411362018]
[2024-04-17 12:47:26,400: INFO: main: Training : batch 1373 Loss: 0.005724285882756077]
[2024-04-17 12:47:27,033: INFO: main: Training : batch 1374 Loss: 0.0077865305195029075]
[2024-04-17 12:47:27,669: INFO: main: Training : batch 1375 Loss: 0.0020668963780107142]
[2024-04-17 12:47:28,307: INFO: main: Training : batch 1376 Loss: 0.01140159777335285]
[2024-04-17 12:47:28,949: INFO: main: Training : batch 1377 Loss: 0.0055663259916341945]
[2024-04-17 12:47:29,586: INFO: main: Training : batch 1378 Loss: 0.024294562510976322]
[2024-04-17 12:47:30,224: INFO: main: Training : batch 1379 Loss: 0.012746016314268308]
[2024-04-17 12:47:30,860: INFO: main: Training : batch 1380 Loss: 0.022201882114555725]
[2024-04-17 12:47:31,493: INFO: main: Training : batch 1381 Loss: 0.007224705612703604]
[2024-04-17 12:47:32,133: INFO: main: Training : batch 1382 Loss: 0.0027651964689426075]
[2024-04-17 12:47:32,766: INFO: main: Training : batch 1383 Loss: 0.007655573584527536]
[2024-04-17 12:47:33,402: INFO: main: Training : batch 1384 Loss: 0.006547371404864086]
[2024-04-17 12:47:34,036: INFO: main: Training : batch 1385 Loss: 0.01853519497591593]
[2024-04-17 12:47:34,667: INFO: main: Training : batch 1386 Loss: 0.00556731462305794]
[2024-04-17 12:47:35,303: INFO: main: Training : batch 1387 Loss: 0.020346129963981777]
[2024-04-17 12:47:35,934: INFO: main: Training : batch 1388 Loss: 0.017988195983018897]
[2024-04-17 12:47:36,570: INFO: main: Training : batch 1389 Loss: 0.015722131099295125]
[2024-04-17 12:47:37,205: INFO: main: Training : batch 1390 Loss: 0.007446244590426415]
[2024-04-17 12:47:37,838: INFO: main: Training : batch 1391 Loss: 0.006931412606089613]
[2024-04-17 12:47:38,473: INFO: main: Training : batch 1392 Loss: 0.008496039684094361]
[2024-04-17 12:47:39,104: INFO: main: Training : batch 1393 Loss: 0.013230397931178861]
[2024-04-17 12:47:39,735: INFO: main: Training : batch 1394 Loss: 0.009629331094717636]
[2024-04-17 12:47:40,365: INFO: main: Training : batch 1395 Loss: 0.0031201595743357425]
[2024-04-17 12:47:40,995: INFO: main: Training : batch 1396 Loss: 0.0006795079699482128]
[2024-04-17 12:47:41,627: INFO: main: Training : batch 1397 Loss: 0.019689853672895268]
[2024-04-17 12:47:42,265: INFO: main: Training : batch 1398 Loss: 0.00093434013390744]
[2024-04-17 12:47:42,905: INFO: main: Training : batch 1399 Loss: 0.0027979437792674975]
[2024-04-17 12:47:43,552: INFO: main: Training : batch 1400 Loss: 0.011006565764593107]
[2024-04-17 12:47:44,200: INFO: main: Training : batch 1401 Loss: 0.003112827928909515]
[2024-04-17 12:47:44,844: INFO: main: Training : batch 1402 Loss: 0.006489059905214835]
[2024-04-17 12:47:45,476: INFO: main: Training : batch 1403 Loss: 0.0051439450842782316]
[2024-04-17 12:47:46,106: INFO: main: Training : batch 1404 Loss: 0.006748538958070373]
[2024-04-17 12:47:46,741: INFO: main: Training : batch 1405 Loss: 0.004309533514120336]
[2024-04-17 12:47:47,369: INFO: main: Training : batch 1406 Loss: 0.00478144374057804]
[2024-04-17 12:47:48,004: INFO: main: Training : batch 1407 Loss: 0.013015533795010609]
[2024-04-17 12:47:48,639: INFO: main: Training : batch 1408 Loss: 0.029909008368551086]
[2024-04-17 12:47:49,271: INFO: main: Training : batch 1409 Loss: 0.025627599650493077]
[2024-04-17 12:47:49,902: INFO: main: Training : batch 1410 Loss: 0.005385663748054723]
[2024-04-17 12:47:50,536: INFO: main: Training : batch 1411 Loss: 0.0028170077069098578]
[2024-04-17 12:47:51,172: INFO: main: Training : batch 1412 Loss: 0.006885711774956496]
[2024-04-17 12:47:51,805: INFO: main: Training : batch 1413 Loss: 0.004534092042089965]
[2024-04-17 12:47:52,433: INFO: main: Training : batch 1414 Loss: 0.009769096004269694]
[2024-04-17 12:47:53,063: INFO: main: Training : batch 1415 Loss: 0.0048830631427889595]
[2024-04-17 12:47:53,691: INFO: main: Training : batch 1416 Loss: 0.007984723420239398]
[2024-04-17 12:47:54,323: INFO: main: Training : batch 1417 Loss: 0.02604196529688174]
[2024-04-17 12:47:54,959: INFO: main: Training : batch 1418 Loss: 0.004156165170850067]
[2024-04-17 12:47:55,595: INFO: main: Training : batch 1419 Loss: 0.0005523616344018972]
[2024-04-17 12:47:56,231: INFO: main: Training : batch 1420 Loss: 0.005406262911075724]
[2024-04-17 12:47:56,869: INFO: main: Training : batch 1421 Loss: 0.01990408405510302]
[2024-04-17 12:47:57,512: INFO: main: Training : batch 1422 Loss: 0.004254797518685559]
[2024-04-17 12:47:58,145: INFO: main: Training : batch 1423 Loss: 0.010460075138550816]
[2024-04-17 12:47:58,777: INFO: main: Training : batch 1424 Loss: 0.0025618089773154994]
[2024-04-17 12:47:59,402: INFO: main: Training : batch 1425 Loss: 0.004031458259562729]
[2024-04-17 12:48:00,032: INFO: main: Training : batch 1426 Loss: 0.013761080364988782]
[2024-04-17 12:48:00,665: INFO: main: Training : batch 1427 Loss: 0.00295845622496057]
[2024-04-17 12:48:01,294: INFO: main: Training : batch 1428 Loss: 0.01143877197321696]
[2024-04-17 12:48:01,927: INFO: main: Training : batch 1429 Loss: 0.024210141367630236]
[2024-04-17 12:48:02,126: INFO: main: Eval Epoch : batch 0 Loss: 0.004984929303294404]
[2024-04-17 12:48:02,326: INFO: main: Eval Epoch : batch 1 Loss: 0.002971565160375737]
[2024-04-17 12:48:02,527: INFO: main: Eval Epoch : batch 2 Loss: 0.03961970315815556]
[2024-04-17 12:48:02,731: INFO: main: Eval Epoch : batch 3 Loss: 0.00922984043578296]
[2024-04-17 12:48:02,933: INFO: main: Eval Epoch : batch 4 Loss: 0.002621484839288458]
[2024-04-17 12:48:03,132: INFO: main: Eval Epoch : batch 5 Loss: 0.012511675987771257]
[2024-04-17 12:48:03,332: INFO: main: Eval Epoch : batch 6 Loss: 0.011319710095917968]
[2024-04-17 12:48:03,533: INFO: main: Eval Epoch : batch 7 Loss: 0.0057264818239524465]
[2024-04-17 12:48:03,731: INFO: main: Eval Epoch : batch 8 Loss: 0.011168403868340586]
[2024-04-17 12:48:03,933: INFO: main: Eval Epoch : batch 9 Loss: 0.018735285732631817]
[2024-04-17 12:48:04,134: INFO: main: Eval Epoch : batch 10 Loss: 0.012986566838949875]
[2024-04-17 12:48:04,334: INFO: main: Eval Epoch : batch 11 Loss: 0.04500271792534457]
[2024-04-17 12:48:04,533: INFO: main: Eval Epoch : batch 12 Loss: 0.012404800308383333]
[2024-04-17 12:48:04,731: INFO: main: Eval Epoch : batch 13 Loss: 0.014885357999636141]
[2024-04-17 12:48:04,932: INFO: main: Eval Epoch : batch 14 Loss: 0.00760610689112982]
[2024-04-17 12:48:05,131: INFO: main: Eval Epoch : batch 15 Loss: 0.012980860777181443]
[2024-04-17 12:48:05,333: INFO: main: Eval Epoch : batch 16 Loss: 0.017353613627587524]
[2024-04-17 12:48:05,534: INFO: main: Eval Epoch : batch 17 Loss: 0.027694509028386806]
[2024-04-17 12:48:05,735: INFO: main: Eval Epoch : batch 18 Loss: 0.00705367964862179]
[2024-04-17 12:48:05,936: INFO: main: Eval Epoch : batch 19 Loss: 0.01626770670053541]
[2024-04-17 12:48:06,138: INFO: main: Eval Epoch : batch 20 Loss: 0.0023967432175616586]
[2024-04-17 12:48:06,339: INFO: main: Eval Epoch : batch 21 Loss: 0.0077569021283003916]
[2024-04-17 12:48:06,541: INFO: main: Eval Epoch : batch 22 Loss: 0.012234926399624528]
[2024-04-17 12:48:06,741: INFO: main: Eval Epoch : batch 23 Loss: 0.018835043693206047]
[2024-04-17 12:48:06,942: INFO: main: Eval Epoch : batch 24 Loss: 0.028520716786824156]
[2024-04-17 12:48:07,142: INFO: main: Eval Epoch : batch 25 Loss: 0.006275890641647021]
[2024-04-17 12:48:07,341: INFO: main: Eval Epoch : batch 26 Loss: 0.004670717626748625]
[2024-04-17 12:48:07,546: INFO: main: Eval Epoch : batch 27 Loss: 0.00934638714217878]
[2024-04-17 12:48:07,744: INFO: main: Eval Epoch : batch 28 Loss: 0.005188604148498895]
[2024-04-17 12:48:07,945: INFO: main: Eval Epoch : batch 29 Loss: 0.005256951162257124]
[2024-04-17 12:48:08,148: INFO: main: Eval Epoch : batch 30 Loss: 0.0064756292953399375]
[2024-04-17 12:48:08,354: INFO: main: Eval Epoch : batch 31 Loss: 0.04196202397605651]
[2024-04-17 12:48:08,560: INFO: main: Eval Epoch : batch 32 Loss: 0.019507157839078814]
[2024-04-17 12:48:08,764: INFO: main: Eval Epoch : batch 33 Loss: 0.005136470031907382]
[2024-04-17 12:48:08,968: INFO: main: Eval Epoch : batch 34 Loss: 0.018618860459627556]
[2024-04-17 12:48:09,172: INFO: main: Eval Epoch : batch 35 Loss: 0.019361984187656653]
[2024-04-17 12:48:09,377: INFO: main: Eval Epoch : batch 36 Loss: 0.012460759810203927]
[2024-04-17 12:48:09,582: INFO: main: Eval Epoch : batch 37 Loss: 0.04264495738237447]
[2024-04-17 12:48:09,788: INFO: main: Eval Epoch : batch 38 Loss: 0.015115426828225172]
[2024-04-17 12:48:10,002: INFO: main: Eval Epoch : batch 39 Loss: 0.005308572653918725]
[2024-04-17 12:48:10,205: INFO: main: Eval Epoch : batch 40 Loss: 0.008623667729740997]
[2024-04-17 12:48:10,412: INFO: main: Eval Epoch : batch 41 Loss: 0.029121369592844858]
[2024-04-17 12:48:10,619: INFO: main: Eval Epoch : batch 42 Loss: 0.005657942000506632]
[2024-04-17 12:48:10,828: INFO: main: Eval Epoch : batch 43 Loss: 0.007084598600685192]
[2024-04-17 12:48:11,031: INFO: main: Eval Epoch : batch 44 Loss: 0.01999553073798704]
[2024-04-17 12:48:11,235: INFO: main: Eval Epoch : batch 45 Loss: 0.009161560018865075]
[2024-04-17 12:48:11,438: INFO: main: Eval Epoch : batch 46 Loss: 0.011531388234532397]
[2024-04-17 12:48:11,643: INFO: main: Eval Epoch : batch 47 Loss: 0.0433032852705595]
[2024-04-17 12:48:11,851: INFO: main: Eval Epoch : batch 48 Loss: 0.019976674775876748]
[2024-04-17 12:48:12,052: INFO: main: Eval Epoch : batch 49 Loss: 0.00866399458503857]
[2024-04-17 12:48:12,253: INFO: main: Eval Epoch : batch 50 Loss: 0.015179619452996644]
[2024-04-17 12:48:12,454: INFO: main: Eval Epoch : batch 51 Loss: 0.047147113675127195]
[2024-04-17 12:48:12,657: INFO: main: Eval Epoch : batch 52 Loss: 0.013987697282409698]
[2024-04-17 12:48:12,858: INFO: main: Eval Epoch : batch 53 Loss: 0.010695647475575045]
[2024-04-17 12:48:13,058: INFO: main: Eval Epoch : batch 54 Loss: 0.018272053663657813]
[2024-04-17 12:48:13,256: INFO: main: Eval Epoch : batch 55 Loss: 0.02843298520295891]
[2024-04-17 12:48:13,455: INFO: main: Eval Epoch : batch 56 Loss: 0.002987489453092052]
[2024-04-17 12:48:13,655: INFO: main: Eval Epoch : batch 57 Loss: 0.014044906234046537]
[2024-04-17 12:48:13,853: INFO: main: Eval Epoch : batch 58 Loss: 0.0011002010099632157]
[2024-04-17 12:48:14,052: INFO: main: Eval Epoch : batch 59 Loss: 0.02346546676651065]
[2024-04-17 12:48:14,254: INFO: main: Eval Epoch : batch 60 Loss: 0.005111440462419986]
[2024-04-17 12:48:14,455: INFO: main: Eval Epoch : batch 61 Loss: 0.007881478738159114]
[2024-04-17 12:48:14,658: INFO: main: Eval Epoch : batch 62 Loss: 0.005622048901785099]
[2024-04-17 12:48:14,860: INFO: main: Eval Epoch : batch 63 Loss: 0.004021685116390497]
[2024-04-17 12:48:15,059: INFO: main: Eval Epoch : batch 64 Loss: 0.003410826671939613]
[2024-04-17 12:48:15,262: INFO: main: Eval Epoch : batch 65 Loss: 0.008551407500190683]
[2024-04-17 12:48:15,461: INFO: main: Eval Epoch : batch 66 Loss: 0.0005006458328735654]
[2024-04-17 12:48:15,662: INFO: main: Eval Epoch : batch 67 Loss: 0.011254896984514347]
[2024-04-17 12:48:15,862: INFO: main: Eval Epoch : batch 68 Loss: 0.05538338598025745]
[2024-04-17 12:48:16,063: INFO: main: Eval Epoch : batch 69 Loss: 0.01577614452083769]
[2024-04-17 12:48:16,262: INFO: main: Eval Epoch : batch 70 Loss: 0.0024054338482825265]
[2024-04-17 12:48:16,464: INFO: main: Eval Epoch : batch 71 Loss: 0.005768955220852396]
[2024-04-17 12:48:16,667: INFO: main: Eval Epoch : batch 72 Loss: 0.02476464768692012]
[2024-04-17 12:48:16,870: INFO: main: Eval Epoch : batch 73 Loss: 0.012625366403678381]
[2024-04-17 12:48:17,070: INFO: main: Eval Epoch : batch 74 Loss: 0.007930895809246535]
[2024-04-17 12:48:17,269: INFO: main: Eval Epoch : batch 75 Loss: 0.01825760541387583]
[2024-04-17 12:48:17,468: INFO: main: Eval Epoch : batch 76 Loss: 0.0048508976925244925]
[2024-04-17 12:48:17,669: INFO: main: Eval Epoch : batch 77 Loss: 0.0004672797412383685]
[2024-04-17 12:48:17,871: INFO: main: Eval Epoch : batch 78 Loss: 0.03817320534814686]
[2024-04-17 12:48:18,072: INFO: main: Eval Epoch : batch 79 Loss: 0.016653951300258384]
[2024-04-17 12:48:18,280: INFO: main: Eval Epoch : batch 80 Loss: 0.01052977625172032]
[2024-04-17 12:48:18,482: INFO: main: Eval Epoch : batch 81 Loss: 0.019301585614279422]
[2024-04-17 12:48:18,681: INFO: main: Eval Epoch : batch 82 Loss: 0.004102892072739859]
[2024-04-17 12:48:18,881: INFO: main: Eval Epoch : batch 83 Loss: 0.00782392861247421]
[2024-04-17 12:48:19,085: INFO: main: Eval Epoch : batch 84 Loss: 0.0004707913394969017]
[2024-04-17 12:48:19,283: INFO: main: Eval Epoch : batch 85 Loss: 0.03671780190927585]
[2024-04-17 12:48:19,483: INFO: main: Eval Epoch : batch 86 Loss: 0.024335894842544992]
[2024-04-17 12:48:19,683: INFO: main: Eval Epoch : batch 87 Loss: 0.006848531203174819]
[2024-04-17 12:48:19,886: INFO: main: Eval Epoch : batch 88 Loss: 0.015102367819587797]
[2024-04-17 12:48:20,091: INFO: main: Eval Epoch : batch 89 Loss: 0.014096211379161934]
[2024-04-17 12:48:20,292: INFO: main: Eval Epoch : batch 90 Loss: 0.026448240512343145]
[2024-04-17 12:48:20,492: INFO: main: Eval Epoch : batch 91 Loss: 0.005882948342718062]
[2024-04-17 12:48:20,690: INFO: main: Eval Epoch : batch 92 Loss: 0.0005116679427799045]
[2024-04-17 12:48:20,893: INFO: main: Eval Epoch : batch 93 Loss: 0.006618615604097078]
[2024-04-17 12:48:21,095: INFO: main: Eval Epoch : batch 94 Loss: 0.02856172806890159]
[2024-04-17 12:48:21,297: INFO: main: Eval Epoch : batch 95 Loss: 0.011028759021336718]
[2024-04-17 12:48:21,498: INFO: main: Eval Epoch : batch 96 Loss: 0.008823663931403339]
[2024-04-17 12:48:21,696: INFO: main: Eval Epoch : batch 97 Loss: 0.006415893855220025]
[2024-04-17 12:48:21,897: INFO: main: Eval Epoch : batch 98 Loss: 0.010905409193187023]
[2024-04-17 12:48:22,104: INFO: main: Eval Epoch : batch 99 Loss: 0.009952347576084007]
[2024-04-17 12:48:22,310: INFO: main: Eval Epoch : batch 100 Loss: 0.015744912923473686]
[2024-04-17 12:48:22,513: INFO: main: Eval Epoch : batch 101 Loss: 0.01574078550075483]
[2024-04-17 12:48:22,717: INFO: main: Eval Epoch : batch 102 Loss: 0.009514543738608593]
[2024-04-17 12:48:22,922: INFO: main: Eval Epoch : batch 103 Loss: 0.0021729768011449165]
[2024-04-17 12:48:23,129: INFO: main: Eval Epoch : batch 104 Loss: 0.015252338539121887]
[2024-04-17 12:48:23,336: INFO: main: Eval Epoch : batch 105 Loss: 0.0022865522679386767]
[2024-04-17 12:48:23,542: INFO: main: Eval Epoch : batch 106 Loss: 0.007137710696253897]
[2024-04-17 12:48:23,748: INFO: main: Eval Epoch : batch 107 Loss: 0.0026651527593691093]
[2024-04-17 12:48:23,954: INFO: main: Eval Epoch : batch 108 Loss: 0.028245194925086942]
[2024-04-17 12:48:24,161: INFO: main: Eval Epoch : batch 109 Loss: 0.005587644239638444]
[2024-04-17 12:48:24,371: INFO: main: Eval Epoch : batch 110 Loss: 0.007485900593717845]
[2024-04-17 12:48:24,581: INFO: main: Eval Epoch : batch 111 Loss: 0.003154497122428783]
[2024-04-17 12:48:24,784: INFO: main: Eval Epoch : batch 112 Loss: 0.020594878946624314]
[2024-04-17 12:48:24,987: INFO: main: Eval Epoch : batch 113 Loss: 0.007668934793663523]
[2024-04-17 12:48:25,192: INFO: main: Eval Epoch : batch 114 Loss: 0.021275948651056618]
[2024-04-17 12:48:25,401: INFO: main: Eval Epoch : batch 115 Loss: 0.008680007216348309]
[2024-04-17 12:48:25,607: INFO: main: Eval Epoch : batch 116 Loss: 0.014294489395821381]
[2024-04-17 12:48:25,811: INFO: main: Eval Epoch : batch 117 Loss: 0.0027872337630994075]
[2024-04-17 12:48:26,012: INFO: main: Eval Epoch : batch 118 Loss: 0.008210806105274762]
[2024-04-17 12:48:26,216: INFO: main: Eval Epoch : batch 119 Loss: 0.025915140644444612]
[2024-04-17 12:48:26,419: INFO: main: Eval Epoch : batch 120 Loss: 0.001434823736299726]
[2024-04-17 12:48:26,624: INFO: main: Eval Epoch : batch 121 Loss: 0.0075059903344546625]
[2024-04-17 12:48:26,827: INFO: main: Eval Epoch : batch 122 Loss: 0.016900043798558625]
[2024-04-17 12:48:27,027: INFO: main: Eval Epoch : batch 123 Loss: 0.018695962313688277]
[2024-04-17 12:48:27,228: INFO: main: Eval Epoch : batch 124 Loss: 0.0014843940205312052]
[2024-04-17 12:48:27,431: INFO: main: Eval Epoch : batch 125 Loss: 0.010399346438202842]
[2024-04-17 12:48:27,633: INFO: main: Eval Epoch : batch 126 Loss: 0.004637141006716627]
[2024-04-17 12:48:27,835: INFO: main: Eval Epoch : batch 127 Loss: 0.0071192704419302325]
[2024-04-17 12:48:28,034: INFO: main: Eval Epoch : batch 128 Loss: 0.013342752909393152]
[2024-04-17 12:48:28,237: INFO: main: Eval Epoch : batch 129 Loss: 0.013055842524107698]
[2024-04-17 12:48:28,438: INFO: main: Eval Epoch : batch 130 Loss: 0.011586197114235116]
[2024-04-17 12:48:28,639: INFO: main: Eval Epoch : batch 131 Loss: 0.016520886245330036]
[2024-04-17 12:48:28,841: INFO: main: Eval Epoch : batch 132 Loss: 0.003271342109596431]
[2024-04-17 12:48:29,040: INFO: main: Eval Epoch : batch 133 Loss: 0.00400100618070373]
[2024-04-17 12:48:29,240: INFO: main: Eval Epoch : batch 134 Loss: 0.004449177196345652]
[2024-04-17 12:48:29,440: INFO: main: Eval Epoch : batch 135 Loss: 0.00458754727823592]
[2024-04-17 12:48:29,644: INFO: main: Eval Epoch : batch 136 Loss: 0.01748252931304114]
[2024-04-17 12:48:29,845: INFO: main: Eval Epoch : batch 137 Loss: 0.019858978071520324]
[2024-04-17 12:48:30,048: INFO: main: Eval Epoch : batch 138 Loss: 0.013786759682534308]
[2024-04-17 12:48:30,248: INFO: main: Eval Epoch : batch 139 Loss: 0.09096404292326517]
[2024-04-17 12:48:30,449: INFO: main: Eval Epoch : batch 140 Loss: 0.025515070320428053]
[2024-04-17 12:48:30,652: INFO: main: Eval Epoch : batch 141 Loss: 0.019140522590178913]
[2024-04-17 12:48:30,854: INFO: main: Eval Epoch : batch 142 Loss: 0.004829553801106854]
[2024-04-17 12:48:31,056: INFO: main: Eval Epoch : batch 143 Loss: 0.01704773540235412]
[2024-04-17 12:48:31,260: INFO: main: Eval Epoch : batch 144 Loss: 0.019794264670460616]
[2024-04-17 12:48:31,458: INFO: main: Eval Epoch : batch 145 Loss: 0.009565638158600457]
[2024-04-17 12:48:31,663: INFO: main: Eval Epoch : batch 146 Loss: 0.017982374853984507]
[2024-04-17 12:48:31,867: INFO: main: Eval Epoch : batch 147 Loss: 0.031113715407476723]
[2024-04-17 12:48:32,069: INFO: main: Eval Epoch : batch 148 Loss: 0.02003035044103905]
[2024-04-17 12:48:32,270: INFO: main: Eval Epoch : batch 149 Loss: 0.005769072894211195]
[2024-04-17 12:48:32,474: INFO: main: Eval Epoch : batch 150 Loss: 0.05006315779214621]
[2024-04-17 12:48:32,675: INFO: main: Eval Epoch : batch 151 Loss: 0.042735342861365815]
[2024-04-17 12:48:32,881: INFO: main: Eval Epoch : batch 152 Loss: 0.0012471625030324091]
[2024-04-17 12:48:33,081: INFO: main: Eval Epoch : batch 153 Loss: 0.004143079208256965]
[2024-04-17 12:48:33,283: INFO: main: Eval Epoch : batch 154 Loss: 0.01779777642024038]
[2024-04-17 12:48:33,488: INFO: main: Eval Epoch : batch 155 Loss: 0.05013127015539991]
[2024-04-17 12:48:33,689: INFO: main: Eval Epoch : batch 156 Loss: 0.019311672538239687]
[2024-04-17 12:48:33,894: INFO: main: Eval Epoch : batch 157 Loss: 0.012368452802203521]
[2024-04-17 12:48:34,094: INFO: main: Eval Epoch : batch 158 Loss: 0.012121511118589038]
[2024-04-17 12:48:34,298: INFO: main: Eval Epoch : batch 159 Loss: 0.004331092692862082]
[2024-04-17 12:48:34,500: INFO: main: Eval Epoch : batch 160 Loss: 0.006412770734378693]
[2024-04-17 12:48:34,701: INFO: main: Eval Epoch : batch 161 Loss: 0.019575447779105193]
[2024-04-17 12:48:34,904: INFO: main: Eval Epoch : batch 162 Loss: 0.014839035148997834]
[2024-04-17 12:48:35,104: INFO: main: Eval Epoch : batch 163 Loss: 0.011217620204055959]
[2024-04-17 12:48:35,308: INFO: main: Eval Epoch : batch 164 Loss: 0.0521846862655602]
[2024-04-17 12:48:35,509: INFO: main: Eval Epoch : batch 165 Loss: 0.011127992362098866]
[2024-04-17 12:48:35,720: INFO: main: Eval Epoch : batch 166 Loss: 0.018681367898526282]
[2024-04-17 12:48:35,927: INFO: main: Eval Epoch : batch 167 Loss: 0.0030033177224927897]
[2024-04-17 12:48:36,131: INFO: main: Eval Epoch : batch 168 Loss: 0.010674319949755703]
[2024-04-17 12:48:36,338: INFO: main: Eval Epoch : batch 169 Loss: 0.015726140806052735]
[2024-04-17 12:48:36,546: INFO: main: Eval Epoch : batch 170 Loss: 0.012243352136059309]
[2024-04-17 12:48:36,759: INFO: main: Eval Epoch : batch 171 Loss: 0.009239867192681937]
[2024-04-17 12:48:36,966: INFO: main: Eval Epoch : batch 172 Loss: 0.025260592263428094]
[2024-04-17 12:48:37,171: INFO: main: Eval Epoch : batch 173 Loss: 0.0010035731808891637]
[2024-04-17 12:48:37,377: INFO: main: Eval Epoch : batch 174 Loss: 0.0152931629352984]
[2024-04-17 12:48:37,581: INFO: main: Eval Epoch : batch 175 Loss: 0.0038015641412471675]
[2024-04-17 12:48:37,794: INFO: main: Eval Epoch : batch 176 Loss: 0.00981120582886186]
[2024-04-17 12:48:38,000: INFO: main: Eval Epoch : batch 177 Loss: 0.0013171450395491108]
[2024-04-17 12:48:38,209: INFO: main: Eval Epoch : batch 178 Loss: 0.0021609884022707753]
[2024-04-17 12:48:38,415: INFO: main: Eval Epoch : batch 179 Loss: 0.0007659347646141547]
[2024-04-17 12:48:38,625: INFO: main: Eval Epoch : batch 180 Loss: 0.016260188766687583]
[2024-04-17 12:48:38,835: INFO: main: Eval Epoch : batch 181 Loss: 0.00582392717053915]
[2024-04-17 12:48:39,054: INFO: main: Eval Epoch : batch 182 Loss: 0.012089980393268276]
[2024-04-17 12:48:39,261: INFO: main: Eval Epoch : batch 183 Loss: 0.015001661288484958]
[2024-04-17 12:48:39,470: INFO: main: Eval Epoch : batch 184 Loss: 0.01811081790550349]
[2024-04-17 12:48:39,671: INFO: main: Eval Epoch : batch 185 Loss: 0.004003553406009291]
[2024-04-17 12:48:39,878: INFO: main: Eval Epoch : batch 186 Loss: 0.007107558799242525]
[2024-04-17 12:48:40,080: INFO: main: Eval Epoch : batch 187 Loss: 0.07004616794541559]
[2024-04-17 12:48:40,282: INFO: main: Eval Epoch : batch 188 Loss: 0.009113163929046894]
[2024-04-17 12:48:40,484: INFO: main: Eval Epoch : batch 189 Loss: 0.013785936863438415]
[2024-04-17 12:48:40,684: INFO: main: Eval Epoch : batch 190 Loss: 0.007179936052014902]
[2024-04-17 12:48:40,891: INFO: main: Eval Epoch : batch 191 Loss: 0.023247627460933985]
[2024-04-17 12:48:41,094: INFO: main: Eval Epoch : batch 192 Loss: 0.008476897131055176]
[2024-04-17 12:48:41,296: INFO: main: Eval Epoch : batch 193 Loss: 0.005637083158425317]
[2024-04-17 12:48:41,500: INFO: main: Eval Epoch : batch 194 Loss: 0.016948031487679448]
[2024-04-17 12:48:41,704: INFO: main: Eval Epoch : batch 195 Loss: 0.008596627052079945]
[2024-04-17 12:48:41,909: INFO: main: Eval Epoch : batch 196 Loss: 0.009391464133306767]
[2024-04-17 12:48:42,114: INFO: main: Eval Epoch : batch 197 Loss: 0.007602141337713294]
[2024-04-17 12:48:42,316: INFO: main: Eval Epoch : batch 198 Loss: 0.004803671511205715]
[2024-04-17 12:48:42,518: INFO: main: Eval Epoch : batch 199 Loss: 0.0025080280804463417]
[2024-04-17 12:48:42,720: INFO: main: Eval Epoch : batch 200 Loss: 0.003225557220856003]
[2024-04-17 12:48:42,921: INFO: main: Eval Epoch : batch 201 Loss: 0.035598031803154385]
[2024-04-17 12:48:43,130: INFO: main: Eval Epoch : batch 202 Loss: 0.02036162499523892]
[2024-04-17 12:48:43,331: INFO: main: Eval Epoch : batch 203 Loss: 0.009116592218628178]
[2024-04-17 12:48:43,534: INFO: main: Eval Epoch : batch 204 Loss: 0.01821266625438901]
[2024-04-17 12:48:43,734: INFO: main: Eval Epoch : batch 205 Loss: 0.0039715235590545225]
[2024-04-17 12:48:43,937: INFO: main: Eval Epoch : batch 206 Loss: 0.001577756070891012]
[2024-04-17 12:48:44,143: INFO: main: Eval Epoch : batch 207 Loss: 0.0023355988310695933]
[2024-04-17 12:48:44,345: INFO: main: Eval Epoch : batch 208 Loss: 0.0012987385206779567]
[2024-04-17 12:48:44,549: INFO: main: Eval Epoch : batch 209 Loss: 0.0017824358585768452]
[2024-04-17 12:48:44,750: INFO: main: Eval Epoch : batch 210 Loss: 0.0082527019114772]
[2024-04-17 12:48:44,950: INFO: main: Eval Epoch : batch 211 Loss: 0.01700295140385368]
[2024-04-17 12:48:45,157: INFO: main: Eval Epoch : batch 212 Loss: 0.008674408918126743]
[2024-04-17 12:48:45,360: INFO: main: Eval Epoch : batch 213 Loss: 0.018700271776412786]
[2024-04-17 12:48:45,568: INFO: main: Eval Epoch : batch 214 Loss: 0.012016688327810865]
[2024-04-17 12:48:45,770: INFO: main: Eval Epoch : batch 215 Loss: 0.02641569198040791]
[2024-04-17 12:48:45,976: INFO: main: Eval Epoch : batch 216 Loss: 0.01342204239175785]
[2024-04-17 12:48:46,180: INFO: main: Eval Epoch : batch 217 Loss: 0.015944096482139992]
[2024-04-17 12:48:46,389: INFO: main: Eval Epoch : batch 218 Loss: 0.02124087765344537]
[2024-04-17 12:48:46,590: INFO: main: Eval Epoch : batch 219 Loss: 0.0007629656096959486]
[2024-04-17 12:48:46,790: INFO: main: Eval Epoch : batch 220 Loss: 0.0037068226571332014]
[2024-04-17 12:48:46,994: INFO: main: Eval Epoch : batch 221 Loss: 0.02225461646681395]
[2024-04-17 12:48:47,197: INFO: main: Eval Epoch : batch 222 Loss: 0.0002116792334280794]
[2024-04-17 12:48:47,404: INFO: main: Eval Epoch : batch 223 Loss: 0.02397825562406853]
[2024-04-17 12:48:47,605: INFO: main: Eval Epoch : batch 224 Loss: 0.0028456560512678066]
[2024-04-17 12:48:47,805: INFO: main: Eval Epoch : batch 225 Loss: 0.028136787484303226]
[2024-04-17 12:48:48,008: INFO: main: Eval Epoch : batch 226 Loss: 0.03385208949704173]
[2024-04-17 12:48:48,210: INFO: main: Eval Epoch : batch 227 Loss: 0.017672932999549064]
[2024-04-17 12:48:48,416: INFO: main: Eval Epoch : batch 228 Loss: 0.00601731749222951]
[2024-04-17 12:48:48,620: INFO: main: Eval Epoch : batch 229 Loss: 0.0020295138597622793]
[2024-04-17 12:48:48,820: INFO: main: Eval Epoch : batch 230 Loss: 0.0033259125701249745]
[2024-04-17 12:48:49,020: INFO: main: Eval Epoch : batch 231 Loss: 0.02552237819936312]
[2024-04-17 12:48:49,222: INFO: main: Eval Epoch : batch 232 Loss: 0.01425307968527513]
[2024-04-17 12:48:49,431: INFO: main: Eval Epoch : batch 233 Loss: 0.01161067364047517]
[2024-04-17 12:48:49,636: INFO: main: Eval Epoch : batch 234 Loss: 0.016625247592857353]
[2024-04-17 12:48:49,840: INFO: main: Eval Epoch : batch 235 Loss: 0.006300864614295]
[2024-04-17 12:48:50,046: INFO: main: Eval Epoch : batch 236 Loss: 0.036304338733753126]
[2024-04-17 12:48:50,252: INFO: main: Eval Epoch : batch 237 Loss: 0.0009266494764586668]
[2024-04-17 12:48:50,460: INFO: main: Eval Epoch : batch 238 Loss: 0.004726386630516474]
[2024-04-17 12:48:50,668: INFO: main: Eval Epoch : batch 239 Loss: 0.02345030507358491]
[2024-04-17 12:48:50,877: INFO: main: Eval Epoch : batch 240 Loss: 0.010041597727995212]
[2024-04-17 12:48:51,089: INFO: main: Eval Epoch : batch 241 Loss: 0.007888339214967083]
[2024-04-17 12:48:51,299: INFO: main: Eval Epoch : batch 242 Loss: 0.030855251771893518]
[2024-04-17 12:48:51,506: INFO: main: Eval Epoch : batch 243 Loss: 0.008562874116129665]
[2024-04-17 12:48:51,725: INFO: main: Eval Epoch : batch 244 Loss: 0.0045598672985487825]
[2024-04-17 12:48:51,933: INFO: main: Eval Epoch : batch 245 Loss: 0.005166834543314021]
[2024-04-17 12:48:52,141: INFO: main: Eval Epoch : batch 246 Loss: 0.006177995469891207]
[2024-04-17 12:48:52,348: INFO: main: Eval Epoch : batch 247 Loss: 0.0067031516769861235]
[2024-04-17 12:48:52,563: INFO: main: Eval Epoch : batch 248 Loss: 0.005412598342620241]
[2024-04-17 12:48:52,772: INFO: main: Eval Epoch : batch 249 Loss: 0.012711357997260447]
[2024-04-17 12:48:52,985: INFO: main: Eval Epoch : batch 250 Loss: 0.0008303347808404164]
[2024-04-17 12:48:53,190: INFO: main: Eval Epoch : batch 251 Loss: 0.011655884996505583]
[2024-04-17 12:48:53,399: INFO: main: Eval Epoch : batch 252 Loss: 0.11499592649670749]
[2024-04-17 12:48:53,603: INFO: main: Eval Epoch : batch 253 Loss: 0.01589454595611032]
[2024-04-17 12:48:53,808: INFO: main: Eval Epoch : batch 254 Loss: 0.0175319884342593]
[2024-04-17 12:48:54,011: INFO: main: Eval Epoch : batch 255 Loss: 0.005897951617198957]
[2024-04-17 12:48:54,213: INFO: main: Eval Epoch : batch 256 Loss: 0.012654751559446445]
[2024-04-17 12:48:54,417: INFO: main: Eval Epoch : batch 257 Loss: 0.03217732074596835]
[2024-04-17 12:48:54,621: INFO: main: Eval Epoch : batch 258 Loss: 0.051665030293651625]
[2024-04-17 12:48:54,824: INFO: main: Eval Epoch : batch 259 Loss: 0.011442204918285938]
[2024-04-17 12:48:55,026: INFO: main: Eval Epoch : batch 260 Loss: 0.023003063020312225]
[2024-04-17 12:48:55,225: INFO: main: Eval Epoch : batch 261 Loss: 0.017231016342115046]
[2024-04-17 12:48:55,425: INFO: main: Eval Epoch : batch 262 Loss: 0.010826928646843056]
[2024-04-17 12:48:55,630: INFO: main: Eval Epoch : batch 263 Loss: 0.013491468554653786]
[2024-04-17 12:48:55,833: INFO: main: Eval Epoch : batch 264 Loss: 0.04147129996539072]
[2024-04-17 12:48:56,036: INFO: main: Eval Epoch : batch 265 Loss: 0.0541223557246556]
[2024-04-17 12:48:56,237: INFO: main: Eval Epoch : batch 266 Loss: 0.011147756204228417]
[2024-04-17 12:48:56,439: INFO: main: Eval Epoch : batch 267 Loss: 0.0056966572672321695]
[2024-04-17 12:48:56,643: INFO: main: Eval Epoch : batch 268 Loss: 0.010106762049088667]
[2024-04-17 12:48:56,848: INFO: main: Eval Epoch : batch 269 Loss: 0.020754164976360005]
[2024-04-17 12:48:57,051: INFO: main: Eval Epoch : batch 270 Loss: 0.02903355350955367]
[2024-04-17 12:48:57,255: INFO: main: Eval Epoch : batch 271 Loss: 0.002841394087761728]
[2024-04-17 12:48:57,459: INFO: main: Eval Epoch : batch 272 Loss: 0.02351943353361081]
[2024-04-17 12:48:57,662: INFO: main: Eval Epoch : batch 273 Loss: 0.0012458932797552668]
[2024-04-17 12:48:57,868: INFO: main: Eval Epoch : batch 274 Loss: 0.0027795863791295673]
[2024-04-17 12:48:58,069: INFO: main: Eval Epoch : batch 275 Loss: 0.00039160075987702884]
[2024-04-17 12:48:58,270: INFO: main: Eval Epoch : batch 276 Loss: 0.010659351739596541]
[2024-04-17 12:48:58,471: INFO: main: Eval Epoch : batch 277 Loss: 0.013108450497177751]
[2024-04-17 12:48:58,675: INFO: main: Eval Epoch : batch 278 Loss: 0.016795051403421263]
[2024-04-17 12:48:58,878: INFO: main: Eval Epoch : batch 279 Loss: 0.00916521066796886]
[2024-04-17 12:48:59,080: INFO: main: Eval Epoch : batch 280 Loss: 0.008594631483333007]
[2024-04-17 12:48:59,281: INFO: main: Eval Epoch : batch 281 Loss: 0.004692923847207931]
[2024-04-17 12:48:59,482: INFO: main: Eval Epoch : batch 282 Loss: 0.006572827321301293]
[2024-04-17 12:48:59,685: INFO: main: Eval Epoch : batch 283 Loss: 0.0059862586689416394]
[2024-04-17 12:48:59,889: INFO: main: Eval Epoch : batch 284 Loss: 0.008266183262698349]
[2024-04-17 12:49:00,092: INFO: main: Eval Epoch : batch 285 Loss: 0.005133646590330679]
[2024-04-17 12:49:00,295: INFO: main: Eval Epoch : batch 286 Loss: 0.002215941070399444]
[2024-04-17 12:49:00,497: INFO: main: Eval Epoch : batch 287 Loss: 0.00397022391322022]
[2024-04-17 12:49:00,696: INFO: main: Eval Epoch : batch 288 Loss: 0.005232044742972647]
[2024-04-17 12:49:00,896: INFO: main: Eval Epoch : batch 289 Loss: 0.002374182130059356]
[2024-04-17 12:49:01,101: INFO: main: Eval Epoch : batch 290 Loss: 0.016877412354934354]
[2024-04-17 12:49:01,305: INFO: main: Eval Epoch : batch 291 Loss: 0.0008660554110424578]
[2024-04-17 12:49:01,509: INFO: main: Eval Epoch : batch 292 Loss: 0.01321099077683378]
[2024-04-17 12:49:01,711: INFO: main: Eval Epoch : batch 293 Loss: 0.00542314439350562]
[2024-04-17 12:49:01,909: INFO: main: Eval Epoch : batch 294 Loss: 0.005556340124547261]
[2024-04-17 12:49:02,112: INFO: main: Eval Epoch : batch 295 Loss: 0.007197489860823316]
[2024-04-17 12:49:02,317: INFO: main: Eval Epoch : batch 296 Loss: 0.023576484043158392]
[2024-04-17 12:49:02,519: INFO: main: Eval Epoch : batch 297 Loss: 0.011573083437660362]
[2024-04-17 12:49:02,721: INFO: main: Eval Epoch : batch 298 Loss: 0.02296712995196753]
[2024-04-17 12:49:02,923: INFO: main: Eval Epoch : batch 299 Loss: 0.01067566155282248]
[2024-04-17 12:49:03,127: INFO: main: Eval Epoch : batch 300 Loss: 0.022032353502928027]
[2024-04-17 12:49:03,340: INFO: main: Eval Epoch : batch 301 Loss: 0.0013384396681536243]
[2024-04-17 12:49:03,546: INFO: main: Eval Epoch : batch 302 Loss: 0.008843392748475303]
[2024-04-17 12:49:03,751: INFO: main: Eval Epoch : batch 303 Loss: 0.002405372763409825]
[2024-04-17 12:49:03,959: INFO: main: Eval Epoch : batch 304 Loss: 0.005623343936868628]
[2024-04-17 12:49:04,165: INFO: main: Eval Epoch : batch 305 Loss: 0.014134505663041495]
[2024-04-17 12:49:04,375: INFO: main: Eval Epoch : batch 306 Loss: 0.009354560931583392]
[2024-04-17 12:49:04,580: INFO: main: Eval Epoch : batch 307 Loss: 0.00699544860598273]
[2024-04-17 12:49:04,787: INFO: main: Eval Epoch : batch 308 Loss: 0.012543392457444898]
[2024-04-17 12:49:04,997: INFO: main: Eval Epoch : batch 309 Loss: 0.030666794197834916]
[2024-04-17 12:49:05,203: INFO: main: Eval Epoch : batch 310 Loss: 0.03141261472689147]
[2024-04-17 12:49:05,408: INFO: main: Eval Epoch : batch 311 Loss: 0.010546710622173792]
[2024-04-17 12:49:05,616: INFO: main: Eval Epoch : batch 312 Loss: 0.005207675176947089]
[2024-04-17 12:49:05,825: INFO: main: Eval Epoch : batch 313 Loss: 0.011899192736256151]
[2024-04-17 12:49:06,034: INFO: main: Eval Epoch : batch 314 Loss: 0.016464318193746736]
[2024-04-17 12:49:06,239: INFO: main: Eval Epoch : batch 315 Loss: 0.0011829785360804117]
[2024-04-17 12:49:06,443: INFO: main: Eval Epoch : batch 316 Loss: 0.01392488477015124]
[2024-04-17 12:49:06,660: INFO: main: Eval Epoch : batch 317 Loss: 0.0022537761231752845]
[2024-04-17 12:49:06,866: INFO: main: Eval Epoch : batch 318 Loss: 0.007176850367302271]
[2024-04-17 12:49:07,069: INFO: main: Eval Epoch : batch 319 Loss: 0.006292613610628986]
[2024-04-17 12:49:07,269: INFO: main: Eval Epoch : batch 320 Loss: 0.0018266397603618882]
[2024-04-17 12:49:07,478: INFO: main: Eval Epoch : batch 321 Loss: 0.004850601067373853]
[2024-04-17 12:49:07,682: INFO: main: Eval Epoch : batch 322 Loss: 0.02999254017772029]
[2024-04-17 12:49:07,881: INFO: main: Eval Epoch : batch 323 Loss: 0.013211168947723763]
[2024-04-17 12:49:08,089: INFO: main: Eval Epoch : batch 324 Loss: 0.014081767770841238]
[2024-04-17 12:49:08,285: INFO: main: Eval Epoch : batch 325 Loss: 0.003608808414529161]
[2024-04-17 12:49:08,490: INFO: main: Eval Epoch : batch 326 Loss: 0.026401037298173727]
[2024-04-17 12:49:08,696: INFO: main: Eval Epoch : batch 327 Loss: 0.011338992813073812]
[2024-04-17 12:49:08,895: INFO: main: Eval Epoch : batch 328 Loss: 0.004538130165303421]
[2024-04-17 12:49:09,096: INFO: main: Eval Epoch : batch 329 Loss: 0.01462055083931267]
[2024-04-17 12:49:09,296: INFO: main: Eval Epoch : batch 330 Loss: 0.022809616529307127]
[2024-04-17 12:49:09,501: INFO: main: Eval Epoch : batch 331 Loss: 0.023897192663244953]
[2024-04-17 12:49:09,704: INFO: main: Eval Epoch : batch 332 Loss: 0.00400724879392003]
[2024-04-17 12:49:09,908: INFO: main: Eval Epoch : batch 333 Loss: 0.018344505424024486]
[2024-04-17 12:49:10,107: INFO: main: Eval Epoch : batch 334 Loss: 0.04372490187132964]
[2024-04-17 12:49:10,311: INFO: main: Eval Epoch : batch 335 Loss: 0.004632175805995216]
[2024-04-17 12:49:10,512: INFO: main: Eval Epoch : batch 336 Loss: 0.0050628264786562694]
[2024-04-17 12:49:10,713: INFO: main: Eval Epoch : batch 337 Loss: 0.03922593606626145]
[2024-04-17 12:49:10,919: INFO: main: Eval Epoch : batch 338 Loss: 0.00622936180508051]
[2024-04-17 12:49:11,118: INFO: main: Eval Epoch : batch 339 Loss: 0.005302058871919903]
[2024-04-17 12:49:11,319: INFO: main: Eval Epoch : batch 340 Loss: 0.012579646814079262]
[2024-04-17 12:49:11,522: INFO: main: Eval Epoch : batch 341 Loss: 0.030656933801864954]
[2024-04-17 12:49:11,724: INFO: main: Eval Epoch : batch 342 Loss: 0.0005086888465509862]
[2024-04-17 12:49:11,932: INFO: main: Eval Epoch : batch 343 Loss: 0.021497003070004385]
[2024-04-17 12:49:12,131: INFO: main: Eval Epoch : batch 344 Loss: 0.02314263977869124]
[2024-04-17 12:49:12,332: INFO: main: Eval Epoch : batch 345 Loss: 0.025080988402388]
[2024-04-17 12:49:12,533: INFO: main: Eval Epoch : batch 346 Loss: 0.009745239274672983]
[2024-04-17 12:49:12,734: INFO: main: Eval Epoch : batch 347 Loss: 0.01893912909842561]
[2024-04-17 12:49:12,940: INFO: main: Eval Epoch : batch 348 Loss: 0.009908432972531791]
[2024-04-17 12:49:13,143: INFO: main: Eval Epoch : batch 349 Loss: 0.005321673996331313]
[2024-04-17 12:49:13,342: INFO: main: Eval Epoch : batch 350 Loss: 0.0020893712909566715]
[2024-04-17 12:49:13,546: INFO: main: Eval Epoch : batch 351 Loss: 0.020053663041600035]
[2024-04-17 12:49:13,749: INFO: main: Eval Epoch : batch 352 Loss: 0.006895931356515915]
[2024-04-17 12:49:13,953: INFO: main: Eval Epoch : batch 353 Loss: 0.0075430049027133015]
[2024-04-17 12:49:14,154: INFO: main: Eval Epoch : batch 354 Loss: 0.019310532067027732]
[2024-04-17 12:49:14,354: INFO: main: Eval Epoch : batch 355 Loss: 0.030259333866916933]
[2024-04-17 12:49:14,554: INFO: main: Eval Epoch : batch 356 Loss: 0.003291686654808761]
[2024-04-17 12:49:14,657: INFO: main: Eval Epoch : batch 357 Loss: 0.0045046299434901545]
[2024-04-17 12:49:29,666: INFO: main: The score of the eval model is {'Accuracy': 0.9929056678594534, 'precision': 0.7012829985893733, 'recall': 0.9056996616639195, 'f1': 0.7904898917240857}]
[2024-04-17 12:49:31,695: INFO: main: Epoch: 4/5]
[2024-04-17 12:49:32,366: INFO: main: Training : batch 0 Loss: 0.001919274377920839]
[2024-04-17 12:49:32,981: INFO: main: Training : batch 1 Loss: 0.0074683922210324605]
[2024-04-17 12:49:33,606: INFO: main: Training : batch 2 Loss: 0.006483565869574524]
[2024-04-17 12:49:34,226: INFO: main: Training : batch 3 Loss: 0.0039998029210250394]
[2024-04-17 12:49:34,848: INFO: main: Training : batch 4 Loss: 0.0018873734198344976]
[2024-04-17 12:49:35,464: INFO: main: Training : batch 5 Loss: 0.0018402579213708387]
[2024-04-17 12:49:36,088: INFO: main: Training : batch 6 Loss: 0.027213230277058333]
[2024-04-17 12:49:36,706: INFO: main: Training : batch 7 Loss: 0.012910983875947872]
[2024-04-17 12:49:37,327: INFO: main: Training : batch 8 Loss: 0.015347275729545935]
[2024-04-17 12:49:37,943: INFO: main: Training : batch 9 Loss: 0.00282836236684535]
[2024-04-17 12:49:38,561: INFO: main: Training : batch 10 Loss: 0.01311267174579864]
[2024-04-17 12:49:39,187: INFO: main: Training : batch 11 Loss: 0.003193581537848301]
[2024-04-17 12:49:39,808: INFO: main: Training : batch 12 Loss: 0.000809738593524204]
[2024-04-17 12:49:40,435: INFO: main: Training : batch 13 Loss: 0.006217515611130739]
[2024-04-17 12:49:41,057: INFO: main: Training : batch 14 Loss: 0.008651794228583911]
[2024-04-17 12:49:41,691: INFO: main: Training : batch 15 Loss: 0.015796460461055473]
[2024-04-17 12:49:42,313: INFO: main: Training : batch 16 Loss: 0.015722109947769975]
[2024-04-17 12:49:42,942: INFO: main: Training : batch 17 Loss: 0.017235554432983528]
[2024-04-17 12:49:43,571: INFO: main: Training : batch 18 Loss: 0.00553173436921764]
[2024-04-17 12:49:44,203: INFO: main: Training : batch 19 Loss: 0.025130123596402463]
[2024-04-17 12:49:44,838: INFO: main: Training : batch 20 Loss: 0.004238799704971998]
[2024-04-17 12:49:45,484: INFO: main: Training : batch 21 Loss: 0.0006556797404684289]
[2024-04-17 12:49:46,131: INFO: main: Training : batch 22 Loss: 0.004181294132347799]
[2024-04-17 12:49:46,788: INFO: main: Training : batch 23 Loss: 0.011463972396735486]
[2024-04-17 12:49:47,430: INFO: main: Training : batch 24 Loss: 0.0048714324032906285]
[2024-04-17 12:49:48,074: INFO: main: Training : batch 25 Loss: 0.00319717600261848]
[2024-04-17 12:49:48,714: INFO: main: Training : batch 26 Loss: 0.005487527001854787]
[2024-04-17 12:49:49,351: INFO: main: Training : batch 27 Loss: 0.002296686339408641]
[2024-04-17 12:49:49,990: INFO: main: Training : batch 28 Loss: 0.008207694409383383]
[2024-04-17 12:49:50,633: INFO: main: Training : batch 29 Loss: 0.018887856281826633]
[2024-04-17 12:49:51,279: INFO: main: Training : batch 30 Loss: 0.002480566349077521]
[2024-04-17 12:49:51,917: INFO: main: Training : batch 31 Loss: 0.01846419855978871]
[2024-04-17 12:49:52,559: INFO: main: Training : batch 32 Loss: 0.011729874359968272]
[2024-04-17 12:49:53,198: INFO: main: Training : batch 33 Loss: 0.005504228199636737]
[2024-04-17 12:49:53,837: INFO: main: Training : batch 34 Loss: 0.0027146952951310143]
[2024-04-17 12:49:54,485: INFO: main: Training : batch 35 Loss: 0.005235282496013074]
[2024-04-17 12:49:55,127: INFO: main: Training : batch 36 Loss: 0.009394694095962062]
[2024-04-17 12:49:55,772: INFO: main: Training : batch 37 Loss: 0.008071082006439885]
[2024-04-17 12:49:56,415: INFO: main: Training : batch 38 Loss: 0.008698457676137215]
[2024-04-17 12:49:57,061: INFO: main: Training : batch 39 Loss: 0.004615035260301295]
[2024-04-17 12:49:57,710: INFO: main: Training : batch 40 Loss: 0.003138179588593977]
[2024-04-17 12:49:58,361: INFO: main: Training : batch 41 Loss: 0.0031312323666470563]
[2024-04-17 12:49:59,011: INFO: main: Training : batch 42 Loss: 0.002152022670856725]
[2024-04-17 12:49:59,666: INFO: main: Training : batch 43 Loss: 0.027456208812347928]
[2024-04-17 12:50:00,334: INFO: main: Training : batch 44 Loss: 0.0042753632536668635]
[2024-04-17 12:50:00,997: INFO: main: Training : batch 45 Loss: 0.021744991124556502]
[2024-04-17 12:50:01,656: INFO: main: Training : batch 46 Loss: 0.005719471029144873]
[2024-04-17 12:50:02,309: INFO: main: Training : batch 47 Loss: 0.00870265667751673]
[2024-04-17 12:50:02,958: INFO: main: Training : batch 48 Loss: 0.00667699627698634]
[2024-04-17 12:50:03,611: INFO: main: Training : batch 49 Loss: 0.002449456157831024]
[2024-04-17 12:50:04,259: INFO: main: Training : batch 50 Loss: 0.0029415281096541435]
[2024-04-17 12:50:04,911: INFO: main: Training : batch 51 Loss: 0.0012836568215131505]
[2024-04-17 12:50:05,571: INFO: main: Training : batch 52 Loss: 0.0011482082236487546]
[2024-04-17 12:50:06,223: INFO: main: Training : batch 53 Loss: 0.016826785830538465]
[2024-04-17 12:50:06,873: INFO: main: Training : batch 54 Loss: 0.0046161678133995925]
[2024-04-17 12:50:07,535: INFO: main: Training : batch 55 Loss: 0.0009804622923578106]
[2024-04-17 12:50:08,185: INFO: main: Training : batch 56 Loss: 0.016505714390810153]
[2024-04-17 12:50:08,835: INFO: main: Training : batch 57 Loss: 0.0008450095983076487]
[2024-04-17 12:50:09,485: INFO: main: Training : batch 58 Loss: 0.005780993471981278]
[2024-04-17 12:50:10,144: INFO: main: Training : batch 59 Loss: 0.002411573290480053]
[2024-04-17 12:50:10,798: INFO: main: Training : batch 60 Loss: 0.008329776100819806]
[2024-04-17 12:50:11,445: INFO: main: Training : batch 61 Loss: 0.00829986860580439]
[2024-04-17 12:50:12,097: INFO: main: Training : batch 62 Loss: 0.0030188157740928426]
[2024-04-17 12:50:12,757: INFO: main: Training : batch 63 Loss: 0.00701255551848114]
[2024-04-17 12:50:13,405: INFO: main: Training : batch 64 Loss: 0.00027120589072805534]
[2024-04-17 12:50:14,058: INFO: main: Training : batch 65 Loss: 0.010325050268261252]
[2024-04-17 12:50:14,709: INFO: main: Training : batch 66 Loss: 0.00400458060348505]
[2024-04-17 12:50:15,370: INFO: main: Training : batch 67 Loss: 0.024563132314366604]
[2024-04-17 12:50:16,020: INFO: main: Training : batch 68 Loss: 0.0021190125361044672]
[2024-04-17 12:50:16,664: INFO: main: Training : batch 69 Loss: 0.008841263897227674]
[2024-04-17 12:50:17,309: INFO: main: Training : batch 70 Loss: 0.006542419242911775]
[2024-04-17 12:50:17,949: INFO: main: Training : batch 71 Loss: 0.02060162830598338]
[2024-04-17 12:50:18,585: INFO: main: Training : batch 72 Loss: 0.002442780972137954]
[2024-04-17 12:50:19,225: INFO: main: Training : batch 73 Loss: 0.0022626473065032646]
[2024-04-17 12:50:19,864: INFO: main: Training : batch 74 Loss: 0.003093444286563398]
[2024-04-17 12:50:20,502: INFO: main: Training : batch 75 Loss: 0.009241395187210495]
[2024-04-17 12:50:21,139: INFO: main: Training : batch 76 Loss: 0.004993427368630624]
[2024-04-17 12:50:21,771: INFO: main: Training : batch 77 Loss: 0.006627857308835778]
[2024-04-17 12:50:22,406: INFO: main: Training : batch 78 Loss: 0.0012889965242775016]
[2024-04-17 12:50:23,038: INFO: main: Training : batch 79 Loss: 0.0024083349825632723]
[2024-04-17 12:50:23,678: INFO: main: Training : batch 80 Loss: 0.0020555786571686795]
[2024-04-17 12:50:24,312: INFO: main: Training : batch 81 Loss: 0.010881454949530835]
[2024-04-17 12:50:24,946: INFO: main: Training : batch 82 Loss: 0.0045930038195322485]
[2024-04-17 12:50:25,591: INFO: main: Training : batch 83 Loss: 0.002835917426120836]
[2024-04-17 12:50:26,228: INFO: main: Training : batch 84 Loss: 0.010599761279889125]
[2024-04-17 12:50:26,863: INFO: main: Training : batch 85 Loss: 0.00810631324234852]
[2024-04-17 12:50:27,495: INFO: main: Training : batch 86 Loss: 0.004406209530781636]
[2024-04-17 12:50:28,131: INFO: main: Training : batch 87 Loss: 0.004630117646215032]
[2024-04-17 12:50:28,771: INFO: main: Training : batch 88 Loss: 0.003327684462079371]
[2024-04-17 12:50:29,403: INFO: main: Training : batch 89 Loss: 0.00726830125440099]
[2024-04-17 12:50:30,035: INFO: main: Training : batch 90 Loss: 0.006579788237697101]
[2024-04-17 12:50:30,665: INFO: main: Training : batch 91 Loss: 0.01854630179738164]
[2024-04-17 12:50:31,289: INFO: main: Training : batch 92 Loss: 0.0019417503661673609]
[2024-04-17 12:50:31,920: INFO: main: Training : batch 93 Loss: 0.006608314614248216]
[2024-04-17 12:50:32,543: INFO: main: Training : batch 94 Loss: 0.005443013180134198]
[2024-04-17 12:50:33,171: INFO: main: Training : batch 95 Loss: 0.006837333689444004]
[2024-04-17 12:50:33,794: INFO: main: Training : batch 96 Loss: 0.002697254556643239]
[2024-04-17 12:50:34,420: INFO: main: Training : batch 97 Loss: 0.0027743058841096556]
[2024-04-17 12:50:35,049: INFO: main: Training : batch 98 Loss: 0.00581529605271258]
[2024-04-17 12:50:35,672: INFO: main: Training : batch 99 Loss: 0.005926641465248536]
[2024-04-17 12:50:36,296: INFO: main: Training : batch 100 Loss: 0.006339829223636706]
[2024-04-17 12:50:36,918: INFO: main: Training : batch 101 Loss: 0.00030143722913017246]
[2024-04-17 12:50:37,544: INFO: main: Training : batch 102 Loss: 0.011784929811455453]
[2024-04-17 12:50:38,172: INFO: main: Training : batch 103 Loss: 0.0077489361847887465]
[2024-04-17 12:50:38,797: INFO: main: Training : batch 104 Loss: 0.0067850332766187915]
[2024-04-17 12:50:39,427: INFO: main: Training : batch 105 Loss: 0.006220282645868357]
[2024-04-17 12:50:40,059: INFO: main: Training : batch 106 Loss: 0.011667772191116664]
[2024-04-17 12:50:40,689: INFO: main: Training : batch 107 Loss: 0.004267181061791618]
[2024-04-17 12:50:41,315: INFO: main: Training : batch 108 Loss: 0.007652586937007272]
[2024-04-17 12:50:41,943: INFO: main: Training : batch 109 Loss: 0.0017802244450741906]
[2024-04-17 12:50:42,561: INFO: main: Training : batch 110 Loss: 0.01412234127112318]
[2024-04-17 12:50:43,188: INFO: main: Training : batch 111 Loss: 0.01065141244378526]
[2024-04-17 12:50:43,810: INFO: main: Training : batch 112 Loss: 0.0028051538272313155]
[2024-04-17 12:50:44,435: INFO: main: Training : batch 113 Loss: 0.003218492605031283]
[2024-04-17 12:50:45,055: INFO: main: Training : batch 114 Loss: 0.013997914249635124]
[2024-04-17 12:50:45,683: INFO: main: Training : batch 115 Loss: 0.006894776495840947]
[2024-04-17 12:50:46,310: INFO: main: Training : batch 116 Loss: 0.006318536868321959]
[2024-04-17 12:50:46,928: INFO: main: Training : batch 117 Loss: 0.010376077858308986]
[2024-04-17 12:50:47,552: INFO: main: Training : batch 118 Loss: 0.00451770144773431]
[2024-04-17 12:50:48,176: INFO: main: Training : batch 119 Loss: 0.008467578436369345]
[2024-04-17 12:50:48,801: INFO: main: Training : batch 120 Loss: 0.0020234041904505064]
[2024-04-17 12:50:49,425: INFO: main: Training : batch 121 Loss: 0.013250805553700963]
[2024-04-17 12:50:50,043: INFO: main: Training : batch 122 Loss: 0.004708052139584385]
[2024-04-17 12:50:50,668: INFO: main: Training : batch 123 Loss: 0.004754427357010671]
[2024-04-17 12:50:51,292: INFO: main: Training : batch 124 Loss: 0.016963695038141897]
[2024-04-17 12:50:51,918: INFO: main: Training : batch 125 Loss: 0.009843264678651497]
[2024-04-17 12:50:52,543: INFO: main: Training : batch 126 Loss: 0.020054494788921438]
[2024-04-17 12:50:53,170: INFO: main: Training : batch 127 Loss: 0.003431231560658617]
[2024-04-17 12:50:53,807: INFO: main: Training : batch 128 Loss: 0.0039195378087840335]
[2024-04-17 12:50:54,440: INFO: main: Training : batch 129 Loss: 0.0004518668100687076]
[2024-04-17 12:50:55,076: INFO: main: Training : batch 130 Loss: 0.004746478962499796]
[2024-04-17 12:50:55,707: INFO: main: Training : batch 131 Loss: 0.00012097077170091518]
[2024-04-17 12:50:56,332: INFO: main: Training : batch 132 Loss: 0.007079361985868339]
[2024-04-17 12:50:56,956: INFO: main: Training : batch 133 Loss: 0.005914649690633473]
[2024-04-17 12:50:57,584: INFO: main: Training : batch 134 Loss: 0.00822129524782105]
[2024-04-17 12:50:58,212: INFO: main: Training : batch 135 Loss: 0.003545829326871568]
[2024-04-17 12:50:58,838: INFO: main: Training : batch 136 Loss: 0.01835638929634205]
[2024-04-17 12:50:59,465: INFO: main: Training : batch 137 Loss: 0.0022040501538174296]
[2024-04-17 12:51:00,092: INFO: main: Training : batch 138 Loss: 0.009648835253806049]
[2024-04-17 12:51:00,717: INFO: main: Training : batch 139 Loss: 0.0027789492222993224]
[2024-04-17 12:51:01,347: INFO: main: Training : batch 140 Loss: 0.010368730480891493]
[2024-04-17 12:51:01,977: INFO: main: Training : batch 141 Loss: 0.013745045363979955]
[2024-04-17 12:51:02,603: INFO: main: Training : batch 142 Loss: 0.0509560830037858]
[2024-04-17 12:51:03,234: INFO: main: Training : batch 143 Loss: 0.01018185088956939]
[2024-04-17 12:51:03,859: INFO: main: Training : batch 144 Loss: 0.02022970464924479]
[2024-04-17 12:51:04,488: INFO: main: Training : batch 145 Loss: 0.003917879366661763]
[2024-04-17 12:51:05,115: INFO: main: Training : batch 146 Loss: 0.004536779685431328]
[2024-04-17 12:51:05,747: INFO: main: Training : batch 147 Loss: 0.005610883000707822]
[2024-04-17 12:51:06,382: INFO: main: Training : batch 148 Loss: 0.006013099090848574]
[2024-04-17 12:51:07,019: INFO: main: Training : batch 149 Loss: 0.0013079448954216582]
[2024-04-17 12:51:07,653: INFO: main: Training : batch 150 Loss: 0.018861929586545475]
[2024-04-17 12:51:08,288: INFO: main: Training : batch 151 Loss: 0.016782920180595]
[2024-04-17 12:51:08,927: INFO: main: Training : batch 152 Loss: 0.012284195426017818]
[2024-04-17 12:51:09,558: INFO: main: Training : batch 153 Loss: 0.003249828837782025]
[2024-04-17 12:51:10,193: INFO: main: Training : batch 154 Loss: 0.00345714562661371]
[2024-04-17 12:51:10,819: INFO: main: Training : batch 155 Loss: 0.012437892100741315]
[2024-04-17 12:51:11,449: INFO: main: Training : batch 156 Loss: 0.0031606930694577672]
[2024-04-17 12:51:12,079: INFO: main: Training : batch 157 Loss: 0.002534476423155214]
[2024-04-17 12:51:12,712: INFO: main: Training : batch 158 Loss: 0.0006908385760444976]
[2024-04-17 12:51:13,344: INFO: main: Training : batch 159 Loss: 0.0021565924651760254]
[2024-04-17 12:51:13,975: INFO: main: Training : batch 160 Loss: 0.011238238390003284]
[2024-04-17 12:51:14,606: INFO: main: Training : batch 161 Loss: 0.006988738700976741]
[2024-04-17 12:51:15,243: INFO: main: Training : batch 162 Loss: 0.0031469834119412575]
[2024-04-17 12:51:15,878: INFO: main: Training : batch 163 Loss: 0.01247683000458015]
[2024-04-17 12:51:16,516: INFO: main: Training : batch 164 Loss: 0.0021312036467626087]
[2024-04-17 12:51:17,151: INFO: main: Training : batch 165 Loss: 0.00606568106051982]
[2024-04-17 12:51:17,792: INFO: main: Training : batch 166 Loss: 0.003144762704668323]
[2024-04-17 12:51:18,426: INFO: main: Training : batch 167 Loss: 0.026175147469830573]
[2024-04-17 12:51:19,058: INFO: main: Training : batch 168 Loss: 0.002867507168625758]
[2024-04-17 12:51:19,690: INFO: main: Training : batch 169 Loss: 0.0023512879265956102]
[2024-04-17 12:51:20,330: INFO: main: Training : batch 170 Loss: 0.013109160920833692]
[2024-04-17 12:51:20,969: INFO: main: Training : batch 171 Loss: 0.0033894668414937924]
[2024-04-17 12:51:21,617: INFO: main: Training : batch 172 Loss: 0.0030954784001776064]
[2024-04-17 12:51:22,262: INFO: main: Training : batch 173 Loss: 0.003628993427973437]
[2024-04-17 12:51:22,898: INFO: main: Training : batch 174 Loss: 0.0006410656746283528]
[2024-04-17 12:51:23,532: INFO: main: Training : batch 175 Loss: 0.001995335104580414]
[2024-04-17 12:51:24,167: INFO: main: Training : batch 176 Loss: 0.013558641993610393]
[2024-04-17 12:51:24,805: INFO: main: Training : batch 177 Loss: 0.007865720613504826]
[2024-04-17 12:51:25,445: INFO: main: Training : batch 178 Loss: 0.013442275103198911]
[2024-04-17 12:51:26,086: INFO: main: Training : batch 179 Loss: 0.009065732540761356]
[2024-04-17 12:51:26,724: INFO: main: Training : batch 180 Loss: 0.004805109021974742]
[2024-04-17 12:51:27,357: INFO: main: Training : batch 181 Loss: 0.004707681563310148]
[2024-04-17 12:51:27,994: INFO: main: Training : batch 182 Loss: 0.005051711364754069]
[2024-04-17 12:51:28,630: INFO: main: Training : batch 183 Loss: 0.017541275929858584]
[2024-04-17 12:51:29,265: INFO: main: Training : batch 184 Loss: 0.019278279256659113]
[2024-04-17 12:51:29,903: INFO: main: Training : batch 185 Loss: 0.0025723461678477996]
[2024-04-17 12:51:30,540: INFO: main: Training : batch 186 Loss: 0.00661633021101386]
[2024-04-17 12:51:31,170: INFO: main: Training : batch 187 Loss: 0.013568061505847356]
[2024-04-17 12:51:31,807: INFO: main: Training : batch 188 Loss: 0.00570075314601863]
[2024-04-17 12:51:32,440: INFO: main: Training : batch 189 Loss: 0.004356938099808374]
[2024-04-17 12:51:33,080: INFO: main: Training : batch 190 Loss: 0.005201427688532515]
[2024-04-17 12:51:33,726: INFO: main: Training : batch 191 Loss: 0.009414980014697755]
[2024-04-17 12:51:34,365: INFO: main: Training : batch 192 Loss: 0.01657983555109058]
[2024-04-17 12:51:35,010: INFO: main: Training : batch 193 Loss: 0.005649881690829079]
[2024-04-17 12:51:35,649: INFO: main: Training : batch 194 Loss: 0.012035612394748334]
[2024-04-17 12:51:36,288: INFO: main: Training : batch 195 Loss: 0.02541680710189723]
[2024-04-17 12:51:36,921: INFO: main: Training : batch 196 Loss: 0.014371988878869793]
[2024-04-17 12:51:37,557: INFO: main: Training : batch 197 Loss: 0.008632835005523995]
[2024-04-17 12:51:38,189: INFO: main: Training : batch 198 Loss: 0.016547517485361832]
[2024-04-17 12:51:38,826: INFO: main: Training : batch 199 Loss: 0.0005580068318517287]
[2024-04-17 12:51:39,461: INFO: main: Training : batch 200 Loss: 0.008289676062125535]
[2024-04-17 12:51:40,092: INFO: main: Training : batch 201 Loss: 0.002707756135353794]
[2024-04-17 12:51:40,726: INFO: main: Training : batch 202 Loss: 0.0019192342969890461]
[2024-04-17 12:51:41,362: INFO: main: Training : batch 203 Loss: 0.007590203311076057]
[2024-04-17 12:51:41,998: INFO: main: Training : batch 204 Loss: 0.0043933528563562035]
[2024-04-17 12:51:42,629: INFO: main: Training : batch 205 Loss: 0.007948178187013648]
[2024-04-17 12:51:43,265: INFO: main: Training : batch 206 Loss: 0.005589066924005747]
[2024-04-17 12:51:43,896: INFO: main: Training : batch 207 Loss: 0.006283104732263227]
[2024-04-17 12:51:44,532: INFO: main: Training : batch 208 Loss: 0.01620867505970236]
[2024-04-17 12:51:45,165: INFO: main: Training : batch 209 Loss: 0.0044770615889786845]
[2024-04-17 12:51:45,799: INFO: main: Training : batch 210 Loss: 0.009615755630003183]
[2024-04-17 12:51:46,442: INFO: main: Training : batch 211 Loss: 0.0063396511590625955]
[2024-04-17 12:51:47,079: INFO: main: Training : batch 212 Loss: 0.0013920305838521624]
[2024-04-17 12:51:47,716: INFO: main: Training : batch 213 Loss: 0.013561656001431312]
[2024-04-17 12:51:48,357: INFO: main: Training : batch 214 Loss: 0.0015087697655867037]
[2024-04-17 12:51:48,994: INFO: main: Training : batch 215 Loss: 0.0019405947228590436]
[2024-04-17 12:51:49,629: INFO: main: Training : batch 216 Loss: 0.009414548507889524]
[2024-04-17 12:51:50,262: INFO: main: Training : batch 217 Loss: 0.003643790764840182]
[2024-04-17 12:51:50,895: INFO: main: Training : batch 218 Loss: 0.0020233925303179963]
[2024-04-17 12:51:51,531: INFO: main: Training : batch 219 Loss: 0.006258201901006992]
[2024-04-17 12:51:52,166: INFO: main: Training : batch 220 Loss: 0.005916384059920322]
[2024-04-17 12:51:52,799: INFO: main: Training : batch 221 Loss: 0.012152764698143314]
[2024-04-17 12:51:53,429: INFO: main: Training : batch 222 Loss: 0.004843228360982306]
[2024-04-17 12:51:54,062: INFO: main: Training : batch 223 Loss: 0.013341047077094263]
[2024-04-17 12:51:54,696: INFO: main: Training : batch 224 Loss: 0.003483472189273709]
[2024-04-17 12:51:55,329: INFO: main: Training : batch 225 Loss: 0.009593682495658895]
[2024-04-17 12:51:55,960: INFO: main: Training : batch 226 Loss: 0.0029238122849421402]
[2024-04-17 12:51:56,590: INFO: main: Training : batch 227 Loss: 0.012704967240651078]
[2024-04-17 12:51:57,220: INFO: main: Training : batch 228 Loss: 0.00491556590918649]
[2024-04-17 12:51:57,850: INFO: main: Training : batch 229 Loss: 0.02099605957733152]
[2024-04-17 12:51:58,479: INFO: main: Training : batch 230 Loss: 0.0038772049353908394]
[2024-04-17 12:51:59,109: INFO: main: Training : batch 231 Loss: 0.0075437980017260655]
[2024-04-17 12:51:59,746: INFO: main: Training : batch 232 Loss: 0.009523597179829778]
[2024-04-17 12:52:00,386: INFO: main: Training : batch 233 Loss: 0.011630646596092701]
[2024-04-17 12:52:01,024: INFO: main: Training : batch 234 Loss: 0.007633870840147735]
[2024-04-17 12:52:01,664: INFO: main: Training : batch 235 Loss: 0.017211346100945903]
[2024-04-17 12:52:02,307: INFO: main: Training : batch 236 Loss: 0.014112084568948127]
[2024-04-17 12:52:02,936: INFO: main: Training : batch 237 Loss: 0.006110372937686081]
[2024-04-17 12:52:03,572: INFO: main: Training : batch 238 Loss: 0.014722681914907314]
[2024-04-17 12:52:04,204: INFO: main: Training : batch 239 Loss: 0.015134954668314878]
[2024-04-17 12:52:04,828: INFO: main: Training : batch 240 Loss: 0.005837699177295048]
[2024-04-17 12:52:05,459: INFO: main: Training : batch 241 Loss: 0.0097998528078641]
[2024-04-17 12:52:06,093: INFO: main: Training : batch 242 Loss: 0.024955619525981465]
[2024-04-17 12:52:06,723: INFO: main: Training : batch 243 Loss: 0.025320950171797068]
[2024-04-17 12:52:07,351: INFO: main: Training : batch 244 Loss: 0.009785507834633434]
[2024-04-17 12:52:07,980: INFO: main: Training : batch 245 Loss: 0.002771610513442318]
[2024-04-17 12:52:08,612: INFO: main: Training : batch 246 Loss: 0.004451707213898048]
[2024-04-17 12:52:09,241: INFO: main: Training : batch 247 Loss: 0.0022105942708207493]
[2024-04-17 12:52:09,875: INFO: main: Training : batch 248 Loss: 0.002239703919098293]
[2024-04-17 12:52:10,505: INFO: main: Training : batch 249 Loss: 0.007733998169592215]
[2024-04-17 12:52:11,138: INFO: main: Training : batch 250 Loss: 0.0033962327845852335]
[2024-04-17 12:52:11,764: INFO: main: Training : batch 251 Loss: 0.00260324376584041]
[2024-04-17 12:52:12,398: INFO: main: Training : batch 252 Loss: 0.02095794145363124]
[2024-04-17 12:52:13,030: INFO: main: Training : batch 253 Loss: 0.007866987051602362]
[2024-04-17 12:52:13,664: INFO: main: Training : batch 254 Loss: 0.009890265850921101]
[2024-04-17 12:52:14,303: INFO: main: Training : batch 255 Loss: 0.018591554895542362]
[2024-04-17 12:52:14,947: INFO: main: Training : batch 256 Loss: 0.016703868435514257]
[2024-04-17 12:52:15,582: INFO: main: Training : batch 257 Loss: 0.00626657485615671]
[2024-04-17 12:52:16,214: INFO: main: Training : batch 258 Loss: 0.0033469859588295724]
[2024-04-17 12:52:16,839: INFO: main: Training : batch 259 Loss: 0.01047068636209132]
[2024-04-17 12:52:17,469: INFO: main: Training : batch 260 Loss: 0.028670523271708333]
[2024-04-17 12:52:18,095: INFO: main: Training : batch 261 Loss: 0.005777049930595049]
[2024-04-17 12:52:18,723: INFO: main: Training : batch 262 Loss: 0.0034588282626269303]
[2024-04-17 12:52:19,350: INFO: main: Training : batch 263 Loss: 0.0090255316615579]
[2024-04-17 12:52:19,984: INFO: main: Training : batch 264 Loss: 0.0033271537413827206]
[2024-04-17 12:52:20,615: INFO: main: Training : batch 265 Loss: 0.01107881470830651]
[2024-04-17 12:52:21,250: INFO: main: Training : batch 266 Loss: 0.00046176464157163006]
[2024-04-17 12:52:21,876: INFO: main: Training : batch 267 Loss: 0.004463583490901844]
[2024-04-17 12:52:22,501: INFO: main: Training : batch 268 Loss: 0.0029463742162737375]
[2024-04-17 12:52:23,131: INFO: main: Training : batch 269 Loss: 0.0005335951914949536]
[2024-04-17 12:52:23,757: INFO: main: Training : batch 270 Loss: 0.015081056550628372]
[2024-04-17 12:52:24,384: INFO: main: Training : batch 271 Loss: 0.0019014586544119979]
[2024-04-17 12:52:25,014: INFO: main: Training : batch 272 Loss: 0.0038007031724589207]
[2024-04-17 12:52:25,649: INFO: main: Training : batch 273 Loss: 0.012871431505831192]
[2024-04-17 12:52:26,288: INFO: main: Training : batch 274 Loss: 0.005265764763602044]
[2024-04-17 12:52:26,925: INFO: main: Training : batch 275 Loss: 0.011498512649825635]
[2024-04-17 12:52:27,560: INFO: main: Training : batch 276 Loss: 0.003549878559872503]
[2024-04-17 12:52:28,197: INFO: main: Training : batch 277 Loss: 0.0037628678866060447]
[2024-04-17 12:52:28,831: INFO: main: Training : batch 278 Loss: 0.007382842853785909]
[2024-04-17 12:52:29,472: INFO: main: Training : batch 279 Loss: 0.000984170182622616]
[2024-04-17 12:52:30,101: INFO: main: Training : batch 280 Loss: 0.0036425672543289414]
[2024-04-17 12:52:30,731: INFO: main: Training : batch 281 Loss: 0.009583588558355812]
[2024-04-17 12:52:31,363: INFO: main: Training : batch 282 Loss: 0.0006024870652623237]
[2024-04-17 12:52:31,990: INFO: main: Training : batch 283 Loss: 0.005167349617656136]
[2024-04-17 12:52:32,616: INFO: main: Training : batch 284 Loss: 0.022673120555956983]
[2024-04-17 12:52:33,243: INFO: main: Training : batch 285 Loss: 0.009116077195089398]
[2024-04-17 12:52:33,870: INFO: main: Training : batch 286 Loss: 0.0028969185481989867]
[2024-04-17 12:52:34,499: INFO: main: Training : batch 287 Loss: 0.007845576363544418]
[2024-04-17 12:52:35,128: INFO: main: Training : batch 288 Loss: 0.0034574911626380263]
[2024-04-17 12:52:35,757: INFO: main: Training : batch 289 Loss: 0.004385055498538375]
[2024-04-17 12:52:36,386: INFO: main: Training : batch 290 Loss: 0.014217698903949808]
[2024-04-17 12:52:37,017: INFO: main: Training : batch 291 Loss: 0.003088516908289866]
[2024-04-17 12:52:37,646: INFO: main: Training : batch 292 Loss: 0.023209943368896722]
[2024-04-17 12:52:38,274: INFO: main: Training : batch 293 Loss: 0.008058510849924422]
[2024-04-17 12:52:38,903: INFO: main: Training : batch 294 Loss: 0.007069437272327119]
[2024-04-17 12:52:39,529: INFO: main: Training : batch 295 Loss: 0.0023782325307843827]
[2024-04-17 12:52:40,161: INFO: main: Training : batch 296 Loss: 0.008983022332955334]
[2024-04-17 12:52:40,796: INFO: main: Training : batch 297 Loss: 0.023372023580962298]
[2024-04-17 12:52:41,430: INFO: main: Training : batch 298 Loss: 0.007071979695032446]
[2024-04-17 12:52:42,066: INFO: main: Training : batch 299 Loss: 0.016100869013401707]
[2024-04-17 12:52:42,700: INFO: main: Training : batch 300 Loss: 0.014913870835424052]
[2024-04-17 12:52:43,329: INFO: main: Training : batch 301 Loss: 0.007053698958437582]
[2024-04-17 12:52:43,958: INFO: main: Training : batch 302 Loss: 0.003422057437810749]
[2024-04-17 12:52:44,587: INFO: main: Training : batch 303 Loss: 0.008126948908827956]
[2024-04-17 12:52:45,217: INFO: main: Training : batch 304 Loss: 0.009364807466018669]
[2024-04-17 12:52:45,851: INFO: main: Training : batch 305 Loss: 0.011999865133167243]
[2024-04-17 12:52:46,482: INFO: main: Training : batch 306 Loss: 0.0023453589383633077]
[2024-04-17 12:52:47,113: INFO: main: Training : batch 307 Loss: 0.0037088559295358676]
[2024-04-17 12:52:47,741: INFO: main: Training : batch 308 Loss: 0.0031337350690740394]
[2024-04-17 12:52:48,369: INFO: main: Training : batch 309 Loss: 0.0034387738242214303]
[2024-04-17 12:52:49,000: INFO: main: Training : batch 310 Loss: 0.005989986957515301]
[2024-04-17 12:52:49,634: INFO: main: Training : batch 311 Loss: 0.021831939015194122]
[2024-04-17 12:52:50,264: INFO: main: Training : batch 312 Loss: 0.00055174764278534]
[2024-04-17 12:52:50,901: INFO: main: Training : batch 313 Loss: 0.008415483542214544]
[2024-04-17 12:52:51,535: INFO: main: Training : batch 314 Loss: 0.004091366392402766]
[2024-04-17 12:52:52,166: INFO: main: Training : batch 315 Loss: 0.01327403373494459]
[2024-04-17 12:52:52,799: INFO: main: Training : batch 316 Loss: 0.01441397863538162]
[2024-04-17 12:52:53,441: INFO: main: Training : batch 317 Loss: 0.00022085336900002202]
[2024-04-17 12:52:54,088: INFO: main: Training : batch 318 Loss: 0.016755226840210132]
[2024-04-17 12:52:54,728: INFO: main: Training : batch 319 Loss: 0.010082244209483466]
[2024-04-17 12:52:55,369: INFO: main: Training : batch 320 Loss: 0.012830603835724047]
[2024-04-17 12:52:56,011: INFO: main: Training : batch 321 Loss: 0.004151770178280943]
[2024-04-17 12:52:56,641: INFO: main: Training : batch 322 Loss: 0.0011139782430387782]
[2024-04-17 12:52:57,272: INFO: main: Training : batch 323 Loss: 0.0036235450456645404]
[2024-04-17 12:52:57,911: INFO: main: Training : batch 324 Loss: 0.00809858285359611]
[2024-04-17 12:52:58,544: INFO: main: Training : batch 325 Loss: 0.0030832064289547066]
[2024-04-17 12:52:59,176: INFO: main: Training : batch 326 Loss: 0.0027415176845795516]
[2024-04-17 12:52:59,808: INFO: main: Training : batch 327 Loss: 0.0018404481090149702]
[2024-04-17 12:53:00,437: INFO: main: Training : batch 328 Loss: 0.022276696499764836]
[2024-04-17 12:53:01,076: INFO: main: Training : batch 329 Loss: 0.001323285769932082]
[2024-04-17 12:53:01,709: INFO: main: Training : batch 330 Loss: 0.0026789217199084617]
[2024-04-17 12:53:02,345: INFO: main: Training : batch 331 Loss: 0.006609709186004922]
[2024-04-17 12:53:02,979: INFO: main: Training : batch 332 Loss: 0.004856793276224259]
[2024-04-17 12:53:03,608: INFO: main: Training : batch 333 Loss: 0.007246933807050826]
[2024-04-17 12:53:04,244: INFO: main: Training : batch 334 Loss: 0.008406854678984587]
[2024-04-17 12:53:04,882: INFO: main: Training : batch 335 Loss: 0.008041838146646626]
[2024-04-17 12:53:05,516: INFO: main: Training : batch 336 Loss: 0.0035328969486408688]
[2024-04-17 12:53:06,149: INFO: main: Training : batch 337 Loss: 0.007637364154493422]
[2024-04-17 12:53:06,796: INFO: main: Training : batch 338 Loss: 0.011547898792965533]
[2024-04-17 12:53:07,436: INFO: main: Training : batch 339 Loss: 0.01086900033334486]
[2024-04-17 12:53:08,079: INFO: main: Training : batch 340 Loss: 0.038611839522334136]
[2024-04-17 12:53:08,722: INFO: main: Training : batch 341 Loss: 0.0018993649461088995]
[2024-04-17 12:53:09,369: INFO: main: Training : batch 342 Loss: 0.014969190108619265]
[2024-04-17 12:53:10,007: INFO: main: Training : batch 343 Loss: 0.01632133327839282]
[2024-04-17 12:53:10,639: INFO: main: Training : batch 344 Loss: 0.008973254715073414]
[2024-04-17 12:53:11,277: INFO: main: Training : batch 345 Loss: 0.0008778479868649922]
[2024-04-17 12:53:11,908: INFO: main: Training : batch 346 Loss: 0.008711473955865235]
[2024-04-17 12:53:12,541: INFO: main: Training : batch 347 Loss: 0.004821831667218478]
[2024-04-17 12:53:13,176: INFO: main: Training : batch 348 Loss: 0.011890528775383814]
[2024-04-17 12:53:13,819: INFO: main: Training : batch 349 Loss: 0.009566380596518567]
[2024-04-17 12:53:14,455: INFO: main: Training : batch 350 Loss: 0.0026217680379008058]
[2024-04-17 12:53:15,088: INFO: main: Training : batch 351 Loss: 0.0062537468746431]
[2024-04-17 12:53:15,724: INFO: main: Training : batch 352 Loss: 0.008609885180542445]
[2024-04-17 12:53:16,358: INFO: main: Training : batch 353 Loss: 0.0011776058662027336]
[2024-04-17 12:53:16,992: INFO: main: Training : batch 354 Loss: 0.013868827077116548]
[2024-04-17 12:53:17,624: INFO: main: Training : batch 355 Loss: 0.0008687763514581911]
[2024-04-17 12:53:18,263: INFO: main: Training : batch 356 Loss: 0.005862558920891907]
[2024-04-17 12:53:18,896: INFO: main: Training : batch 357 Loss: 0.00812540666238803]
[2024-04-17 12:53:19,526: INFO: main: Training : batch 358 Loss: 0.002286199412912438]
[2024-04-17 12:53:20,166: INFO: main: Training : batch 359 Loss: 0.0044384474149969655]
[2024-04-17 12:53:20,812: INFO: main: Training : batch 360 Loss: 0.0044518364088054355]
[2024-04-17 12:53:21,458: INFO: main: Training : batch 361 Loss: 0.02790871610871996]
[2024-04-17 12:53:22,100: INFO: main: Training : batch 362 Loss: 0.00315636403821302]
[2024-04-17 12:53:22,740: INFO: main: Training : batch 363 Loss: 0.023593393788202653]
[2024-04-17 12:53:23,371: INFO: main: Training : batch 364 Loss: 0.0029187684445671465]
[2024-04-17 12:53:24,006: INFO: main: Training : batch 365 Loss: 0.0006205553147849523]
[2024-04-17 12:53:24,643: INFO: main: Training : batch 366 Loss: 0.025615580457348415]
[2024-04-17 12:53:25,271: INFO: main: Training : batch 367 Loss: 0.011529765982126409]
[2024-04-17 12:53:25,908: INFO: main: Training : batch 368 Loss: 0.005582904893787593]
[2024-04-17 12:53:26,542: INFO: main: Training : batch 369 Loss: 0.012569864678830922]
[2024-04-17 12:53:27,177: INFO: main: Training : batch 370 Loss: 0.0048007852710349385]
[2024-04-17 12:53:27,809: INFO: main: Training : batch 371 Loss: 0.0005336389401587587]
[2024-04-17 12:53:28,445: INFO: main: Training : batch 372 Loss: 0.011663060372117912]
[2024-04-17 12:53:29,080: INFO: main: Training : batch 373 Loss: 0.0068676167320467696]
[2024-04-17 12:53:29,707: INFO: main: Training : batch 374 Loss: 0.01223507821390784]
[2024-04-17 12:53:30,345: INFO: main: Training : batch 375 Loss: 0.0035784123870239047]
[2024-04-17 12:53:30,975: INFO: main: Training : batch 376 Loss: 0.0013304430041158996]
[2024-04-17 12:53:31,602: INFO: main: Training : batch 377 Loss: 0.01151399100310956]
[2024-04-17 12:53:32,233: INFO: main: Training : batch 378 Loss: 0.007944658851079325]
[2024-04-17 12:53:32,860: INFO: main: Training : batch 379 Loss: 0.03156928318219917]
[2024-04-17 12:53:33,495: INFO: main: Training : batch 380 Loss: 0.00023396821104266921]
[2024-04-17 12:53:34,139: INFO: main: Training : batch 381 Loss: 0.006340309862308684]
[2024-04-17 12:53:34,773: INFO: main: Training : batch 382 Loss: 0.005662398847891099]
[2024-04-17 12:53:35,409: INFO: main: Training : batch 383 Loss: 0.009035680359044814]
[2024-04-17 12:53:36,046: INFO: main: Training : batch 384 Loss: 0.011693871641196986]
[2024-04-17 12:53:36,690: INFO: main: Training : batch 385 Loss: 0.0029745857524905443]
[2024-04-17 12:53:37,321: INFO: main: Training : batch 386 Loss: 0.005502406415439621]
[2024-04-17 12:53:37,953: INFO: main: Training : batch 387 Loss: 0.015136260260199647]
[2024-04-17 12:53:38,578: INFO: main: Training : batch 388 Loss: 0.002030772756675117]
[2024-04-17 12:53:39,209: INFO: main: Training : batch 389 Loss: 0.021617140339845395]
[2024-04-17 12:53:39,837: INFO: main: Training : batch 390 Loss: 0.013393775749442439]
[2024-04-17 12:53:40,466: INFO: main: Training : batch 391 Loss: 0.007915630300494882]
[2024-04-17 12:53:41,093: INFO: main: Training : batch 392 Loss: 0.009502900098508939]
[2024-04-17 12:53:41,723: INFO: main: Training : batch 393 Loss: 0.001708498680850714]
[2024-04-17 12:53:42,351: INFO: main: Training : batch 394 Loss: 0.002237998976754385]
[2024-04-17 12:53:42,978: INFO: main: Training : batch 395 Loss: 0.0018477815391578418]
[2024-04-17 12:53:43,605: INFO: main: Training : batch 396 Loss: 0.0022190877338013663]
[2024-04-17 12:53:44,232: INFO: main: Training : batch 397 Loss: 0.011660356762411438]
[2024-04-17 12:53:44,862: INFO: main: Training : batch 398 Loss: 0.0014343234781695135]
[2024-04-17 12:53:45,485: INFO: main: Training : batch 399 Loss: 0.017660063882776307]
[2024-04-17 12:53:46,113: INFO: main: Training : batch 400 Loss: 0.004792265933041761]
[2024-04-17 12:53:46,737: INFO: main: Training : batch 401 Loss: 0.0047282937406983245]
[2024-04-17 12:53:47,371: INFO: main: Training : batch 402 Loss: 0.005110453217613001]
[2024-04-17 12:53:48,013: INFO: main: Training : batch 403 Loss: 0.004004400652933619]
[2024-04-17 12:53:48,648: INFO: main: Training : batch 404 Loss: 0.0013544144661460234]
[2024-04-17 12:53:49,281: INFO: main: Training : batch 405 Loss: 0.006583148793859334]
[2024-04-17 12:53:49,918: INFO: main: Training : batch 406 Loss: 0.0018015884956913203]
[2024-04-17 12:53:50,550: INFO: main: Training : batch 407 Loss: 0.023999877114459698]
[2024-04-17 12:53:51,177: INFO: main: Training : batch 408 Loss: 0.007025684537807888]
[2024-04-17 12:53:51,816: INFO: main: Training : batch 409 Loss: 0.005523534184522217]
[2024-04-17 12:53:52,449: INFO: main: Training : batch 410 Loss: 0.005437787502456673]
[2024-04-17 12:53:53,078: INFO: main: Training : batch 411 Loss: 0.001099285827160386]
[2024-04-17 12:53:53,707: INFO: main: Training : batch 412 Loss: 0.004497811045772236]
[2024-04-17 12:53:54,337: INFO: main: Training : batch 413 Loss: 0.005218813910866326]
[2024-04-17 12:53:54,964: INFO: main: Training : batch 414 Loss: 0.002480436790078754]
[2024-04-17 12:53:55,593: INFO: main: Training : batch 415 Loss: 0.006081220793818732]
[2024-04-17 12:53:56,221: INFO: main: Training : batch 416 Loss: 0.009893817587461513]
[2024-04-17 12:53:56,850: INFO: main: Training : batch 417 Loss: 0.011830233634039058]
[2024-04-17 12:53:57,481: INFO: main: Training : batch 418 Loss: 0.012553384323995043]
[2024-04-17 12:53:58,111: INFO: main: Training : batch 419 Loss: 0.027760188276250156]
[2024-04-17 12:53:58,740: INFO: main: Training : batch 420 Loss: 0.01625648934401942]
[2024-04-17 12:53:59,370: INFO: main: Training : batch 421 Loss: 0.002945170880225852]
[2024-04-17 12:53:59,997: INFO: main: Training : batch 422 Loss: 0.01338206187272807]
[2024-04-17 12:54:00,636: INFO: main: Training : batch 423 Loss: 0.003878454735660351]
[2024-04-17 12:54:01,270: INFO: main: Training : batch 424 Loss: 0.00487300368971411]
[2024-04-17 12:54:01,907: INFO: main: Training : batch 425 Loss: 0.0006149738079640182]
[2024-04-17 12:54:02,544: INFO: main: Training : batch 426 Loss: 0.0036518102330836896]
[2024-04-17 12:54:03,188: INFO: main: Training : batch 427 Loss: 0.004551603991984637]
[2024-04-17 12:54:03,818: INFO: main: Training : batch 428 Loss: 0.00890653932915921]
[2024-04-17 12:54:04,451: INFO: main: Training : batch 429 Loss: 0.0037099730693140928]
[2024-04-17 12:54:05,083: INFO: main: Training : batch 430 Loss: 0.0066043471220601615]
[2024-04-17 12:54:05,714: INFO: main: Training : batch 431 Loss: 0.0037821349820883316]
[2024-04-17 12:54:06,346: INFO: main: Training : batch 432 Loss: 0.0060265285464240525]
[2024-04-17 12:54:06,976: INFO: main: Training : batch 433 Loss: 0.007422839405789622]
[2024-04-17 12:54:07,607: INFO: main: Training : batch 434 Loss: 0.013801121974070492]
[2024-04-17 12:54:08,240: INFO: main: Training : batch 435 Loss: 0.00569630549512043]
[2024-04-17 12:54:08,870: INFO: main: Training : batch 436 Loss: 0.0018953210905762077]
[2024-04-17 12:54:09,504: INFO: main: Training : batch 437 Loss: 0.0032223782051620646]
[2024-04-17 12:54:10,139: INFO: main: Training : batch 438 Loss: 0.0036467499556627416]
[2024-04-17 12:54:10,768: INFO: main: Training : batch 439 Loss: 0.005177886003967463]
[2024-04-17 12:54:11,404: INFO: main: Training : batch 440 Loss: 0.012155644512806927]
[2024-04-17 12:54:12,038: INFO: main: Training : batch 441 Loss: 0.008206733233903757]
[2024-04-17 12:54:12,670: INFO: main: Training : batch 442 Loss: 0.0015770876130825943]
[2024-04-17 12:54:13,299: INFO: main: Training : batch 443 Loss: 0.003819718005632189]
[2024-04-17 12:54:13,937: INFO: main: Training : batch 444 Loss: 5.099088583764652e-05]
[2024-04-17 12:54:14,582: INFO: main: Training : batch 445 Loss: 0.010707636668216225]
[2024-04-17 12:54:15,218: INFO: main: Training : batch 446 Loss: 0.004592351421224721]
[2024-04-17 12:54:15,857: INFO: main: Training : batch 447 Loss: 0.011068049738795407]
[2024-04-17 12:54:16,490: INFO: main: Training : batch 448 Loss: 0.004440418579300615]
[2024-04-17 12:54:17,128: INFO: main: Training : batch 449 Loss: 0.04236661000852439]
[2024-04-17 12:54:17,759: INFO: main: Training : batch 450 Loss: 0.04609439255478068]
[2024-04-17 12:54:18,392: INFO: main: Training : batch 451 Loss: 0.0016975456477188074]
[2024-04-17 12:54:19,026: INFO: main: Training : batch 452 Loss: 0.004243833998750332]
[2024-04-17 12:54:19,653: INFO: main: Training : batch 453 Loss: 0.0035651763525716886]
[2024-04-17 12:54:20,287: INFO: main: Training : batch 454 Loss: 0.004388128666273719]
[2024-04-17 12:54:20,917: INFO: main: Training : batch 455 Loss: 0.008244713494823327]
[2024-04-17 12:54:21,550: INFO: main: Training : batch 456 Loss: 0.0013115068462695551]
[2024-04-17 12:54:22,184: INFO: main: Training : batch 457 Loss: 0.0036156277685309313]
[2024-04-17 12:54:22,815: INFO: main: Training : batch 458 Loss: 0.0007222015781451188]
[2024-04-17 12:54:23,447: INFO: main: Training : batch 459 Loss: 0.007618279116460899]
[2024-04-17 12:54:24,082: INFO: main: Training : batch 460 Loss: 0.006158032135672659]
[2024-04-17 12:54:24,714: INFO: main: Training : batch 461 Loss: 0.006752595814872106]
[2024-04-17 12:54:25,344: INFO: main: Training : batch 462 Loss: 0.012982813613732411]
[2024-04-17 12:54:25,978: INFO: main: Training : batch 463 Loss: 0.007478509922339633]
[2024-04-17 12:54:26,612: INFO: main: Training : batch 464 Loss: 0.0029316868749831686]
[2024-04-17 12:54:27,251: INFO: main: Training : batch 465 Loss: 0.004134586649545224]
[2024-04-17 12:54:27,890: INFO: main: Training : batch 466 Loss: 0.0046666425436029295]
[2024-04-17 12:54:28,538: INFO: main: Training : batch 467 Loss: 0.004089895743569893]
[2024-04-17 12:54:29,179: INFO: main: Training : batch 468 Loss: 0.007767799135254529]
[2024-04-17 12:54:29,816: INFO: main: Training : batch 469 Loss: 0.002614106440307451]
[2024-04-17 12:54:30,457: INFO: main: Training : batch 470 Loss: 0.0055049422153171474]
[2024-04-17 12:54:31,092: INFO: main: Training : batch 471 Loss: 0.0007492040968543789]
[2024-04-17 12:54:31,724: INFO: main: Training : batch 472 Loss: 0.008631402639997654]
[2024-04-17 12:54:32,360: INFO: main: Training : batch 473 Loss: 0.014832401000868384]
[2024-04-17 12:54:32,994: INFO: main: Training : batch 474 Loss: 0.02856984301699016]
[2024-04-17 12:54:33,629: INFO: main: Training : batch 475 Loss: 0.0031887904749324614]
[2024-04-17 12:54:34,265: INFO: main: Training : batch 476 Loss: 0.02030567099042629]
[2024-04-17 12:54:34,892: INFO: main: Training : batch 477 Loss: 0.004079716685380637]
[2024-04-17 12:54:35,523: INFO: main: Training : batch 478 Loss: 0.01835710096515921]
[2024-04-17 12:54:36,155: INFO: main: Training : batch 479 Loss: 0.00039305603647360167]
[2024-04-17 12:54:36,784: INFO: main: Training : batch 480 Loss: 0.011289283507006696]
[2024-04-17 12:54:37,418: INFO: main: Training : batch 481 Loss: 0.007067938036420435]
[2024-04-17 12:54:38,051: INFO: main: Training : batch 482 Loss: 0.005999922470269627]
[2024-04-17 12:54:38,681: INFO: main: Training : batch 483 Loss: 0.005868019684968605]
[2024-04-17 12:54:39,309: INFO: main: Training : batch 484 Loss: 0.010740415878167807]
[2024-04-17 12:54:39,943: INFO: main: Training : batch 485 Loss: 0.013393727065199888]
[2024-04-17 12:54:40,577: INFO: main: Training : batch 486 Loss: 0.004517333187405981]
[2024-04-17 12:54:41,222: INFO: main: Training : batch 487 Loss: 0.014354723672984356]
[2024-04-17 12:54:41,869: INFO: main: Training : batch 488 Loss: 0.0011697723070478651]
[2024-04-17 12:54:42,513: INFO: main: Training : batch 489 Loss: 0.009235436297954404]
[2024-04-17 12:54:43,154: INFO: main: Training : batch 490 Loss: 0.0021357497439109256]
[2024-04-17 12:54:43,793: INFO: main: Training : batch 491 Loss: 0.001748468817168783]
[2024-04-17 12:54:44,427: INFO: main: Training : batch 492 Loss: 0.005214423761181735]
[2024-04-17 12:54:45,057: INFO: main: Training : batch 493 Loss: 0.022148666608516335]
[2024-04-17 12:54:45,690: INFO: main: Training : batch 494 Loss: 0.00701560524565511]
[2024-04-17 12:54:46,319: INFO: main: Training : batch 495 Loss: 0.005304456263192437]
[2024-04-17 12:54:46,947: INFO: main: Training : batch 496 Loss: 0.0018763870272329379]
[2024-04-17 12:54:47,576: INFO: main: Training : batch 497 Loss: 0.004915423542455847]
[2024-04-17 12:54:48,210: INFO: main: Training : batch 498 Loss: 0.007639637593947229]
[2024-04-17 12:54:48,842: INFO: main: Training : batch 499 Loss: 0.019491791770581808]
[2024-04-17 12:54:49,473: INFO: main: Training : batch 500 Loss: 0.0056650544430842656]
[2024-04-17 12:54:50,110: INFO: main: Training : batch 501 Loss: 0.017802611710541885]
[2024-04-17 12:54:50,742: INFO: main: Training : batch 502 Loss: 0.0014125282144788513]
[2024-04-17 12:54:51,372: INFO: main: Training : batch 503 Loss: 0.00820369819378334]
[2024-04-17 12:54:52,009: INFO: main: Training : batch 504 Loss: 0.00597632361173972]
[2024-04-17 12:54:52,636: INFO: main: Training : batch 505 Loss: 0.0002809027664615772]
[2024-04-17 12:54:53,267: INFO: main: Training : batch 506 Loss: 0.010862765024970107]
[2024-04-17 12:54:53,895: INFO: main: Training : batch 507 Loss: 0.00129362139930984]
[2024-04-17 12:54:54,526: INFO: main: Training : batch 508 Loss: 0.008617575598748792]
[2024-04-17 12:54:55,163: INFO: main: Training : batch 509 Loss: 0.003332440300907529]
[2024-04-17 12:54:55,798: INFO: main: Training : batch 510 Loss: 0.0058471129012545205]
[2024-04-17 12:54:56,435: INFO: main: Training : batch 511 Loss: 0.004862880851588735]
[2024-04-17 12:54:57,079: INFO: main: Training : batch 512 Loss: 0.007487902654165399]
[2024-04-17 12:54:57,720: INFO: main: Training : batch 513 Loss: 0.017842871028736253]
[2024-04-17 12:54:58,349: INFO: main: Training : batch 514 Loss: 0.008671336432919282]
[2024-04-17 12:54:58,984: INFO: main: Training : batch 515 Loss: 0.00999644627432613]
[2024-04-17 12:54:59,614: INFO: main: Training : batch 516 Loss: 0.015957440831866166]
[2024-04-17 12:55:00,246: INFO: main: Training : batch 517 Loss: 0.003994555743364234]
[2024-04-17 12:55:00,878: INFO: main: Training : batch 518 Loss: 0.002681677943738869]
[2024-04-17 12:55:01,512: INFO: main: Training : batch 519 Loss: 0.005275530201556344]
[2024-04-17 12:55:02,149: INFO: main: Training : batch 520 Loss: 0.015355711857654117]
[2024-04-17 12:55:02,784: INFO: main: Training : batch 521 Loss: 0.012190335359619706]
[2024-04-17 12:55:03,420: INFO: main: Training : batch 522 Loss: 0.008847370964484248]
[2024-04-17 12:55:04,051: INFO: main: Training : batch 523 Loss: 0.014073117377666441]
[2024-04-17 12:55:04,680: INFO: main: Training : batch 524 Loss: 0.016194645779830184]
[2024-04-17 12:55:05,318: INFO: main: Training : batch 525 Loss: 0.007086886996852593]
[2024-04-17 12:55:05,953: INFO: main: Training : batch 526 Loss: 0.002154733965874984]
[2024-04-17 12:55:06,585: INFO: main: Training : batch 527 Loss: 0.008474767867978392]
[2024-04-17 12:55:07,219: INFO: main: Training : batch 528 Loss: 0.002605109479473766]
[2024-04-17 12:55:07,853: INFO: main: Training : batch 529 Loss: 0.001993356772659089]
[2024-04-17 12:55:08,497: INFO: main: Training : batch 530 Loss: 0.017703957567792514]
[2024-04-17 12:55:09,148: INFO: main: Training : batch 531 Loss: 0.003067405654253289]
[2024-04-17 12:55:09,792: INFO: main: Training : batch 532 Loss: 0.006432787083290976]
[2024-04-17 12:55:10,434: INFO: main: Training : batch 533 Loss: 0.013104966685722056]
[2024-04-17 12:55:11,073: INFO: main: Training : batch 534 Loss: 0.009677357265780133]
[2024-04-17 12:55:11,708: INFO: main: Training : batch 535 Loss: 0.00024661300773119206]
[2024-04-17 12:55:12,332: INFO: main: Training : batch 536 Loss: 0.003460263102790601]
[2024-04-17 12:55:12,965: INFO: main: Training : batch 537 Loss: 0.0006492359742884934]
[2024-04-17 12:55:13,596: INFO: main: Training : batch 538 Loss: 0.004283759047708979]
[2024-04-17 12:55:14,233: INFO: main: Training : batch 539 Loss: 0.0037315395048300934]
[2024-04-17 12:55:14,864: INFO: main: Training : batch 540 Loss: 0.0007294780586566855]
[2024-04-17 12:55:15,495: INFO: main: Training : batch 541 Loss: 0.006642277302943842]
[2024-04-17 12:55:16,126: INFO: main: Training : batch 542 Loss: 0.006518978656126369]
[2024-04-17 12:55:16,758: INFO: main: Training : batch 543 Loss: 0.0008653939687322731]
[2024-04-17 12:55:17,394: INFO: main: Training : batch 544 Loss: 0.01112268373240032]
[2024-04-17 12:55:18,027: INFO: main: Training : batch 545 Loss: 0.0022952590198875127]
[2024-04-17 12:55:18,662: INFO: main: Training : batch 546 Loss: 0.0017242562522587115]
[2024-04-17 12:55:19,296: INFO: main: Training : batch 547 Loss: 0.006849526666868919]
[2024-04-17 12:55:19,931: INFO: main: Training : batch 548 Loss: 0.015102556163785622]
[2024-04-17 12:55:20,563: INFO: main: Training : batch 549 Loss: 0.001550002386653467]
[2024-04-17 12:55:21,193: INFO: main: Training : batch 550 Loss: 0.009921623117191922]
[2024-04-17 12:55:21,833: INFO: main: Training : batch 551 Loss: 0.014865834709495464]
[2024-04-17 12:55:22,482: INFO: main: Training : batch 552 Loss: 0.0024095211761353615]
[2024-04-17 12:55:23,130: INFO: main: Training : batch 553 Loss: 0.003386671088033846]
[2024-04-17 12:55:23,772: INFO: main: Training : batch 554 Loss: 0.020296253157235353]
[2024-04-17 12:55:24,408: INFO: main: Training : batch 555 Loss: 0.001273038364983232]
[2024-04-17 12:55:25,043: INFO: main: Training : batch 556 Loss: 0.002656539310261993]
[2024-04-17 12:55:25,676: INFO: main: Training : batch 557 Loss: 0.007644885306988627]
[2024-04-17 12:55:26,306: INFO: main: Training : batch 558 Loss: 0.0064218924125379]
[2024-04-17 12:55:26,942: INFO: main: Training : batch 559 Loss: 0.007941519779082001]
[2024-04-17 12:55:27,576: INFO: main: Training : batch 560 Loss: 0.051965143491253134]
[2024-04-17 12:55:28,212: INFO: main: Training : batch 561 Loss: 0.00793946704894752]
[2024-04-17 12:55:28,845: INFO: main: Training : batch 562 Loss: 0.022521451561681914]
[2024-04-17 12:55:29,481: INFO: main: Training : batch 563 Loss: 0.00337829397744247]
[2024-04-17 12:55:30,116: INFO: main: Training : batch 564 Loss: 0.006402649328958432]
[2024-04-17 12:55:30,752: INFO: main: Training : batch 565 Loss: 0.008562381338230908]
[2024-04-17 12:55:31,379: INFO: main: Training : batch 566 Loss: 0.0016539564939526965]
[2024-04-17 12:55:32,010: INFO: main: Training : batch 567 Loss: 0.009061315854131094]
[2024-04-17 12:55:32,640: INFO: main: Training : batch 568 Loss: 0.003854490882841105]
[2024-04-17 12:55:33,273: INFO: main: Training : batch 569 Loss: 0.00867915107912718]
[2024-04-17 12:55:33,905: INFO: main: Training : batch 570 Loss: 0.004498891864117513]
[2024-04-17 12:55:34,538: INFO: main: Training : batch 571 Loss: 0.0010563377572544868]
[2024-04-17 12:55:35,181: INFO: main: Training : batch 572 Loss: 0.005306256565733578]
[2024-04-17 12:55:35,828: INFO: main: Training : batch 573 Loss: 0.004935559473214784]
[2024-04-17 12:55:36,471: INFO: main: Training : batch 574 Loss: 0.004429470893056017]
[2024-04-17 12:55:37,106: INFO: main: Training : batch 575 Loss: 0.014594392798316638]
[2024-04-17 12:55:37,744: INFO: main: Training : batch 576 Loss: 0.022463132320485798]
[2024-04-17 12:55:38,383: INFO: main: Training : batch 577 Loss: 0.0036209817433460194]
[2024-04-17 12:55:39,014: INFO: main: Training : batch 578 Loss: 0.010393097974629052]
[2024-04-17 12:55:39,644: INFO: main: Training : batch 579 Loss: 0.0037597158472740724]
[2024-04-17 12:55:40,281: INFO: main: Training : batch 580 Loss: 0.017595780609757777]
[2024-04-17 12:55:40,911: INFO: main: Training : batch 581 Loss: 0.004053736798602972]
[2024-04-17 12:55:41,542: INFO: main: Training : batch 582 Loss: 0.009446275561825247]
[2024-04-17 12:55:42,177: INFO: main: Training : batch 583 Loss: 0.018130550161760035]
[2024-04-17 12:55:42,810: INFO: main: Training : batch 584 Loss: 0.0009965154344341847]
[2024-04-17 12:55:43,441: INFO: main: Training : batch 585 Loss: 0.009508562915259451]
[2024-04-17 12:55:44,072: INFO: main: Training : batch 586 Loss: 0.019480192811013975]
[2024-04-17 12:55:44,702: INFO: main: Training : batch 587 Loss: 0.0046944271507844]
[2024-04-17 12:55:45,331: INFO: main: Training : batch 588 Loss: 0.010923014354651617]
[2024-04-17 12:55:45,959: INFO: main: Training : batch 589 Loss: 0.0035466720131155254]
[2024-04-17 12:55:46,593: INFO: main: Training : batch 590 Loss: 0.003374851085989357]
[2024-04-17 12:55:47,221: INFO: main: Training : batch 591 Loss: 0.003701884539270481]
[2024-04-17 12:55:47,852: INFO: main: Training : batch 592 Loss: 0.012295443894831566]
[2024-04-17 12:55:48,488: INFO: main: Training : batch 593 Loss: 0.0057788534611724314]
[2024-04-17 12:55:49,134: INFO: main: Training : batch 594 Loss: 0.00483854438618233]
[2024-04-17 12:55:49,772: INFO: main: Training : batch 595 Loss: 0.024219834048677735]
[2024-04-17 12:55:50,406: INFO: main: Training : batch 596 Loss: 0.01468993952003401]
[2024-04-17 12:55:51,042: INFO: main: Training : batch 597 Loss: 0.0037532635884563355]
[2024-04-17 12:55:51,685: INFO: main: Training : batch 598 Loss: 0.016328044214948087]
[2024-04-17 12:55:52,316: INFO: main: Training : batch 599 Loss: 0.006274727246868143]
[2024-04-17 12:55:52,950: INFO: main: Training : batch 600 Loss: 0.005099429124803596]
[2024-04-17 12:55:53,582: INFO: main: Training : batch 601 Loss: 0.017352149631753765]
[2024-04-17 12:55:54,214: INFO: main: Training : batch 602 Loss: 0.0030308863047697893]
[2024-04-17 12:55:54,845: INFO: main: Training : batch 603 Loss: 0.002963942152339862]
[2024-04-17 12:55:55,480: INFO: main: Training : batch 604 Loss: 0.010749881674512095]
[2024-04-17 12:55:56,106: INFO: main: Training : batch 605 Loss: 0.01599062745304081]
[2024-04-17 12:55:56,734: INFO: main: Training : batch 606 Loss: 0.003022937274659273]
[2024-04-17 12:55:57,362: INFO: main: Training : batch 607 Loss: 0.002121310275003503]
[2024-04-17 12:55:57,992: INFO: main: Training : batch 608 Loss: 0.005361262707491177]
[2024-04-17 12:55:58,618: INFO: main: Training : batch 609 Loss: 0.013709685734412561]
[2024-04-17 12:55:59,252: INFO: main: Training : batch 610 Loss: 0.007294266103685713]
[2024-04-17 12:55:59,886: INFO: main: Training : batch 611 Loss: 0.004156584943006947]
[2024-04-17 12:56:00,516: INFO: main: Training : batch 612 Loss: 0.004772524533851976]
[2024-04-17 12:56:01,142: INFO: main: Training : batch 613 Loss: 0.008206947337365907]
[2024-04-17 12:56:01,775: INFO: main: Training : batch 614 Loss: 0.004814365716162114]
[2024-04-17 12:56:02,413: INFO: main: Training : batch 615 Loss: 0.00862569097973285]
[2024-04-17 12:56:03,058: INFO: main: Training : batch 616 Loss: 0.00953348382110716]
[2024-04-17 12:56:03,697: INFO: main: Training : batch 617 Loss: 0.018259178639810542]
[2024-04-17 12:56:04,331: INFO: main: Training : batch 618 Loss: 0.01099200554220561]
[2024-04-17 12:56:04,971: INFO: main: Training : batch 619 Loss: 0.001826903949581025]
[2024-04-17 12:56:05,597: INFO: main: Training : batch 620 Loss: 0.010918262848659247]
[2024-04-17 12:56:06,227: INFO: main: Training : batch 621 Loss: 0.003852025441510073]
[2024-04-17 12:56:06,858: INFO: main: Training : batch 622 Loss: 0.005464983433744724]
[2024-04-17 12:56:07,485: INFO: main: Training : batch 623 Loss: 0.007976458840472026]
[2024-04-17 12:56:08,115: INFO: main: Training : batch 624 Loss: 0.004410330144615713]
[2024-04-17 12:56:08,748: INFO: main: Training : batch 625 Loss: 0.014184791900898091]
[2024-04-17 12:56:09,381: INFO: main: Training : batch 626 Loss: 0.004994735856175066]
[2024-04-17 12:56:10,012: INFO: main: Training : batch 627 Loss: 0.003063435046543293]
[2024-04-17 12:56:10,641: INFO: main: Training : batch 628 Loss: 0.0015831112425158259]
[2024-04-17 12:56:11,269: INFO: main: Training : batch 629 Loss: 0.003579545428373792]
[2024-04-17 12:56:11,899: INFO: main: Training : batch 630 Loss: 0.0077888320258333405]
[2024-04-17 12:56:12,526: INFO: main: Training : batch 631 Loss: 0.0026654707719493426]
[2024-04-17 12:56:13,154: INFO: main: Training : batch 632 Loss: 0.01637884375757473]
[2024-04-17 12:56:13,784: INFO: main: Training : batch 633 Loss: 0.002639895557289544]
[2024-04-17 12:56:14,415: INFO: main: Training : batch 634 Loss: 0.04790551100982647]
[2024-04-17 12:56:15,046: INFO: main: Training : batch 635 Loss: 0.002979905091738192]
[2024-04-17 12:56:15,682: INFO: main: Training : batch 636 Loss: 0.002327710039678421]
[2024-04-17 12:56:16,326: INFO: main: Training : batch 637 Loss: 0.004870106215644373]
[2024-04-17 12:56:16,973: INFO: main: Training : batch 638 Loss: 0.019851126881106684]
[2024-04-17 12:56:17,619: INFO: main: Training : batch 639 Loss: 0.004396364584502216]
[2024-04-17 12:56:18,260: INFO: main: Training : batch 640 Loss: 0.006469980215878225]
[2024-04-17 12:56:18,904: INFO: main: Training : batch 641 Loss: 0.00565868044352914]
[2024-04-17 12:56:19,539: INFO: main: Training : batch 642 Loss: 0.005528395580004082]
[2024-04-17 12:56:20,173: INFO: main: Training : batch 643 Loss: 0.0065163310568556905]
[2024-04-17 12:56:20,804: INFO: main: Training : batch 644 Loss: 0.0044303750582203585]
[2024-04-17 12:56:21,438: INFO: main: Training : batch 645 Loss: 0.0041741654004546925]
[2024-04-17 12:56:22,072: INFO: main: Training : batch 646 Loss: 0.0002504025056507979]
[2024-04-17 12:56:22,701: INFO: main: Training : batch 647 Loss: 0.02317425958840002]
[2024-04-17 12:56:23,335: INFO: main: Training : batch 648 Loss: 0.002110008891649104]
[2024-04-17 12:56:23,972: INFO: main: Training : batch 649 Loss: 0.0058289058972822915]
[2024-04-17 12:56:24,608: INFO: main: Training : batch 650 Loss: 0.01971767924477718]
[2024-04-17 12:56:25,242: INFO: main: Training : batch 651 Loss: 0.008859923940869263]
[2024-04-17 12:56:25,874: INFO: main: Training : batch 652 Loss: 0.0030050995745831578]
[2024-04-17 12:56:26,512: INFO: main: Training : batch 653 Loss: 0.005178142540734258]
[2024-04-17 12:56:27,151: INFO: main: Training : batch 654 Loss: 0.006654701805128075]
[2024-04-17 12:56:27,787: INFO: main: Training : batch 655 Loss: 0.00731169694757581]
[2024-04-17 12:56:28,419: INFO: main: Training : batch 656 Loss: 0.00640220304727781]
[2024-04-17 12:56:29,057: INFO: main: Training : batch 657 Loss: 0.007431438768910238]
[2024-04-17 12:56:29,704: INFO: main: Training : batch 658 Loss: 0.0019203872080012143]
[2024-04-17 12:56:30,353: INFO: main: Training : batch 659 Loss: 0.005636996946292787]
[2024-04-17 12:56:31,001: INFO: main: Training : batch 660 Loss: 0.005219604324244709]
[2024-04-17 12:56:31,644: INFO: main: Training : batch 661 Loss: 0.001433220402374707]
[2024-04-17 12:56:32,280: INFO: main: Training : batch 662 Loss: 0.02432898651861976]
[2024-04-17 12:56:32,914: INFO: main: Training : batch 663 Loss: 0.0015514736235060485]
[2024-04-17 12:56:33,550: INFO: main: Training : batch 664 Loss: 0.003897443568769961]
[2024-04-17 12:56:34,184: INFO: main: Training : batch 665 Loss: 0.007129513275070774]
[2024-04-17 12:56:34,816: INFO: main: Training : batch 666 Loss: 0.020579519905212944]
[2024-04-17 12:56:35,449: INFO: main: Training : batch 667 Loss: 0.0074648191361264445]
[2024-04-17 12:56:36,087: INFO: main: Training : batch 668 Loss: 0.009743072635842717]
[2024-04-17 12:56:36,720: INFO: main: Training : batch 669 Loss: 0.008729534104131109]
[2024-04-17 12:56:37,356: INFO: main: Training : batch 670 Loss: 0.010825560552371296]
[2024-04-17 12:56:37,991: INFO: main: Training : batch 671 Loss: 0.007651831054001629]
[2024-04-17 12:56:38,626: INFO: main: Training : batch 672 Loss: 0.02282424465452243]
[2024-04-17 12:56:39,259: INFO: main: Training : batch 673 Loss: 0.004046989405973911]
[2024-04-17 12:56:39,893: INFO: main: Training : batch 674 Loss: 0.006211601334662279]
[2024-04-17 12:56:40,527: INFO: main: Training : batch 675 Loss: 0.00428285898482524]
[2024-04-17 12:56:41,162: INFO: main: Training : batch 676 Loss: 0.01050782191289185]
[2024-04-17 12:56:41,793: INFO: main: Training : batch 677 Loss: 0.011930841541578217]
[2024-04-17 12:56:42,444: INFO: main: Training : batch 678 Loss: 0.0012869706126175675]
[2024-04-17 12:56:43,086: INFO: main: Training : batch 679 Loss: 0.011497182588287221]
[2024-04-17 12:56:43,728: INFO: main: Training : batch 680 Loss: 0.01587120984467343]
[2024-04-17 12:56:44,367: INFO: main: Training : batch 681 Loss: 0.007035032160804335]
[2024-04-17 12:56:45,010: INFO: main: Training : batch 682 Loss: 0.011856482600088099]
[2024-04-17 12:56:45,652: INFO: main: Training : batch 683 Loss: 0.006563242748557918]
[2024-04-17 12:56:46,290: INFO: main: Training : batch 684 Loss: 0.019170765442472787]
[2024-04-17 12:56:46,926: INFO: main: Training : batch 685 Loss: 0.005198480435552854]
[2024-04-17 12:56:47,559: INFO: main: Training : batch 686 Loss: 0.02333568712522569]
[2024-04-17 12:56:48,191: INFO: main: Training : batch 687 Loss: 0.007218654997889848]
[2024-04-17 12:56:48,827: INFO: main: Training : batch 688 Loss: 0.004928774703078404]
[2024-04-17 12:56:49,463: INFO: main: Training : batch 689 Loss: 0.013941662793761248]
[2024-04-17 12:56:50,096: INFO: main: Training : batch 690 Loss: 0.001941619999140426]
[2024-04-17 12:56:50,729: INFO: main: Training : batch 691 Loss: 0.007759054931669716]
[2024-04-17 12:56:51,361: INFO: main: Training : batch 692 Loss: 0.001266431592805091]
[2024-04-17 12:56:51,993: INFO: main: Training : batch 693 Loss: 0.012408977757343672]
[2024-04-17 12:56:52,625: INFO: main: Training : batch 694 Loss: 0.005389741899923857]
[2024-04-17 12:56:53,265: INFO: main: Training : batch 695 Loss: 0.015042195331472104]
[2024-04-17 12:56:53,899: INFO: main: Training : batch 696 Loss: 0.011039431043518384]
[2024-04-17 12:56:54,534: INFO: main: Training : batch 697 Loss: 0.0077705344831998195]
[2024-04-17 12:56:55,165: INFO: main: Training : batch 698 Loss: 0.01265516588591735]
[2024-04-17 12:56:55,800: INFO: main: Training : batch 699 Loss: 0.003258551033706934]
[2024-04-17 12:56:56,444: INFO: main: Training : batch 700 Loss: 0.0004893868324276322]
[2024-04-17 12:56:57,085: INFO: main: Training : batch 701 Loss: 0.007988947770039554]
[2024-04-17 12:56:57,724: INFO: main: Training : batch 702 Loss: 0.004423465064769975]
[2024-04-17 12:56:58,364: INFO: main: Training : batch 703 Loss: 0.0014785324775038933]
[2024-04-17 12:56:59,002: INFO: main: Training : batch 704 Loss: 0.0019896875588324026]
[2024-04-17 12:56:59,634: INFO: main: Training : batch 705 Loss: 0.0033437233983946595]
[2024-04-17 12:57:00,266: INFO: main: Training : batch 706 Loss: 0.005057300023030022]
[2024-04-17 12:57:00,894: INFO: main: Training : batch 707 Loss: 0.0014866040199872973]
[2024-04-17 12:57:01,531: INFO: main: Training : batch 708 Loss: 0.026907644769205978]
[2024-04-17 12:57:02,162: INFO: main: Training : batch 709 Loss: 0.0033977057111386467]
[2024-04-17 12:57:02,796: INFO: main: Training : batch 710 Loss: 0.004280314126706358]
[2024-04-17 12:57:03,431: INFO: main: Training : batch 711 Loss: 0.020324770693043153]
[2024-04-17 12:57:04,063: INFO: main: Training : batch 712 Loss: 0.009886531975998638]
[2024-04-17 12:57:04,692: INFO: main: Training : batch 713 Loss: 0.02119931457882216]
[2024-04-17 12:57:05,326: INFO: main: Training : batch 714 Loss: 0.026536438367138548]
[2024-04-17 12:57:05,958: INFO: main: Training : batch 715 Loss: 0.010046306222502722]
[2024-04-17 12:57:06,587: INFO: main: Training : batch 716 Loss: 0.00722977863260311]
[2024-04-17 12:57:07,216: INFO: main: Training : batch 717 Loss: 0.001981656370023543]
[2024-04-17 12:57:07,845: INFO: main: Training : batch 718 Loss: 0.0035600691835168674]
[2024-04-17 12:57:08,478: INFO: main: Training : batch 719 Loss: 0.00030551469988219636]
[2024-04-17 12:57:09,108: INFO: main: Training : batch 720 Loss: 0.01058146152226643]
[2024-04-17 12:57:09,747: INFO: main: Training : batch 721 Loss: 0.009579017149455034]
[2024-04-17 12:57:10,384: INFO: main: Training : batch 722 Loss: 0.003376646771743007]
[2024-04-17 12:57:11,025: INFO: main: Training : batch 723 Loss: 0.01215741298681679]
[2024-04-17 12:57:11,661: INFO: main: Training : batch 724 Loss: 0.0041341802940832225]
[2024-04-17 12:57:12,302: INFO: main: Training : batch 725 Loss: 0.0045958815984203085]
[2024-04-17 12:57:12,934: INFO: main: Training : batch 726 Loss: 0.008358950434911248]
[2024-04-17 12:57:13,568: INFO: main: Training : batch 727 Loss: 0.003938533176591661]
[2024-04-17 12:57:14,195: INFO: main: Training : batch 728 Loss: 0.0016708408396626316]
[2024-04-17 12:57:14,825: INFO: main: Training : batch 729 Loss: 0.0011266342326392872]
[2024-04-17 12:57:15,454: INFO: main: Training : batch 730 Loss: 0.0068351663706924075]
[2024-04-17 12:57:16,087: INFO: main: Training : batch 731 Loss: 0.0057648527795745325]
[2024-04-17 12:57:16,721: INFO: main: Training : batch 732 Loss: 0.0433716976119706]
[2024-04-17 12:57:17,354: INFO: main: Training : batch 733 Loss: 0.008905329305806365]
[2024-04-17 12:57:17,986: INFO: main: Training : batch 734 Loss: 0.010443266447744931]
[2024-04-17 12:57:18,620: INFO: main: Training : batch 735 Loss: 0.006110491191494464]
[2024-04-17 12:57:19,248: INFO: main: Training : batch 736 Loss: 0.005761171225885796]
[2024-04-17 12:57:19,890: INFO: main: Training : batch 737 Loss: 0.0009521244947801094]
[2024-04-17 12:57:20,520: INFO: main: Training : batch 738 Loss: 0.010538497122079892]
[2024-04-17 12:57:21,153: INFO: main: Training : batch 739 Loss: 0.002683110653464099]
[2024-04-17 12:57:21,783: INFO: main: Training : batch 740 Loss: 0.00966828627163509]
[2024-04-17 12:57:22,408: INFO: main: Training : batch 741 Loss: 0.016374640261300113]
[2024-04-17 12:57:23,048: INFO: main: Training : batch 742 Loss: 0.0026406975368459982]
[2024-04-17 12:57:23,686: INFO: main: Training : batch 743 Loss: 0.00667213002428433]
[2024-04-17 12:57:24,324: INFO: main: Training : batch 744 Loss: 0.0062644041304091985]
[2024-04-17 12:57:24,960: INFO: main: Training : batch 745 Loss: 0.002044280271481796]
[2024-04-17 12:57:25,592: INFO: main: Training : batch 746 Loss: 0.013740761820461562]
[2024-04-17 12:57:26,229: INFO: main: Training : batch 747 Loss: 0.0019900578688360126]
[2024-04-17 12:57:26,859: INFO: main: Training : batch 748 Loss: 0.0032286472149861763]
[2024-04-17 12:57:27,487: INFO: main: Training : batch 749 Loss: 0.0076573701909861385]
[2024-04-17 12:57:28,117: INFO: main: Training : batch 750 Loss: 0.001332135360888212]
[2024-04-17 12:57:28,747: INFO: main: Training : batch 751 Loss: 0.005099090376691424]
[2024-04-17 12:57:29,381: INFO: main: Training : batch 752 Loss: 0.008432689380719784]
[2024-04-17 12:57:30,016: INFO: main: Training : batch 753 Loss: 0.001992754845757021]
[2024-04-17 12:57:30,645: INFO: main: Training : batch 754 Loss: 0.01510307393094803]
[2024-04-17 12:57:31,267: INFO: main: Training : batch 755 Loss: 0.002437269081589805]
[2024-04-17 12:57:31,902: INFO: main: Training : batch 756 Loss: 0.003008693586926048]
[2024-04-17 12:57:32,531: INFO: main: Training : batch 757 Loss: 0.0020970459257852883]
[2024-04-17 12:57:33,162: INFO: main: Training : batch 758 Loss: 0.004023233654703075]
[2024-04-17 12:57:33,788: INFO: main: Training : batch 759 Loss: 0.005425107695795965]
[2024-04-17 12:57:34,417: INFO: main: Training : batch 760 Loss: 0.002497812955406623]
[2024-04-17 12:57:35,046: INFO: main: Training : batch 761 Loss: 0.02242374405069527]
[2024-04-17 12:57:35,674: INFO: main: Training : batch 762 Loss: 0.002166747493846984]
[2024-04-17 12:57:36,318: INFO: main: Training : batch 763 Loss: 0.015483766096950995]
[2024-04-17 12:57:36,953: INFO: main: Training : batch 764 Loss: 0.021110660422038498]
[2024-04-17 12:57:37,597: INFO: main: Training : batch 765 Loss: 0.0026761195564713705]
[2024-04-17 12:57:38,230: INFO: main: Training : batch 766 Loss: 0.0030568427027898065]
[2024-04-17 12:57:38,872: INFO: main: Training : batch 767 Loss: 0.001269681985633602]
[2024-04-17 12:57:39,509: INFO: main: Training : batch 768 Loss: 0.021140998275159636]
[2024-04-17 12:57:40,141: INFO: main: Training : batch 769 Loss: 0.007278071699913554]
[2024-04-17 12:57:40,775: INFO: main: Training : batch 770 Loss: 0.004143521545494156]
[2024-04-17 12:57:41,407: INFO: main: Training : batch 771 Loss: 0.003332729202820508]
[2024-04-17 12:57:42,049: INFO: main: Training : batch 772 Loss: 0.006987199561037442]
[2024-04-17 12:57:42,679: INFO: main: Training : batch 773 Loss: 0.0033733915561787644]
[2024-04-17 12:57:43,305: INFO: main: Training : batch 774 Loss: 0.004647631052238506]
[2024-04-17 12:57:43,936: INFO: main: Training : batch 775 Loss: 0.001095842798900592]
[2024-04-17 12:57:44,566: INFO: main: Training : batch 776 Loss: 0.0008554742337344209]
[2024-04-17 12:57:45,197: INFO: main: Training : batch 777 Loss: 0.003750799937802297]
[2024-04-17 12:57:45,828: INFO: main: Training : batch 778 Loss: 0.009694749794293672]
[2024-04-17 12:57:46,462: INFO: main: Training : batch 779 Loss: 0.0007934664074320592]
[2024-04-17 12:57:47,094: INFO: main: Training : batch 780 Loss: 0.004297131836546849]
[2024-04-17 12:57:47,723: INFO: main: Training : batch 781 Loss: 0.014514065100597315]
[2024-04-17 12:57:48,350: INFO: main: Training : batch 782 Loss: 0.0017178196603310919]
[2024-04-17 12:57:48,976: INFO: main: Training : batch 783 Loss: 0.0140456971141852]
[2024-04-17 12:57:49,609: INFO: main: Training : batch 784 Loss: 0.002019766732660976]
[2024-04-17 12:57:50,247: INFO: main: Training : batch 785 Loss: 0.005296733160222032]
[2024-04-17 12:57:50,880: INFO: main: Training : batch 786 Loss: 0.003840961354888104]
[2024-04-17 12:57:51,517: INFO: main: Training : batch 787 Loss: 0.02008422426283222]
[2024-04-17 12:57:52,158: INFO: main: Training : batch 788 Loss: 0.0012245977591721728]
[2024-04-17 12:57:52,795: INFO: main: Training : batch 789 Loss: 0.010142769348118757]
[2024-04-17 12:57:53,432: INFO: main: Training : batch 790 Loss: 0.0125376120460828]
[2024-04-17 12:57:54,059: INFO: main: Training : batch 791 Loss: 0.011829015132758625]
[2024-04-17 12:57:54,689: INFO: main: Training : batch 792 Loss: 0.0055692723941388024]
[2024-04-17 12:57:55,322: INFO: main: Training : batch 793 Loss: 0.010482389735831432]
[2024-04-17 12:57:55,954: INFO: main: Training : batch 794 Loss: 0.00498993749528131]
[2024-04-17 12:57:56,582: INFO: main: Training : batch 795 Loss: 0.0005373218724321507]
[2024-04-17 12:57:57,210: INFO: main: Training : batch 796 Loss: 0.008267410562412817]
[2024-04-17 12:57:57,836: INFO: main: Training : batch 797 Loss: 0.008668370187052946]
[2024-04-17 12:57:58,469: INFO: main: Training : batch 798 Loss: 0.0033910367178904667]
[2024-04-17 12:57:59,098: INFO: main: Training : batch 799 Loss: 0.0022131638835061703]
[2024-04-17 12:57:59,730: INFO: main: Training : batch 800 Loss: 0.010023604764452416]
[2024-04-17 12:58:00,356: INFO: main: Training : batch 801 Loss: 0.0019703685041499042]
[2024-04-17 12:58:00,986: INFO: main: Training : batch 802 Loss: 0.0035654998075867264]
[2024-04-17 12:58:01,614: INFO: main: Training : batch 803 Loss: 0.01329767217832149]
[2024-04-17 12:58:02,243: INFO: main: Training : batch 804 Loss: 0.0028705099278011075]
[2024-04-17 12:58:02,872: INFO: main: Training : batch 805 Loss: 0.0025305484862486317]
[2024-04-17 12:58:03,513: INFO: main: Training : batch 806 Loss: 0.009065657250266793]
[2024-04-17 12:58:04,154: INFO: main: Training : batch 807 Loss: 0.0011652921786694252]
[2024-04-17 12:58:04,792: INFO: main: Training : batch 808 Loss: 0.014342712248995743]
[2024-04-17 12:58:05,428: INFO: main: Training : batch 809 Loss: 0.008084413187138085]
[2024-04-17 12:58:06,067: INFO: main: Training : batch 810 Loss: 0.008216324517779554]
[2024-04-17 12:58:06,698: INFO: main: Training : batch 811 Loss: 0.001498764394264654]
[2024-04-17 12:58:07,330: INFO: main: Training : batch 812 Loss: 0.004127983457754175]
[2024-04-17 12:58:07,960: INFO: main: Training : batch 813 Loss: 0.0008301910852343631]
[2024-04-17 12:58:08,593: INFO: main: Training : batch 814 Loss: 0.0043630386670141744]
[2024-04-17 12:58:09,223: INFO: main: Training : batch 815 Loss: 0.0013988112698257147]
[2024-04-17 12:58:09,855: INFO: main: Training : batch 816 Loss: 0.0017380435436897613]
[2024-04-17 12:58:10,484: INFO: main: Training : batch 817 Loss: 0.00047427430402662306]
[2024-04-17 12:58:11,117: INFO: main: Training : batch 818 Loss: 0.0024238722588697226]
[2024-04-17 12:58:11,749: INFO: main: Training : batch 819 Loss: 0.0037578888697368438]
[2024-04-17 12:58:12,379: INFO: main: Training : batch 820 Loss: 0.0006034963889646797]
[2024-04-17 12:58:13,010: INFO: main: Training : batch 821 Loss: 0.009349788892711527]
[2024-04-17 12:58:13,639: INFO: main: Training : batch 822 Loss: 0.0028416163659624257]
[2024-04-17 12:58:14,267: INFO: main: Training : batch 823 Loss: 0.004557561549891237]
[2024-04-17 12:58:14,900: INFO: main: Training : batch 824 Loss: 0.015320294509992423]
[2024-04-17 12:58:15,530: INFO: main: Training : batch 825 Loss: 0.00911566026567585]
[2024-04-17 12:58:16,160: INFO: main: Training : batch 826 Loss: 0.006534640177552044]
[2024-04-17 12:58:16,799: INFO: main: Training : batch 827 Loss: 0.013763320623226383]
[2024-04-17 12:58:17,439: INFO: main: Training : batch 828 Loss: 0.012342162353589427]
[2024-04-17 12:58:18,071: INFO: main: Training : batch 829 Loss: 0.002932834927774759]
[2024-04-17 12:58:18,708: INFO: main: Training : batch 830 Loss: 0.00863420152143541]
[2024-04-17 12:58:19,342: INFO: main: Training : batch 831 Loss: 0.007361878843027573]
[2024-04-17 12:58:19,980: INFO: main: Training : batch 832 Loss: 0.010850726299558132]
[2024-04-17 12:58:20,611: INFO: main: Training : batch 833 Loss: 0.010330041440902334]
[2024-04-17 12:58:21,242: INFO: main: Training : batch 834 Loss: 0.007428716894678025]
[2024-04-17 12:58:21,875: INFO: main: Training : batch 835 Loss: 0.003625704715781518]
[2024-04-17 12:58:22,504: INFO: main: Training : batch 836 Loss: 0.00572682392734881]
[2024-04-17 12:58:23,134: INFO: main: Training : batch 837 Loss: 0.012507399162515268]
[2024-04-17 12:58:23,759: INFO: main: Training : batch 838 Loss: 0.008172490015780898]
[2024-04-17 12:58:24,391: INFO: main: Training : batch 839 Loss: 0.0029572432204511626]
[2024-04-17 12:58:25,023: INFO: main: Training : batch 840 Loss: 0.013615284316699575]
[2024-04-17 12:58:25,654: INFO: main: Training : batch 841 Loss: 0.007892757464280543]
[2024-04-17 12:58:26,287: INFO: main: Training : batch 842 Loss: 0.014866054031734584]
[2024-04-17 12:58:26,914: INFO: main: Training : batch 843 Loss: 0.004467325865015177]
[2024-04-17 12:58:27,551: INFO: main: Training : batch 844 Loss: 0.013548209696898501]
[2024-04-17 12:58:28,182: INFO: main: Training : batch 845 Loss: 0.007061587923545534]
[2024-04-17 12:58:28,813: INFO: main: Training : batch 846 Loss: 0.0003761487179380889]
[2024-04-17 12:58:29,439: INFO: main: Training : batch 847 Loss: 0.003714585538218235]
[2024-04-17 12:58:30,069: INFO: main: Training : batch 848 Loss: 0.008811794868658669]
[2024-04-17 12:58:30,706: INFO: main: Training : batch 849 Loss: 0.013505146564657344]
[2024-04-17 12:58:31,351: INFO: main: Training : batch 850 Loss: 0.006426640562687468]
[2024-04-17 12:58:31,990: INFO: main: Training : batch 851 Loss: 0.0016119165822544544]
[2024-04-17 12:58:32,624: INFO: main: Training : batch 852 Loss: 0.002712336441087897]
[2024-04-17 12:58:33,263: INFO: main: Training : batch 853 Loss: 0.003319910687095367]
[2024-04-17 12:58:33,893: INFO: main: Training : batch 854 Loss: 0.00410644039817637]
[2024-04-17 12:58:34,526: INFO: main: Training : batch 855 Loss: 0.009637338934189665]
[2024-04-17 12:58:35,158: INFO: main: Training : batch 856 Loss: 0.0025062706030706526]
[2024-04-17 12:58:35,792: INFO: main: Training : batch 857 Loss: 0.005145555176299724]
[2024-04-17 12:58:36,420: INFO: main: Training : batch 858 Loss: 0.00109862867117982]
[2024-04-17 12:58:37,054: INFO: main: Training : batch 859 Loss: 0.013224987444048932]
[2024-04-17 12:58:37,687: INFO: main: Training : batch 860 Loss: 0.0051214519894657706]
[2024-04-17 12:58:38,316: INFO: main: Training : batch 861 Loss: 0.005960645886448989]
[2024-04-17 12:58:38,946: INFO: main: Training : batch 862 Loss: 0.0016010612250934278]
[2024-04-17 12:58:39,577: INFO: main: Training : batch 863 Loss: 0.04222501997911719]
[2024-04-17 12:58:40,212: INFO: main: Training : batch 864 Loss: 0.004981190314848652]
[2024-04-17 12:58:40,844: INFO: main: Training : batch 865 Loss: 0.008131949891298604]
[2024-04-17 12:58:41,469: INFO: main: Training : batch 866 Loss: 0.0013805706468229615]
[2024-04-17 12:58:42,102: INFO: main: Training : batch 867 Loss: 0.016722133624511762]
[2024-04-17 12:58:42,729: INFO: main: Training : batch 868 Loss: 0.000943159360075416]
[2024-04-17 12:58:43,360: INFO: main: Training : batch 869 Loss: 0.006581250087207083]
[2024-04-17 12:58:43,996: INFO: main: Training : batch 870 Loss: 0.010154627230974746]
[2024-04-17 12:58:44,634: INFO: main: Training : batch 871 Loss: 0.0035321613838043193]
[2024-04-17 12:58:45,268: INFO: main: Training : batch 872 Loss: 0.009030477304971911]
[2024-04-17 12:58:45,906: INFO: main: Training : batch 873 Loss: 0.01510625972663675]
[2024-04-17 12:58:46,556: INFO: main: Training : batch 874 Loss: 0.0031109724943952635]
[2024-04-17 12:58:47,196: INFO: main: Training : batch 875 Loss: 0.01467156710062235]
[2024-04-17 12:58:47,826: INFO: main: Training : batch 876 Loss: 0.0032344067702471144]
[2024-04-17 12:58:48,458: INFO: main: Training : batch 877 Loss: 0.009134993667275555]
[2024-04-17 12:58:49,092: INFO: main: Training : batch 878 Loss: 0.009633866322616863]
[2024-04-17 12:58:49,727: INFO: main: Training : batch 879 Loss: 0.005776841053550707]
[2024-04-17 12:58:50,354: INFO: main: Training : batch 880 Loss: 0.010593696338309122]
[2024-04-17 12:58:50,991: INFO: main: Training : batch 881 Loss: 0.00785642578671783]
[2024-04-17 12:58:51,626: INFO: main: Training : batch 882 Loss: 0.006708020993580329]
[2024-04-17 12:58:52,257: INFO: main: Training : batch 883 Loss: 0.00726104415713349]
[2024-04-17 12:58:52,885: INFO: main: Training : batch 884 Loss: 0.008118754456155616]
[2024-04-17 12:58:53,521: INFO: main: Training : batch 885 Loss: 0.01589052246512813]
[2024-04-17 12:58:54,150: INFO: main: Training : batch 886 Loss: 0.010331556299725025]
[2024-04-17 12:58:54,785: INFO: main: Training : batch 887 Loss: 0.0028336051606512864]
[2024-04-17 12:58:55,420: INFO: main: Training : batch 888 Loss: 0.017088318425621134]
[2024-04-17 12:58:56,053: INFO: main: Training : batch 889 Loss: 0.006725316438181425]
[2024-04-17 12:58:56,686: INFO: main: Training : batch 890 Loss: 0.01640098521867392]
[2024-04-17 12:58:57,317: INFO: main: Training : batch 891 Loss: 0.0019309872462234747]
[2024-04-17 12:58:57,955: INFO: main: Training : batch 892 Loss: 0.013411009892976226]
[2024-04-17 12:58:58,599: INFO: main: Training : batch 893 Loss: 0.006259974851376329]
[2024-04-17 12:58:59,240: INFO: main: Training : batch 894 Loss: 0.0017394318196917139]
[2024-04-17 12:58:59,885: INFO: main: Training : batch 895 Loss: 0.0018265269513900242]
[2024-04-17 12:59:00,530: INFO: main: Training : batch 896 Loss: 0.009752060246371683]
[2024-04-17 12:59:01,163: INFO: main: Training : batch 897 Loss: 0.007049195931267008]
[2024-04-17 12:59:01,795: INFO: main: Training : batch 898 Loss: 0.0034688966434303302]
[2024-04-17 12:59:02,431: INFO: main: Training : batch 899 Loss: 0.010732236679785609]
[2024-04-17 12:59:03,067: INFO: main: Training : batch 900 Loss: 0.0038173673115104918]
[2024-04-17 12:59:03,703: INFO: main: Training : batch 901 Loss: 0.0032380581501489174]
[2024-04-17 12:59:04,339: INFO: main: Training : batch 902 Loss: 0.0024442089223768687]
[2024-04-17 12:59:04,973: INFO: main: Training : batch 903 Loss: 0.011632006812825373]
[2024-04-17 12:59:05,604: INFO: main: Training : batch 904 Loss: 0.03712716151229173]
[2024-04-17 12:59:06,239: INFO: main: Training : batch 905 Loss: 0.005353682664823303]
[2024-04-17 12:59:06,874: INFO: main: Training : batch 906 Loss: 0.0070488773003559]
[2024-04-17 12:59:07,507: INFO: main: Training : batch 907 Loss: 0.002489593126900359]
[2024-04-17 12:59:08,140: INFO: main: Training : batch 908 Loss: 0.012220711959799095]
[2024-04-17 12:59:08,775: INFO: main: Training : batch 909 Loss: 0.006916938192040245]
[2024-04-17 12:59:09,414: INFO: main: Training : batch 910 Loss: 0.004596461680808934]
[2024-04-17 12:59:10,049: INFO: main: Training : batch 911 Loss: 0.022558498902980725]
[2024-04-17 12:59:10,691: INFO: main: Training : batch 912 Loss: 0.019529990769379946]
[2024-04-17 12:59:11,336: INFO: main: Training : batch 913 Loss: 0.016479068267478938]
[2024-04-17 12:59:11,982: INFO: main: Training : batch 914 Loss: 0.004589668583265053]
[2024-04-17 12:59:12,624: INFO: main: Training : batch 915 Loss: 0.012262540081151626]
[2024-04-17 12:59:13,264: INFO: main: Training : batch 916 Loss: 0.0063207277055137385]
[2024-04-17 12:59:13,904: INFO: main: Training : batch 917 Loss: 0.0004138095612100448]
[2024-04-17 12:59:14,535: INFO: main: Training : batch 918 Loss: 0.0029709805226113607]
[2024-04-17 12:59:15,168: INFO: main: Training : batch 919 Loss: 0.018564733770080263]
[2024-04-17 12:59:15,800: INFO: main: Training : batch 920 Loss: 0.009459737901237317]
[2024-04-17 12:59:16,437: INFO: main: Training : batch 921 Loss: 0.00263548546153715]
[2024-04-17 12:59:17,068: INFO: main: Training : batch 922 Loss: 0.02461189697991457]
[2024-04-17 12:59:17,703: INFO: main: Training : batch 923 Loss: 0.010674585904035779]
[2024-04-17 12:59:18,333: INFO: main: Training : batch 924 Loss: 0.006729565706642123]
[2024-04-17 12:59:18,966: INFO: main: Training : batch 925 Loss: 0.015030570943515812]
[2024-04-17 12:59:19,595: INFO: main: Training : batch 926 Loss: 0.0029509472407809097]
[2024-04-17 12:59:20,227: INFO: main: Training : batch 927 Loss: 0.0016389793748418707]
[2024-04-17 12:59:20,851: INFO: main: Training : batch 928 Loss: 0.0039710457440764926]
[2024-04-17 12:59:21,485: INFO: main: Training : batch 929 Loss: 0.007922738708328898]
[2024-04-17 12:59:22,120: INFO: main: Training : batch 930 Loss: 0.002686561981716165]
[2024-04-17 12:59:22,757: INFO: main: Training : batch 931 Loss: 0.0009579998539800419]
[2024-04-17 12:59:23,391: INFO: main: Training : batch 932 Loss: 0.006229612099938156]
[2024-04-17 12:59:24,027: INFO: main: Training : batch 933 Loss: 0.010370470439511337]
[2024-04-17 12:59:24,662: INFO: main: Training : batch 934 Loss: 0.009812258239605085]
[2024-04-17 12:59:25,306: INFO: main: Training : batch 935 Loss: 0.005535082250380101]
[2024-04-17 12:59:25,945: INFO: main: Training : batch 936 Loss: 0.006069239321844729]
[2024-04-17 12:59:26,579: INFO: main: Training : batch 937 Loss: 0.007387658391077489]
[2024-04-17 12:59:27,218: INFO: main: Training : batch 938 Loss: 0.010996176596933032]
[2024-04-17 12:59:27,851: INFO: main: Training : batch 939 Loss: 0.005129774348598676]
[2024-04-17 12:59:28,482: INFO: main: Training : batch 940 Loss: 0.0011961000082917018]
[2024-04-17 12:59:29,111: INFO: main: Training : batch 941 Loss: 0.004765351325417083]
[2024-04-17 12:59:29,746: INFO: main: Training : batch 942 Loss: 0.001752158443452827]
[2024-04-17 12:59:30,382: INFO: main: Training : batch 943 Loss: 0.005244136170019911]
[2024-04-17 12:59:31,014: INFO: main: Training : batch 944 Loss: 0.0008738457158450219]
[2024-04-17 12:59:31,642: INFO: main: Training : batch 945 Loss: 0.00468242986830246]
[2024-04-17 12:59:32,274: INFO: main: Training : batch 946 Loss: 0.0006816592525222308]
[2024-04-17 12:59:32,901: INFO: main: Training : batch 947 Loss: 0.002636457981272933]
[2024-04-17 12:59:33,530: INFO: main: Training : batch 948 Loss: 0.014798524419474417]
[2024-04-17 12:59:34,164: INFO: main: Training : batch 949 Loss: 0.004191227693017288]
[2024-04-17 12:59:34,795: INFO: main: Training : batch 950 Loss: 0.005697939781465125]
[2024-04-17 12:59:35,424: INFO: main: Training : batch 951 Loss: 0.0053887318000255015]
[2024-04-17 12:59:36,056: INFO: main: Training : batch 952 Loss: 0.016977274114550685]
[2024-04-17 12:59:36,679: INFO: main: Training : batch 953 Loss: 0.008963261668465386]
[2024-04-17 12:59:37,311: INFO: main: Training : batch 954 Loss: 0.005172138752671801]
[2024-04-17 12:59:37,945: INFO: main: Training : batch 955 Loss: 0.0013230309998845348]
[2024-04-17 12:59:38,580: INFO: main: Training : batch 956 Loss: 0.008381472283941017]
[2024-04-17 12:59:39,215: INFO: main: Training : batch 957 Loss: 0.00987610292032855]
[2024-04-17 12:59:39,848: INFO: main: Training : batch 958 Loss: 0.006027687196763542]
[2024-04-17 12:59:40,485: INFO: main: Training : batch 959 Loss: 0.011941871937935084]
[2024-04-17 12:59:41,123: INFO: main: Training : batch 960 Loss: 0.0014377425623928607]
[2024-04-17 12:59:41,756: INFO: main: Training : batch 961 Loss: 0.002730654821523309]
[2024-04-17 12:59:42,385: INFO: main: Training : batch 962 Loss: 0.01116616383207141]
[2024-04-17 12:59:43,016: INFO: main: Training : batch 963 Loss: 0.016303178653539717]
[2024-04-17 12:59:43,639: INFO: main: Training : batch 964 Loss: 0.003973607869202088]
[2024-04-17 12:59:44,273: INFO: main: Training : batch 965 Loss: 0.002088722421773031]
[2024-04-17 12:59:44,900: INFO: main: Training : batch 966 Loss: 0.0015813272331888745]
[2024-04-17 12:59:45,530: INFO: main: Training : batch 967 Loss: 0.01315685205246049]
[2024-04-17 12:59:46,160: INFO: main: Training : batch 968 Loss: 0.0011242211385655407]
[2024-04-17 12:59:46,793: INFO: main: Training : batch 969 Loss: 0.008624518091579715]
[2024-04-17 12:59:47,424: INFO: main: Training : batch 970 Loss: 0.0017710185820916495]
[2024-04-17 12:59:48,052: INFO: main: Training : batch 971 Loss: 0.0010103279011845663]
[2024-04-17 12:59:48,685: INFO: main: Training : batch 972 Loss: 0.011400471614202486]
[2024-04-17 12:59:49,313: INFO: main: Training : batch 973 Loss: 0.006294734565205032]
[2024-04-17 12:59:49,944: INFO: main: Training : batch 974 Loss: 0.00924739120436004]
[2024-04-17 12:59:50,565: INFO: main: Training : batch 975 Loss: 0.015015665961230428]
[2024-04-17 12:59:51,206: INFO: main: Training : batch 976 Loss: 0.007939880954260566]
[2024-04-17 12:59:51,843: INFO: main: Training : batch 977 Loss: 0.006838189144463089]
[2024-04-17 12:59:52,479: INFO: main: Training : batch 978 Loss: 0.006058362998404976]
[2024-04-17 12:59:53,113: INFO: main: Training : batch 979 Loss: 0.004237143859819911]
[2024-04-17 12:59:53,747: INFO: main: Training : batch 980 Loss: 0.004275934087473214]
[2024-04-17 12:59:54,385: INFO: main: Training : batch 981 Loss: 0.00376189101700711]
[2024-04-17 12:59:55,013: INFO: main: Training : batch 982 Loss: 0.00355482007623764]
[2024-04-17 12:59:55,642: INFO: main: Training : batch 983 Loss: 0.014234213011944108]
[2024-04-17 12:59:56,273: INFO: main: Training : batch 984 Loss: 0.007344479787290578]
[2024-04-17 12:59:56,903: INFO: main: Training : batch 985 Loss: 0.004004077206920828]
[2024-04-17 12:59:57,530: INFO: main: Training : batch 986 Loss: 0.01036086418307437]
[2024-04-17 12:59:58,161: INFO: main: Training : batch 987 Loss: 0.003405861627898026]
[2024-04-17 12:59:58,789: INFO: main: Training : batch 988 Loss: 0.016526482318816378]
[2024-04-17 12:59:59,420: INFO: main: Training : batch 989 Loss: 0.003944051013927479]
[2024-04-17 13:00:00,048: INFO: main: Training : batch 990 Loss: 0.00800889519863508]
[2024-04-17 13:00:00,675: INFO: main: Training : batch 991 Loss: 0.006904160358709331]
[2024-04-17 13:00:01,306: INFO: main: Training : batch 992 Loss: 0.0016283812966480724]
[2024-04-17 13:00:01,938: INFO: main: Training : batch 993 Loss: 0.005197142336057838]
[2024-04-17 13:00:02,566: INFO: main: Training : batch 994 Loss: 0.011802297855166058]
[2024-04-17 13:00:03,194: INFO: main: Training : batch 995 Loss: 0.0023827015796783007]
[2024-04-17 13:00:03,824: INFO: main: Training : batch 996 Loss: 0.003526865352864994]
[2024-04-17 13:00:04,460: INFO: main: Training : batch 997 Loss: 0.005049747105102377]
[2024-04-17 13:00:05,094: INFO: main: Training : batch 998 Loss: 0.004609529994642763]
[2024-04-17 13:00:05,734: INFO: main: Training : batch 999 Loss: 0.002911226732174977]
[2024-04-17 13:00:06,377: INFO: main: Training : batch 1000 Loss: 0.028267346554472755]
[2024-04-17 13:00:07,024: INFO: main: Training : batch 1001 Loss: 0.0009267890537117959]
[2024-04-17 13:00:07,665: INFO: main: Training : batch 1002 Loss: 0.003917775752501496]
[2024-04-17 13:00:08,301: INFO: main: Training : batch 1003 Loss: 0.002270246173861268]
[2024-04-17 13:00:08,931: INFO: main: Training : batch 1004 Loss: 0.005822751672267573]
[2024-04-17 13:00:09,559: INFO: main: Training : batch 1005 Loss: 0.007725285949533799]
[2024-04-17 13:00:10,188: INFO: main: Training : batch 1006 Loss: 0.010446848568227822]
[2024-04-17 13:00:10,824: INFO: main: Training : batch 1007 Loss: 0.004940711213580755]
[2024-04-17 13:00:11,455: INFO: main: Training : batch 1008 Loss: 0.008680159623759976]
[2024-04-17 13:00:12,088: INFO: main: Training : batch 1009 Loss: 0.00039007406644322887]
[2024-04-17 13:00:12,725: INFO: main: Training : batch 1010 Loss: 0.01111896549750457]
[2024-04-17 13:00:13,354: INFO: main: Training : batch 1011 Loss: 0.021258514267003755]
[2024-04-17 13:00:13,986: INFO: main: Training : batch 1012 Loss: 0.0034824765086796106]
[2024-04-17 13:00:14,618: INFO: main: Training : batch 1013 Loss: 0.002179824069672428]
[2024-04-17 13:00:15,248: INFO: main: Training : batch 1014 Loss: 0.011978682835820108]
[2024-04-17 13:00:15,879: INFO: main: Training : batch 1015 Loss: 0.008519083727334531]
[2024-04-17 13:00:16,516: INFO: main: Training : batch 1016 Loss: 0.014730949167829558]
[2024-04-17 13:00:17,148: INFO: main: Training : batch 1017 Loss: 0.007657340779227549]
[2024-04-17 13:00:17,782: INFO: main: Training : batch 1018 Loss: 0.0018854430467561006]
[2024-04-17 13:00:18,420: INFO: main: Training : batch 1019 Loss: 0.0007849909336081218]
[2024-04-17 13:00:19,061: INFO: main: Training : batch 1020 Loss: 0.0020205692185131315]
[2024-04-17 13:00:19,707: INFO: main: Training : batch 1021 Loss: 0.001298387553511146]
[2024-04-17 13:00:20,350: INFO: main: Training : batch 1022 Loss: 0.009327996694432234]
[2024-04-17 13:00:20,986: INFO: main: Training : batch 1023 Loss: 0.00536904930602887]
[2024-04-17 13:00:21,630: INFO: main: Training : batch 1024 Loss: 0.017263837331658708]
[2024-04-17 13:00:22,262: INFO: main: Training : batch 1025 Loss: 0.01054066399862081]
[2024-04-17 13:00:22,894: INFO: main: Training : batch 1026 Loss: 0.006598023128362878]
[2024-04-17 13:00:23,529: INFO: main: Training : batch 1027 Loss: 0.0026230414399936795]
[2024-04-17 13:00:24,160: INFO: main: Training : batch 1028 Loss: 0.0012752792997421645]
[2024-04-17 13:00:24,795: INFO: main: Training : batch 1029 Loss: 0.0016600336765093529]
[2024-04-17 13:00:25,428: INFO: main: Training : batch 1030 Loss: 0.008928096525253185]
[2024-04-17 13:00:26,062: INFO: main: Training : batch 1031 Loss: 0.031151780240492526]
[2024-04-17 13:00:26,697: INFO: main: Training : batch 1032 Loss: 0.000186426038012384]
[2024-04-17 13:00:27,336: INFO: main: Training : batch 1033 Loss: 0.002543392251338192]
[2024-04-17 13:00:27,973: INFO: main: Training : batch 1034 Loss: 0.0028689015694720486]
[2024-04-17 13:00:28,606: INFO: main: Training : batch 1035 Loss: 0.003982134744806981]
[2024-04-17 13:00:29,237: INFO: main: Training : batch 1036 Loss: 0.010331503589318922]
[2024-04-17 13:00:29,873: INFO: main: Training : batch 1037 Loss: 0.0022689088605039945]
[2024-04-17 13:00:30,506: INFO: main: Training : batch 1038 Loss: 0.004615033330054329]
[2024-04-17 13:00:31,141: INFO: main: Training : batch 1039 Loss: 0.0021502465230512345]
[2024-04-17 13:00:31,787: INFO: main: Training : batch 1040 Loss: 0.006193336545801178]
[2024-04-17 13:00:32,425: INFO: main: Training : batch 1041 Loss: 0.009046758832954565]
[2024-04-17 13:00:33,066: INFO: main: Training : batch 1042 Loss: 0.006783624641628675]
[2024-04-17 13:00:33,702: INFO: main: Training : batch 1043 Loss: 0.014704760119357264]
[2024-04-17 13:00:34,340: INFO: main: Training : batch 1044 Loss: 0.006080212992898668]
[2024-04-17 13:00:34,983: INFO: main: Training : batch 1045 Loss: 0.008198676288568612]
[2024-04-17 13:00:35,617: INFO: main: Training : batch 1046 Loss: 0.0005440837949680183]
[2024-04-17 13:00:36,251: INFO: main: Training : batch 1047 Loss: 0.012504346874311905]
[2024-04-17 13:00:36,886: INFO: main: Training : batch 1048 Loss: 0.00021213827506133637]
[2024-04-17 13:00:37,518: INFO: main: Training : batch 1049 Loss: 0.010278479235339524]
[2024-04-17 13:00:38,155: INFO: main: Training : batch 1050 Loss: 0.00029203908983453983]
[2024-04-17 13:00:38,788: INFO: main: Training : batch 1051 Loss: 0.006116384778485895]
[2024-04-17 13:00:39,415: INFO: main: Training : batch 1052 Loss: 0.014993799268491698]
[2024-04-17 13:00:40,047: INFO: main: Training : batch 1053 Loss: 0.011835830868213923]
[2024-04-17 13:00:40,678: INFO: main: Training : batch 1054 Loss: 0.012430387351290794]
[2024-04-17 13:00:41,309: INFO: main: Training : batch 1055 Loss: 0.0013814837468731128]
[2024-04-17 13:00:41,944: INFO: main: Training : batch 1056 Loss: 0.004861191814639674]
[2024-04-17 13:00:42,578: INFO: main: Training : batch 1057 Loss: 0.01725316008644852]
[2024-04-17 13:00:43,210: INFO: main: Training : batch 1058 Loss: 0.00018022192790824253]
[2024-04-17 13:00:43,843: INFO: main: Training : batch 1059 Loss: 0.006578885010731733]
[2024-04-17 13:00:44,477: INFO: main: Training : batch 1060 Loss: 0.02059108741969896]
[2024-04-17 13:00:45,105: INFO: main: Training : batch 1061 Loss: 0.009605290373572235]
[2024-04-17 13:00:45,746: INFO: main: Training : batch 1062 Loss: 0.01169052102695448]
[2024-04-17 13:00:46,390: INFO: main: Training : batch 1063 Loss: 0.0010926345170323008]
[2024-04-17 13:00:47,034: INFO: main: Training : batch 1064 Loss: 0.0022311211826477476]
[2024-04-17 13:00:47,675: INFO: main: Training : batch 1065 Loss: 0.0029662703429184315]
[2024-04-17 13:00:48,315: INFO: main: Training : batch 1066 Loss: 0.00201844397412241]
[2024-04-17 13:00:48,948: INFO: main: Training : batch 1067 Loss: 0.007011215582277911]
[2024-04-17 13:00:49,580: INFO: main: Training : batch 1068 Loss: 0.003764215070468214]
[2024-04-17 13:00:50,213: INFO: main: Training : batch 1069 Loss: 0.024038536054857187]
[2024-04-17 13:00:50,842: INFO: main: Training : batch 1070 Loss: 0.01503351590263947]
[2024-04-17 13:00:51,474: INFO: main: Training : batch 1071 Loss: 0.005582846453218028]
[2024-04-17 13:00:52,107: INFO: main: Training : batch 1072 Loss: 0.018081863384215042]
[2024-04-17 13:00:52,736: INFO: main: Training : batch 1073 Loss: 0.009057398080520923]
[2024-04-17 13:00:53,364: INFO: main: Training : batch 1074 Loss: 0.03075264287284236]
[2024-04-17 13:00:53,993: INFO: main: Training : batch 1075 Loss: 0.008986766212968641]
[2024-04-17 13:00:54,626: INFO: main: Training : batch 1076 Loss: 0.017801201412138375]
[2024-04-17 13:00:55,260: INFO: main: Training : batch 1077 Loss: 0.012304133673872005]
[2024-04-17 13:00:55,893: INFO: main: Training : batch 1078 Loss: 0.004605365030660633]
[2024-04-17 13:00:56,524: INFO: main: Training : batch 1079 Loss: 0.01593875180761416]
[2024-04-17 13:00:57,152: INFO: main: Training : batch 1080 Loss: 0.005008823503602544]
[2024-04-17 13:00:57,785: INFO: main: Training : batch 1081 Loss: 0.01389554717563849]
[2024-04-17 13:00:58,420: INFO: main: Training : batch 1082 Loss: 0.0045729791750420785]
[2024-04-17 13:00:59,061: INFO: main: Training : batch 1083 Loss: 0.005529737373616905]
[2024-04-17 13:00:59,703: INFO: main: Training : batch 1084 Loss: 0.01746953838594412]
[2024-04-17 13:01:00,340: INFO: main: Training : batch 1085 Loss: 0.008706096573717481]
[2024-04-17 13:01:00,978: INFO: main: Training : batch 1086 Loss: 0.0008407398316358895]
[2024-04-17 13:01:01,621: INFO: main: Training : batch 1087 Loss: 0.004309077916036915]
[2024-04-17 13:01:02,255: INFO: main: Training : batch 1088 Loss: 0.003056956635387991]
[2024-04-17 13:01:02,884: INFO: main: Training : batch 1089 Loss: 0.009820188195050742]
[2024-04-17 13:01:03,518: INFO: main: Training : batch 1090 Loss: 0.010711428897283394]
[2024-04-17 13:01:04,150: INFO: main: Training : batch 1091 Loss: 0.002324390561579648]
[2024-04-17 13:01:04,777: INFO: main: Training : batch 1092 Loss: 0.011776411967213775]
[2024-04-17 13:01:05,412: INFO: main: Training : batch 1093 Loss: 0.0030640214959340257]
[2024-04-17 13:01:06,040: INFO: main: Training : batch 1094 Loss: 0.009018962561270882]
[2024-04-17 13:01:06,674: INFO: main: Training : batch 1095 Loss: 0.01472564545302033]
[2024-04-17 13:01:07,307: INFO: main: Training : batch 1096 Loss: 0.00726429847427226]
[2024-04-17 13:01:07,940: INFO: main: Training : batch 1097 Loss: 0.023954974742111515]
[2024-04-17 13:01:08,573: INFO: main: Training : batch 1098 Loss: 0.003599225862387466]
[2024-04-17 13:01:09,205: INFO: main: Training : batch 1099 Loss: 0.0015319115617815147]
[2024-04-17 13:01:09,837: INFO: main: Training : batch 1100 Loss: 0.001381040662970338]
[2024-04-17 13:01:10,467: INFO: main: Training : batch 1101 Loss: 0.017598783898498213]
[2024-04-17 13:01:11,099: INFO: main: Training : batch 1102 Loss: 0.0011149297485459858]
[2024-04-17 13:01:11,730: INFO: main: Training : batch 1103 Loss: 0.004037053910647981]
[2024-04-17 13:01:12,370: INFO: main: Training : batch 1104 Loss: 0.012045121360802843]
[2024-04-17 13:01:13,008: INFO: main: Training : batch 1105 Loss: 0.011752897806447131]
[2024-04-17 13:01:13,646: INFO: main: Training : batch 1106 Loss: 0.006523104587061326]
[2024-04-17 13:01:14,284: INFO: main: Training : batch 1107 Loss: 0.0059299956211533]
[2024-04-17 13:01:14,923: INFO: main: Training : batch 1108 Loss: 0.00848730238478143]
[2024-04-17 13:01:15,569: INFO: main: Training : batch 1109 Loss: 0.03930207517851915]
[2024-04-17 13:01:16,204: INFO: main: Training : batch 1110 Loss: 0.002259397910159515]
[2024-04-17 13:01:16,832: INFO: main: Training : batch 1111 Loss: 0.004372612766577282]
[2024-04-17 13:01:17,465: INFO: main: Training : batch 1112 Loss: 0.012831495696161155]
[2024-04-17 13:01:18,097: INFO: main: Training : batch 1113 Loss: 0.008103401906192046]
[2024-04-17 13:01:18,728: INFO: main: Training : batch 1114 Loss: 0.012365147942031666]
[2024-04-17 13:01:19,362: INFO: main: Training : batch 1115 Loss: 0.012461844267764558]
[2024-04-17 13:01:20,004: INFO: main: Training : batch 1116 Loss: 0.010183002860196516]
[2024-04-17 13:01:20,633: INFO: main: Training : batch 1117 Loss: 0.006075582863859328]
[2024-04-17 13:01:21,259: INFO: main: Training : batch 1118 Loss: 0.0061677641800743635]
[2024-04-17 13:01:21,891: INFO: main: Training : batch 1119 Loss: 0.010246992896121618]
[2024-04-17 13:01:22,520: INFO: main: Training : batch 1120 Loss: 0.005759881334220849]
[2024-04-17 13:01:23,150: INFO: main: Training : batch 1121 Loss: 0.004133130167357612]
[2024-04-17 13:01:23,786: INFO: main: Training : batch 1122 Loss: 0.0026345012141103146]
[2024-04-17 13:01:24,418: INFO: main: Training : batch 1123 Loss: 0.000516708792134514]
[2024-04-17 13:01:25,050: INFO: main: Training : batch 1124 Loss: 0.0194716193730092]
[2024-04-17 13:01:25,679: INFO: main: Training : batch 1125 Loss: 0.0023164914403218544]
[2024-04-17 13:01:26,316: INFO: main: Training : batch 1126 Loss: 0.007758815448716419]
[2024-04-17 13:01:26,955: INFO: main: Training : batch 1127 Loss: 0.0029641208516839115]
[2024-04-17 13:01:27,595: INFO: main: Training : batch 1128 Loss: 0.0015223033284284552]
[2024-04-17 13:01:28,238: INFO: main: Training : batch 1129 Loss: 0.007718268006905574]
[2024-04-17 13:01:28,876: INFO: main: Training : batch 1130 Loss: 0.0076674012863650635]
[2024-04-17 13:01:29,512: INFO: main: Training : batch 1131 Loss: 0.003221258436651558]
[2024-04-17 13:01:30,147: INFO: main: Training : batch 1132 Loss: 0.00972983644235609]
[2024-04-17 13:01:30,778: INFO: main: Training : batch 1133 Loss: 0.009094085910176778]
[2024-04-17 13:01:31,407: INFO: main: Training : batch 1134 Loss: 0.003062818310655667]
[2024-04-17 13:01:32,035: INFO: main: Training : batch 1135 Loss: 0.003466007294509148]
[2024-04-17 13:01:32,666: INFO: main: Training : batch 1136 Loss: 0.0038367443446896796]
[2024-04-17 13:01:33,294: INFO: main: Training : batch 1137 Loss: 0.006336279570445492]
[2024-04-17 13:01:33,929: INFO: main: Training : batch 1138 Loss: 0.007355637901762273]
[2024-04-17 13:01:34,560: INFO: main: Training : batch 1139 Loss: 0.008958940414301281]
[2024-04-17 13:01:35,191: INFO: main: Training : batch 1140 Loss: 0.017024531497372458]
[2024-04-17 13:01:35,822: INFO: main: Training : batch 1141 Loss: 0.011869713679004176]
[2024-04-17 13:01:36,455: INFO: main: Training : batch 1142 Loss: 0.0034643757375203703]
[2024-04-17 13:01:37,086: INFO: main: Training : batch 1143 Loss: 0.0009663747689160602]
[2024-04-17 13:01:37,718: INFO: main: Training : batch 1144 Loss: 0.004251175936374314]
[2024-04-17 13:01:38,347: INFO: main: Training : batch 1145 Loss: 0.008080234856114499]
[2024-04-17 13:01:38,982: INFO: main: Training : batch 1146 Loss: 0.013069586314085982]
[2024-04-17 13:01:39,621: INFO: main: Training : batch 1147 Loss: 0.025425412565167414]
[2024-04-17 13:01:40,262: INFO: main: Training : batch 1148 Loss: 0.0027820399967317757]
[2024-04-17 13:01:40,898: INFO: main: Training : batch 1149 Loss: 0.006140790721175769]
[2024-04-17 13:01:41,533: INFO: main: Training : batch 1150 Loss: 0.004933779195562104]
[2024-04-17 13:01:42,177: INFO: main: Training : batch 1151 Loss: 0.0005715004309425548]
[2024-04-17 13:01:42,811: INFO: main: Training : batch 1152 Loss: 0.03415884458226319]
[2024-04-17 13:01:43,440: INFO: main: Training : batch 1153 Loss: 0.004174461322106554]
[2024-04-17 13:01:44,068: INFO: main: Training : batch 1154 Loss: 0.013981706436555993]
[2024-04-17 13:01:44,699: INFO: main: Training : batch 1155 Loss: 0.001119038231720309]
[2024-04-17 13:01:45,324: INFO: main: Training : batch 1156 Loss: 0.003530987598848931]
[2024-04-17 13:01:45,958: INFO: main: Training : batch 1157 Loss: 0.007213172815868873]
[2024-04-17 13:01:46,588: INFO: main: Training : batch 1158 Loss: 0.008910709517967538]
[2024-04-17 13:01:47,218: INFO: main: Training : batch 1159 Loss: 0.007778093197765175]
[2024-04-17 13:01:47,851: INFO: main: Training : batch 1160 Loss: 0.006498873963564388]
[2024-04-17 13:01:48,482: INFO: main: Training : batch 1161 Loss: 0.010982830817835144]
[2024-04-17 13:01:49,115: INFO: main: Training : batch 1162 Loss: 0.007615942356627088]
[2024-04-17 13:01:49,747: INFO: main: Training : batch 1163 Loss: 0.008028529041617966]
[2024-04-17 13:01:50,384: INFO: main: Training : batch 1164 Loss: 0.008031805046053705]
[2024-04-17 13:01:51,014: INFO: main: Training : batch 1165 Loss: 0.0027418496084378923]
[2024-04-17 13:01:51,650: INFO: main: Training : batch 1166 Loss: 0.006855748267307564]
[2024-04-17 13:01:52,282: INFO: main: Training : batch 1167 Loss: 0.0030247771675915077]
[2024-04-17 13:01:52,920: INFO: main: Training : batch 1168 Loss: 0.0005426207489115099]
[2024-04-17 13:01:53,561: INFO: main: Training : batch 1169 Loss: 0.002025851541356381]
[2024-04-17 13:01:54,194: INFO: main: Training : batch 1170 Loss: 0.011646758185563773]
[2024-04-17 13:01:54,834: INFO: main: Training : batch 1171 Loss: 0.006519735598808975]
[2024-04-17 13:01:55,472: INFO: main: Training : batch 1172 Loss: 0.005879325126807762]
[2024-04-17 13:01:56,108: INFO: main: Training : batch 1173 Loss: 0.008275231409925875]
[2024-04-17 13:01:56,741: INFO: main: Training : batch 1174 Loss: 0.01668545097569442]
[2024-04-17 13:01:57,377: INFO: main: Training : batch 1175 Loss: 0.0057524419598026745]
[2024-04-17 13:01:58,007: INFO: main: Training : batch 1176 Loss: 0.014156952124777647]
[2024-04-17 13:01:58,639: INFO: main: Training : batch 1177 Loss: 0.009343535467292366]
[2024-04-17 13:01:59,273: INFO: main: Training : batch 1178 Loss: 0.010637290895384455]
[2024-04-17 13:01:59,909: INFO: main: Training : batch 1179 Loss: 0.012044598769135611]
[2024-04-17 13:02:00,543: INFO: main: Training : batch 1180 Loss: 0.0036957719916763223]
[2024-04-17 13:02:01,174: INFO: main: Training : batch 1181 Loss: 0.0057737319078686525]
[2024-04-17 13:02:01,805: INFO: main: Training : batch 1182 Loss: 0.00014162354335818506]
[2024-04-17 13:02:02,436: INFO: main: Training : batch 1183 Loss: 0.002897290554620987]
[2024-04-17 13:02:03,061: INFO: main: Training : batch 1184 Loss: 0.003619483334924691]
[2024-04-17 13:02:03,686: INFO: main: Training : batch 1185 Loss: 0.0065585107502942695]
[2024-04-17 13:02:04,318: INFO: main: Training : batch 1186 Loss: 0.01608131760173471]
[2024-04-17 13:02:04,957: INFO: main: Training : batch 1187 Loss: 0.007781634750878839]
[2024-04-17 13:02:05,586: INFO: main: Training : batch 1188 Loss: 0.010546620048851985]
[2024-04-17 13:02:06,224: INFO: main: Training : batch 1189 Loss: 0.004206430307349716]
[2024-04-17 13:02:06,861: INFO: main: Training : batch 1190 Loss: 0.023934103395608704]
[2024-04-17 13:02:07,499: INFO: main: Training : batch 1191 Loss: 0.0019548977041978417]
[2024-04-17 13:02:08,137: INFO: main: Training : batch 1192 Loss: 0.0029696852202431606]
[2024-04-17 13:02:08,778: INFO: main: Training : batch 1193 Loss: 0.007322686747374145]
[2024-04-17 13:02:09,417: INFO: main: Training : batch 1194 Loss: 0.004774783387670727]
[2024-04-17 13:02:10,049: INFO: main: Training : batch 1195 Loss: 0.014283174880347917]
[2024-04-17 13:02:10,678: INFO: main: Training : batch 1196 Loss: 0.004544238674245074]
[2024-04-17 13:02:11,307: INFO: main: Training : batch 1197 Loss: 0.007812492830738197]
[2024-04-17 13:02:11,938: INFO: main: Training : batch 1198 Loss: 0.005690619051427757]
[2024-04-17 13:02:12,568: INFO: main: Training : batch 1199 Loss: 0.005966614514973237]
[2024-04-17 13:02:13,196: INFO: main: Training : batch 1200 Loss: 0.0013312763102268181]
[2024-04-17 13:02:13,822: INFO: main: Training : batch 1201 Loss: 0.011303722034629987]
[2024-04-17 13:02:14,449: INFO: main: Training : batch 1202 Loss: 0.005090855252755209]
[2024-04-17 13:02:15,083: INFO: main: Training : batch 1203 Loss: 0.0019180964667891347]
[2024-04-17 13:02:15,713: INFO: main: Training : batch 1204 Loss: 0.004068248252900434]
[2024-04-17 13:02:16,336: INFO: main: Training : batch 1205 Loss: 0.007188203581064251]
[2024-04-17 13:02:16,970: INFO: main: Training : batch 1206 Loss: 0.01141913129874567]
[2024-04-17 13:02:17,604: INFO: main: Training : batch 1207 Loss: 0.013370926714692236]
[2024-04-17 13:02:18,231: INFO: main: Training : batch 1208 Loss: 0.003218163934719603]
[2024-04-17 13:02:18,858: INFO: main: Training : batch 1209 Loss: 0.012034947962447919]
[2024-04-17 13:02:19,487: INFO: main: Training : batch 1210 Loss: 0.010127274617462908]
[2024-04-17 13:02:20,118: INFO: main: Training : batch 1211 Loss: 0.021004705394473622]
[2024-04-17 13:02:20,762: INFO: main: Training : batch 1212 Loss: 0.0069245602006977325]
[2024-04-17 13:02:21,397: INFO: main: Training : batch 1213 Loss: 0.013616676130075193]
[2024-04-17 13:02:22,044: INFO: main: Training : batch 1214 Loss: 0.001503196173340049]
[2024-04-17 13:02:22,682: INFO: main: Training : batch 1215 Loss: 0.009905164052136718]
[2024-04-17 13:02:23,311: INFO: main: Training : batch 1216 Loss: 0.004213651645095186]
[2024-04-17 13:02:23,941: INFO: main: Training : batch 1217 Loss: 0.002294555932757792]
[2024-04-17 13:02:24,572: INFO: main: Training : batch 1218 Loss: 0.00032047357746251064]
[2024-04-17 13:02:25,203: INFO: main: Training : batch 1219 Loss: 0.018246167704917247]
[2024-04-17 13:02:25,833: INFO: main: Training : batch 1220 Loss: 0.0033157144810592905]
[2024-04-17 13:02:26,468: INFO: main: Training : batch 1221 Loss: 0.001966657877383407]
[2024-04-17 13:02:27,100: INFO: main: Training : batch 1222 Loss: 0.023965662642181303]
[2024-04-17 13:02:27,733: INFO: main: Training : batch 1223 Loss: 0.0016243281255773493]
[2024-04-17 13:02:28,362: INFO: main: Training : batch 1224 Loss: 0.005069735091634958]
[2024-04-17 13:02:28,992: INFO: main: Training : batch 1225 Loss: 0.0014605640955009489]
[2024-04-17 13:02:29,625: INFO: main: Training : batch 1226 Loss: 0.0025117784877827804]
[2024-04-17 13:02:30,260: INFO: main: Training : batch 1227 Loss: 0.0014511402604414467]
[2024-04-17 13:02:30,890: INFO: main: Training : batch 1228 Loss: 0.016709460202992656]
[2024-04-17 13:02:31,517: INFO: main: Training : batch 1229 Loss: 0.005241209458792346]
[2024-04-17 13:02:32,146: INFO: main: Training : batch 1230 Loss: 0.0105323072982756]
[2024-04-17 13:02:32,779: INFO: main: Training : batch 1231 Loss: 0.0006852944921577538]
[2024-04-17 13:02:33,414: INFO: main: Training : batch 1232 Loss: 0.007810293912375582]
[2024-04-17 13:02:34,060: INFO: main: Training : batch 1233 Loss: 0.0035952394365272827]
[2024-04-17 13:02:34,702: INFO: main: Training : batch 1234 Loss: 0.004456366026925898]
[2024-04-17 13:02:35,338: INFO: main: Training : batch 1235 Loss: 0.005028228820713767]
[2024-04-17 13:02:35,973: INFO: main: Training : batch 1236 Loss: 0.0071480824559749705]
[2024-04-17 13:02:36,609: INFO: main: Training : batch 1237 Loss: 0.007689487716289162]
[2024-04-17 13:02:37,238: INFO: main: Training : batch 1238 Loss: 0.010422118619208536]
[2024-04-17 13:02:37,868: INFO: main: Training : batch 1239 Loss: 0.005178237049515866]
[2024-04-17 13:02:38,498: INFO: main: Training : batch 1240 Loss: 0.009680861459408512]
[2024-04-17 13:02:39,133: INFO: main: Training : batch 1241 Loss: 0.0069719781088223125]
[2024-04-17 13:02:39,762: INFO: main: Training : batch 1242 Loss: 0.009140213351049762]
[2024-04-17 13:02:40,396: INFO: main: Training : batch 1243 Loss: 0.007724263343564344]
[2024-04-17 13:02:41,030: INFO: main: Training : batch 1244 Loss: 0.009821690827180905]
[2024-04-17 13:02:41,659: INFO: main: Training : batch 1245 Loss: 0.010922033933327493]
[2024-04-17 13:02:42,294: INFO: main: Training : batch 1246 Loss: 0.015179989606649997]
[2024-04-17 13:02:42,931: INFO: main: Training : batch 1247 Loss: 0.0014901295669468764]
[2024-04-17 13:02:43,557: INFO: main: Training : batch 1248 Loss: 0.001723211816602622]
[2024-04-17 13:02:44,193: INFO: main: Training : batch 1249 Loss: 0.0019160985616603436]
[2024-04-17 13:02:44,823: INFO: main: Training : batch 1250 Loss: 0.018697283380492837]
[2024-04-17 13:02:45,457: INFO: main: Training : batch 1251 Loss: 0.0025895895323646196]
[2024-04-17 13:02:46,090: INFO: main: Training : batch 1252 Loss: 0.0028988911271015604]
[2024-04-17 13:02:46,724: INFO: main: Training : batch 1253 Loss: 0.004014449760762752]
[2024-04-17 13:02:47,373: INFO: main: Training : batch 1254 Loss: 0.0036716241388509104]
[2024-04-17 13:02:48,018: INFO: main: Training : batch 1255 Loss: 0.009940734510751465]
[2024-04-17 13:02:48,656: INFO: main: Training : batch 1256 Loss: 0.0015145222468052884]
[2024-04-17 13:02:49,297: INFO: main: Training : batch 1257 Loss: 0.0072136091359042325]
[2024-04-17 13:02:49,937: INFO: main: Training : batch 1258 Loss: 0.012196602605212652]
[2024-04-17 13:02:50,569: INFO: main: Training : batch 1259 Loss: 0.0009683456298653157]
[2024-04-17 13:02:51,201: INFO: main: Training : batch 1260 Loss: 0.008202705648152097]
[2024-04-17 13:02:51,834: INFO: main: Training : batch 1261 Loss: 0.005776784055251221]
[2024-04-17 13:02:52,470: INFO: main: Training : batch 1262 Loss: 0.011851756996827348]
[2024-04-17 13:02:53,110: INFO: main: Training : batch 1263 Loss: 0.0017054368929971758]
[2024-04-17 13:02:53,744: INFO: main: Training : batch 1264 Loss: 0.035538005497933414]
[2024-04-17 13:02:54,378: INFO: main: Training : batch 1265 Loss: 0.0035582077287813585]
[2024-04-17 13:02:55,012: INFO: main: Training : batch 1266 Loss: 0.004647582647990788]
[2024-04-17 13:02:55,640: INFO: main: Training : batch 1267 Loss: 0.0023001043838652307]
[2024-04-17 13:02:56,274: INFO: main: Training : batch 1268 Loss: 0.012927513101865594]
[2024-04-17 13:02:56,902: INFO: main: Training : batch 1269 Loss: 0.009323056370507565]
[2024-04-17 13:02:57,538: INFO: main: Training : batch 1270 Loss: 0.001168253748501574]
[2024-04-17 13:02:58,171: INFO: main: Training : batch 1271 Loss: 0.002961194102753589]
[2024-04-17 13:02:58,803: INFO: main: Training : batch 1272 Loss: 0.021476397120045215]
[2024-04-17 13:02:59,435: INFO: main: Training : batch 1273 Loss: 0.0011376079233892113]
[2024-04-17 13:03:00,066: INFO: main: Training : batch 1274 Loss: 0.00787575805917804]
[2024-04-17 13:03:00,704: INFO: main: Training : batch 1275 Loss: 0.003139984503040457]
[2024-04-17 13:03:01,344: INFO: main: Training : batch 1276 Loss: 0.006346782435048393]
[2024-04-17 13:03:01,981: INFO: main: Training : batch 1277 Loss: 0.004028504583163512]
[2024-04-17 13:03:02,618: INFO: main: Training : batch 1278 Loss: 0.014299510922570143]
[2024-04-17 13:03:03,259: INFO: main: Training : batch 1279 Loss: 0.025267698438155503]
[2024-04-17 13:03:03,892: INFO: main: Training : batch 1280 Loss: 0.0016565272313783489]
[2024-04-17 13:03:04,522: INFO: main: Training : batch 1281 Loss: 0.002820893004057482]
[2024-04-17 13:03:05,153: INFO: main: Training : batch 1282 Loss: 0.0024702144859379436]
[2024-04-17 13:03:05,784: INFO: main: Training : batch 1283 Loss: 0.026874354897849544]
[2024-04-17 13:03:06,415: INFO: main: Training : batch 1284 Loss: 0.005828333112620506]
[2024-04-17 13:03:07,043: INFO: main: Training : batch 1285 Loss: 0.0027179685479086463]
[2024-04-17 13:03:07,675: INFO: main: Training : batch 1286 Loss: 0.003084853878820583]
[2024-04-17 13:03:08,309: INFO: main: Training : batch 1287 Loss: 0.007868637637888278]
[2024-04-17 13:03:08,940: INFO: main: Training : batch 1288 Loss: 0.006393205392525483]
[2024-04-17 13:03:09,571: INFO: main: Training : batch 1289 Loss: 0.016854673391277875]
[2024-04-17 13:03:10,206: INFO: main: Training : batch 1290 Loss: 0.0065501497892671255]
[2024-04-17 13:03:10,838: INFO: main: Training : batch 1291 Loss: 0.004567552038716313]
[2024-04-17 13:03:11,467: INFO: main: Training : batch 1292 Loss: 0.010503339866563342]
[2024-04-17 13:03:12,101: INFO: main: Training : batch 1293 Loss: 0.010929039659974405]
[2024-04-17 13:03:12,733: INFO: main: Training : batch 1294 Loss: 0.021092994848251306]
[2024-04-17 13:03:13,362: INFO: main: Training : batch 1295 Loss: 0.018735090672056563]
[2024-04-17 13:03:13,995: INFO: main: Training : batch 1296 Loss: 0.013307595323606291]
[2024-04-17 13:03:14,630: INFO: main: Training : batch 1297 Loss: 0.006331834618519012]
[2024-04-17 13:03:15,267: INFO: main: Training : batch 1298 Loss: 0.013526883053238226]
[2024-04-17 13:03:15,907: INFO: main: Training : batch 1299 Loss: 0.010227255352757593]
[2024-04-17 13:03:16,546: INFO: main: Training : batch 1300 Loss: 0.0023938996373528533]
[2024-04-17 13:03:17,180: INFO: main: Training : batch 1301 Loss: 0.02272435253287627]
[2024-04-17 13:03:17,812: INFO: main: Training : batch 1302 Loss: 0.009450442082818632]
[2024-04-17 13:03:18,440: INFO: main: Training : batch 1303 Loss: 0.01699883230006889]
[2024-04-17 13:03:19,077: INFO: main: Training : batch 1304 Loss: 0.00766583847225711]
[2024-04-17 13:03:19,707: INFO: main: Training : batch 1305 Loss: 0.005424321196467653]
[2024-04-17 13:03:20,338: INFO: main: Training : batch 1306 Loss: 0.0027008470228506247]
[2024-04-17 13:03:20,965: INFO: main: Training : batch 1307 Loss: 0.0013265094920090169]
[2024-04-17 13:03:21,600: INFO: main: Training : batch 1308 Loss: 0.008165277627789425]
[2024-04-17 13:03:22,233: INFO: main: Training : batch 1309 Loss: 0.005210973341635877]
[2024-04-17 13:03:22,868: INFO: main: Training : batch 1310 Loss: 0.004370253903505305]
[2024-04-17 13:03:23,501: INFO: main: Training : batch 1311 Loss: 0.01720201116029714]
[2024-04-17 13:03:24,133: INFO: main: Training : batch 1312 Loss: 0.007787798191866667]
[2024-04-17 13:03:24,763: INFO: main: Training : batch 1313 Loss: 0.008377849539938977]
[2024-04-17 13:03:25,398: INFO: main: Training : batch 1314 Loss: 0.007364100735248776]
[2024-04-17 13:03:26,028: INFO: main: Training : batch 1315 Loss: 0.01365617164645292]
[2024-04-17 13:03:26,658: INFO: main: Training : batch 1316 Loss: 0.010090124703210898]
[2024-04-17 13:03:27,306: INFO: main: Training : batch 1317 Loss: 0.004598127915677281]
[2024-04-17 13:03:27,942: INFO: main: Training : batch 1318 Loss: 0.010478681129624487]
[2024-04-17 13:03:28,573: INFO: main: Training : batch 1319 Loss: 0.011063903786737134]
[2024-04-17 13:03:29,210: INFO: main: Training : batch 1320 Loss: 0.0032171101099773493]
[2024-04-17 13:03:29,847: INFO: main: Training : batch 1321 Loss: 0.005491001056222675]
[2024-04-17 13:03:30,479: INFO: main: Training : batch 1322 Loss: 0.0038703516100266353]
[2024-04-17 13:03:31,110: INFO: main: Training : batch 1323 Loss: 0.0014730345228576318]
[2024-04-17 13:03:31,743: INFO: main: Training : batch 1324 Loss: 0.003180619822372959]
[2024-04-17 13:03:32,376: INFO: main: Training : batch 1325 Loss: 0.005540247784837876]
[2024-04-17 13:03:33,005: INFO: main: Training : batch 1326 Loss: 0.0011378030473385497]
[2024-04-17 13:03:33,637: INFO: main: Training : batch 1327 Loss: 0.012695215518678561]
[2024-04-17 13:03:34,270: INFO: main: Training : batch 1328 Loss: 0.019896046861962117]
[2024-04-17 13:03:34,900: INFO: main: Training : batch 1329 Loss: 0.024325018828204117]
[2024-04-17 13:03:35,532: INFO: main: Training : batch 1330 Loss: 0.00391313829056626]
[2024-04-17 13:03:36,166: INFO: main: Training : batch 1331 Loss: 0.016085613320874253]
[2024-04-17 13:03:36,801: INFO: main: Training : batch 1332 Loss: 0.015010864606121605]
[2024-04-17 13:03:37,435: INFO: main: Training : batch 1333 Loss: 0.0168309222478772]
[2024-04-17 13:03:38,069: INFO: main: Training : batch 1334 Loss: 0.002518782715501505]
[2024-04-17 13:03:38,703: INFO: main: Training : batch 1335 Loss: 0.007263968315187452]
[2024-04-17 13:03:39,334: INFO: main: Training : batch 1336 Loss: 0.00350541367425103]
[2024-04-17 13:03:39,965: INFO: main: Training : batch 1337 Loss: 0.010129884652537101]
[2024-04-17 13:03:40,600: INFO: main: Training : batch 1338 Loss: 0.007249536901729958]
[2024-04-17 13:03:41,235: INFO: main: Training : batch 1339 Loss: 0.018296034844832616]
[2024-04-17 13:03:41,870: INFO: main: Training : batch 1340 Loss: 0.019913973304015835]
[2024-04-17 13:03:42,515: INFO: main: Training : batch 1341 Loss: 0.0017974899265596985]
[2024-04-17 13:03:43,153: INFO: main: Training : batch 1342 Loss: 0.003886995879320256]
[2024-04-17 13:03:43,791: INFO: main: Training : batch 1343 Loss: 0.011021966357845558]
[2024-04-17 13:03:44,426: INFO: main: Training : batch 1344 Loss: 0.01544182316591333]
[2024-04-17 13:03:45,059: INFO: main: Training : batch 1345 Loss: 0.014422568772852385]
[2024-04-17 13:03:45,699: INFO: main: Training : batch 1346 Loss: 0.0028284482922169827]
[2024-04-17 13:03:46,333: INFO: main: Training : batch 1347 Loss: 0.008106894886494185]
[2024-04-17 13:03:46,970: INFO: main: Training : batch 1348 Loss: 0.003527477332985401]
[2024-04-17 13:03:47,603: INFO: main: Training : batch 1349 Loss: 0.010016750635507285]
[2024-04-17 13:03:48,238: INFO: main: Training : batch 1350 Loss: 0.02151808940706678]
[2024-04-17 13:03:48,866: INFO: main: Training : batch 1351 Loss: 0.012236779668556425]
[2024-04-17 13:03:49,500: INFO: main: Training : batch 1352 Loss: 0.00886506169072304]
[2024-04-17 13:03:50,132: INFO: main: Training : batch 1353 Loss: 0.006337137865646171]
[2024-04-17 13:03:50,764: INFO: main: Training : batch 1354 Loss: 0.0019989823258071025]
[2024-04-17 13:03:51,399: INFO: main: Training : batch 1355 Loss: 0.0016161445327304328]
[2024-04-17 13:03:52,032: INFO: main: Training : batch 1356 Loss: 0.00483381783575272]
[2024-04-17 13:03:52,663: INFO: main: Training : batch 1357 Loss: 0.0018591260957685216]
[2024-04-17 13:03:53,300: INFO: main: Training : batch 1358 Loss: 0.015487419603831936]
[2024-04-17 13:03:53,939: INFO: main: Training : batch 1359 Loss: 0.003751480931798547]
[2024-04-17 13:03:54,582: INFO: main: Training : batch 1360 Loss: 0.0039418425606708675]
[2024-04-17 13:03:55,220: INFO: main: Training : batch 1361 Loss: 0.0010839387786803435]
[2024-04-17 13:03:55,860: INFO: main: Training : batch 1362 Loss: 0.019900855713857635]
[2024-04-17 13:03:56,498: INFO: main: Training : batch 1363 Loss: 0.0019990941923107453]
[2024-04-17 13:03:57,148: INFO: main: Training : batch 1364 Loss: 0.02783386563782433]
[2024-04-17 13:03:57,780: INFO: main: Training : batch 1365 Loss: 0.0031088545730724605]
[2024-04-17 13:03:58,414: INFO: main: Training : batch 1366 Loss: 0.014568478571962631]
[2024-04-17 13:03:59,046: INFO: main: Training : batch 1367 Loss: 0.019053482738479482]
[2024-04-17 13:03:59,678: INFO: main: Training : batch 1368 Loss: 0.009939347361555823]
[2024-04-17 13:04:00,309: INFO: main: Training : batch 1369 Loss: 0.02206102978265056]
[2024-04-17 13:04:00,940: INFO: main: Training : batch 1370 Loss: 0.003071913593430654]
[2024-04-17 13:04:01,572: INFO: main: Training : batch 1371 Loss: 0.006366311371018284]
[2024-04-17 13:04:02,203: INFO: main: Training : batch 1372 Loss: 0.0017340209069590098]
[2024-04-17 13:04:02,836: INFO: main: Training : batch 1373 Loss: 0.003803476824626185]
[2024-04-17 13:04:03,471: INFO: main: Training : batch 1374 Loss: 0.037942246821189324]
[2024-04-17 13:04:04,107: INFO: main: Training : batch 1375 Loss: 0.00375284585967826]
[2024-04-17 13:04:04,738: INFO: main: Training : batch 1376 Loss: 0.012157183703797224]
[2024-04-17 13:04:05,371: INFO: main: Training : batch 1377 Loss: 0.0015953766955114374]
[2024-04-17 13:04:06,006: INFO: main: Training : batch 1378 Loss: 0.0034979670215391817]
[2024-04-17 13:04:06,636: INFO: main: Training : batch 1379 Loss: 0.01044657975156046]
[2024-04-17 13:04:07,267: INFO: main: Training : batch 1380 Loss: 0.014194738375475927]
[2024-04-17 13:04:07,909: INFO: main: Training : batch 1381 Loss: 0.013094247715066419]
[2024-04-17 13:04:08,555: INFO: main: Training : batch 1382 Loss: 0.004448702429638311]
[2024-04-17 13:04:09,197: INFO: main: Training : batch 1383 Loss: 0.010136149947223078]
[2024-04-17 13:04:09,832: INFO: main: Training : batch 1384 Loss: 0.01316241230742658]
[2024-04-17 13:04:10,480: INFO: main: Training : batch 1385 Loss: 0.006852400580089175]
[2024-04-17 13:04:11,115: INFO: main: Training : batch 1386 Loss: 0.01987748096096178]
[2024-04-17 13:04:11,757: INFO: main: Training : batch 1387 Loss: 0.007257926796706066]
[2024-04-17 13:04:12,391: INFO: main: Training : batch 1388 Loss: 0.0050368820571657655]
[2024-04-17 13:04:13,021: INFO: main: Training : batch 1389 Loss: 0.005776311267090189]
[2024-04-17 13:04:13,657: INFO: main: Training : batch 1390 Loss: 0.001515082857768746]
[2024-04-17 13:04:14,292: INFO: main: Training : batch 1391 Loss: 0.0026306805680263844]
[2024-04-17 13:04:14,924: INFO: main: Training : batch 1392 Loss: 0.003283554000636922]
[2024-04-17 13:04:15,557: INFO: main: Training : batch 1393 Loss: 0.005398304955116878]
[2024-04-17 13:04:16,186: INFO: main: Training : batch 1394 Loss: 0.002729704464330028]
[2024-04-17 13:04:16,814: INFO: main: Training : batch 1395 Loss: 0.0003420606584832027]
[2024-04-17 13:04:17,447: INFO: main: Training : batch 1396 Loss: 0.0012329539857526487]
[2024-04-17 13:04:18,076: INFO: main: Training : batch 1397 Loss: 0.004796055811581204]
[2024-04-17 13:04:18,709: INFO: main: Training : batch 1398 Loss: 0.0028869996471797763]
[2024-04-17 13:04:19,340: INFO: main: Training : batch 1399 Loss: 0.006768550837324421]
[2024-04-17 13:04:19,972: INFO: main: Training : batch 1400 Loss: 0.005652965571128333]
[2024-04-17 13:04:20,608: INFO: main: Training : batch 1401 Loss: 0.0018868460702195185]
[2024-04-17 13:04:21,250: INFO: main: Training : batch 1402 Loss: 0.0005545140987482654]
[2024-04-17 13:04:21,894: INFO: main: Training : batch 1403 Loss: 0.003693346437632092]
[2024-04-17 13:04:22,529: INFO: main: Training : batch 1404 Loss: 0.017848640779023865]
[2024-04-17 13:04:23,172: INFO: main: Training : batch 1405 Loss: 0.00010719089637496138]
[2024-04-17 13:04:23,809: INFO: main: Training : batch 1406 Loss: 0.0048181822783605254]
[2024-04-17 13:04:24,435: INFO: main: Training : batch 1407 Loss: 0.024874024191888187]
[2024-04-17 13:04:25,069: INFO: main: Training : batch 1408 Loss: 0.004506565303264713]
[2024-04-17 13:04:25,700: INFO: main: Training : batch 1409 Loss: 0.01851084936894757]
[2024-04-17 13:04:26,336: INFO: main: Training : batch 1410 Loss: 0.011258865621179458]
[2024-04-17 13:04:26,969: INFO: main: Training : batch 1411 Loss: 0.0004402746692180774]
[2024-04-17 13:04:27,601: INFO: main: Training : batch 1412 Loss: 0.015970468831835054]
[2024-04-17 13:04:28,232: INFO: main: Training : batch 1413 Loss: 0.020899456581339237]
[2024-04-17 13:04:28,862: INFO: main: Training : batch 1414 Loss: 0.005105710208481904]
[2024-04-17 13:04:29,493: INFO: main: Training : batch 1415 Loss: 0.006275402737441472]
[2024-04-17 13:04:30,124: INFO: main: Training : batch 1416 Loss: 0.018948037224518325]
[2024-04-17 13:04:30,756: INFO: main: Training : batch 1417 Loss: 0.0036982338697122024]
[2024-04-17 13:04:31,394: INFO: main: Training : batch 1418 Loss: 0.011483326895722828]
[2024-04-17 13:04:32,028: INFO: main: Training : batch 1419 Loss: 0.001779269763531271]
[2024-04-17 13:04:32,660: INFO: main: Training : batch 1420 Loss: 0.008100261070764538]
[2024-04-17 13:04:33,297: INFO: main: Training : batch 1421 Loss: 0.003310768112285373]
[2024-04-17 13:04:33,927: INFO: main: Training : batch 1422 Loss: 0.004862344608022413]
[2024-04-17 13:04:34,567: INFO: main: Training : batch 1423 Loss: 0.004722024613282137]
[2024-04-17 13:04:35,205: INFO: main: Training : batch 1424 Loss: 0.00046624320840962444]
[2024-04-17 13:04:35,842: INFO: main: Training : batch 1425 Loss: 0.007585751023351554]
[2024-04-17 13:04:36,481: INFO: main: Training : batch 1426 Loss: 0.01382412311516414]
[2024-04-17 13:04:37,122: INFO: main: Training : batch 1427 Loss: 0.005626641474377806]
[2024-04-17 13:04:37,754: INFO: main: Training : batch 1428 Loss: 0.010448732577890947]
[2024-04-17 13:04:38,383: INFO: main: Training : batch 1429 Loss: 0.003758185295723361]
[2024-04-17 13:04:38,582: INFO: main: Eval Epoch : batch 0 Loss: 0.005094819695076009]
[2024-04-17 13:04:38,782: INFO: main: Eval Epoch : batch 1 Loss: 0.003971953201178059]
[2024-04-17 13:04:38,983: INFO: main: Eval Epoch : batch 2 Loss: 0.02898691907471512]
[2024-04-17 13:04:39,182: INFO: main: Eval Epoch : batch 3 Loss: 0.005712123380178891]
[2024-04-17 13:04:39,389: INFO: main: Eval Epoch : batch 4 Loss: 0.004063104371475752]
[2024-04-17 13:04:39,587: INFO: main: Eval Epoch : batch 5 Loss: 0.012120430396091883]
[2024-04-17 13:04:39,789: INFO: main: Eval Epoch : batch 6 Loss: 0.011144628840614009]
[2024-04-17 13:04:39,991: INFO: main: Eval Epoch : batch 7 Loss: 0.004875384616724185]
[2024-04-17 13:04:40,190: INFO: main: Eval Epoch : batch 8 Loss: 0.011970827101327854]
[2024-04-17 13:04:40,397: INFO: main: Eval Epoch : batch 9 Loss: 0.015960136303286535]
[2024-04-17 13:04:40,598: INFO: main: Eval Epoch : batch 10 Loss: 0.01533382719606176]
[2024-04-17 13:04:40,801: INFO: main: Eval Epoch : batch 11 Loss: 0.034215493851017714]
[2024-04-17 13:04:41,001: INFO: main: Eval Epoch : batch 12 Loss: 0.010733396093466257]
[2024-04-17 13:04:41,204: INFO: main: Eval Epoch : batch 13 Loss: 0.01605301124723917]
[2024-04-17 13:04:41,411: INFO: main: Eval Epoch : batch 14 Loss: 0.009375542572769553]
[2024-04-17 13:04:41,621: INFO: main: Eval Epoch : batch 15 Loss: 0.0058674145993627535]
[2024-04-17 13:04:41,822: INFO: main: Eval Epoch : batch 16 Loss: 0.018225676569562532]
[2024-04-17 13:04:42,023: INFO: main: Eval Epoch : batch 17 Loss: 0.02567640619780723]
[2024-04-17 13:04:42,222: INFO: main: Eval Epoch : batch 18 Loss: 0.01605944169119497]
[2024-04-17 13:04:42,425: INFO: main: Eval Epoch : batch 19 Loss: 0.007866513928457356]
[2024-04-17 13:04:42,626: INFO: main: Eval Epoch : batch 20 Loss: 0.0026314979372064474]
[2024-04-17 13:04:42,824: INFO: main: Eval Epoch : batch 21 Loss: 0.012281694025058033]
[2024-04-17 13:04:43,024: INFO: main: Eval Epoch : batch 22 Loss: 0.013986699609110791]
[2024-04-17 13:04:43,231: INFO: main: Eval Epoch : batch 23 Loss: 0.019191208212482134]
[2024-04-17 13:04:43,435: INFO: main: Eval Epoch : batch 24 Loss: 0.027171063524486092]
[2024-04-17 13:04:43,636: INFO: main: Eval Epoch : batch 25 Loss: 0.008051574026893552]
[2024-04-17 13:04:43,836: INFO: main: Eval Epoch : batch 26 Loss: 0.00548579477083187]
[2024-04-17 13:04:44,037: INFO: main: Eval Epoch : batch 27 Loss: 0.010625899738409366]
[2024-04-17 13:04:44,240: INFO: main: Eval Epoch : batch 28 Loss: 0.004972007715269064]
[2024-04-17 13:04:44,446: INFO: main: Eval Epoch : batch 29 Loss: 0.0028262072714586497]
[2024-04-17 13:04:44,647: INFO: main: Eval Epoch : batch 30 Loss: 0.004947167104891623]
[2024-04-17 13:04:44,847: INFO: main: Eval Epoch : batch 31 Loss: 0.027196344755618433]
[2024-04-17 13:04:45,046: INFO: main: Eval Epoch : batch 32 Loss: 0.010077029361986907]
[2024-04-17 13:04:45,251: INFO: main: Eval Epoch : batch 33 Loss: 0.0033221956756095167]
[2024-04-17 13:04:45,458: INFO: main: Eval Epoch : batch 34 Loss: 0.02580366537401535]
[2024-04-17 13:04:45,660: INFO: main: Eval Epoch : batch 35 Loss: 0.015455468204377038]
[2024-04-17 13:04:45,858: INFO: main: Eval Epoch : batch 36 Loss: 0.007459987174736839]
[2024-04-17 13:04:46,063: INFO: main: Eval Epoch : batch 37 Loss: 0.03122813380462774]
[2024-04-17 13:04:46,269: INFO: main: Eval Epoch : batch 38 Loss: 0.013475150460321581]
[2024-04-17 13:04:46,472: INFO: main: Eval Epoch : batch 39 Loss: 0.0034132393695844475]
[2024-04-17 13:04:46,674: INFO: main: Eval Epoch : batch 40 Loss: 0.008424685244366075]
[2024-04-17 13:04:46,872: INFO: main: Eval Epoch : batch 41 Loss: 0.01872400988262001]
[2024-04-17 13:04:47,076: INFO: main: Eval Epoch : batch 42 Loss: 0.0014464258353821145]
[2024-04-17 13:04:47,287: INFO: main: Eval Epoch : batch 43 Loss: 0.00786564984042045]
[2024-04-17 13:04:47,495: INFO: main: Eval Epoch : batch 44 Loss: 0.01838157768216193]
[2024-04-17 13:04:47,701: INFO: main: Eval Epoch : batch 45 Loss: 0.009367231857200502]
[2024-04-17 13:04:47,905: INFO: main: Eval Epoch : batch 46 Loss: 0.012321162111237674]
[2024-04-17 13:04:48,112: INFO: main: Eval Epoch : batch 47 Loss: 0.03456985598723461]
[2024-04-17 13:04:48,322: INFO: main: Eval Epoch : batch 48 Loss: 0.02202830101051492]
[2024-04-17 13:04:48,530: INFO: main: Eval Epoch : batch 49 Loss: 0.006492408361953417]
[2024-04-17 13:04:48,733: INFO: main: Eval Epoch : batch 50 Loss: 0.015678485747327355]
[2024-04-17 13:04:48,939: INFO: main: Eval Epoch : batch 51 Loss: 0.0431125616393577]
[2024-04-17 13:04:49,143: INFO: main: Eval Epoch : batch 52 Loss: 0.0038954688225847564]
[2024-04-17 13:04:49,348: INFO: main: Eval Epoch : batch 53 Loss: 0.01464586626009654]
[2024-04-17 13:04:49,558: INFO: main: Eval Epoch : batch 54 Loss: 0.013615019152791119]
[2024-04-17 13:04:49,763: INFO: main: Eval Epoch : batch 55 Loss: 0.018466288446852712]
[2024-04-17 13:04:49,969: INFO: main: Eval Epoch : batch 56 Loss: 0.0012966629735838558]
[2024-04-17 13:04:50,173: INFO: main: Eval Epoch : batch 57 Loss: 0.0102212288682336]
[2024-04-17 13:04:50,380: INFO: main: Eval Epoch : batch 58 Loss: 0.00044650628971282415]
[2024-04-17 13:04:50,585: INFO: main: Eval Epoch : batch 59 Loss: 0.03902041241050737]
[2024-04-17 13:04:50,793: INFO: main: Eval Epoch : batch 60 Loss: 0.004657926391658545]
[2024-04-17 13:04:50,999: INFO: main: Eval Epoch : batch 61 Loss: 0.008142881041231818]
[2024-04-17 13:04:51,207: INFO: main: Eval Epoch : batch 62 Loss: 0.006021864375640823]
[2024-04-17 13:04:51,410: INFO: main: Eval Epoch : batch 63 Loss: 0.0019350199455687384]
[2024-04-17 13:04:51,610: INFO: main: Eval Epoch : batch 64 Loss: 0.01146621159658349]
[2024-04-17 13:04:51,816: INFO: main: Eval Epoch : batch 65 Loss: 0.010317979710337112]
[2024-04-17 13:04:52,019: INFO: main: Eval Epoch : batch 66 Loss: 0.00013664247173642295]
[2024-04-17 13:04:52,219: INFO: main: Eval Epoch : batch 67 Loss: 0.007438155199434781]
[2024-04-17 13:04:52,421: INFO: main: Eval Epoch : batch 68 Loss: 0.06815264324015319]
[2024-04-17 13:04:52,622: INFO: main: Eval Epoch : batch 69 Loss: 0.021459742650218744]
[2024-04-17 13:04:52,826: INFO: main: Eval Epoch : batch 70 Loss: 0.0008513574908018172]
[2024-04-17 13:04:53,032: INFO: main: Eval Epoch : batch 71 Loss: 0.007207337577360574]
[2024-04-17 13:04:53,233: INFO: main: Eval Epoch : batch 72 Loss: 0.015303792464012967]
[2024-04-17 13:04:53,432: INFO: main: Eval Epoch : batch 73 Loss: 0.002662017550702515]
[2024-04-17 13:04:53,632: INFO: main: Eval Epoch : batch 74 Loss: 0.006663474039059135]
[2024-04-17 13:04:53,837: INFO: main: Eval Epoch : batch 75 Loss: 0.014791324747999112]
[2024-04-17 13:04:54,039: INFO: main: Eval Epoch : batch 76 Loss: 0.004815053349374166]
[2024-04-17 13:04:54,243: INFO: main: Eval Epoch : batch 77 Loss: 0.00031755971608361675]
[2024-04-17 13:04:54,445: INFO: main: Eval Epoch : batch 78 Loss: 0.026435583172965526]
[2024-04-17 13:04:54,644: INFO: main: Eval Epoch : batch 79 Loss: 0.0127711303099527]
[2024-04-17 13:04:54,851: INFO: main: Eval Epoch : batch 80 Loss: 0.013949589602626511]
[2024-04-17 13:04:55,054: INFO: main: Eval Epoch : batch 81 Loss: 0.01273023824690886]
[2024-04-17 13:04:55,257: INFO: main: Eval Epoch : batch 82 Loss: 0.0070380698301513315]
[2024-04-17 13:04:55,456: INFO: main: Eval Epoch : batch 83 Loss: 0.009269909855991845]
[2024-04-17 13:04:55,657: INFO: main: Eval Epoch : batch 84 Loss: 0.00020265813176259325]
[2024-04-17 13:04:55,864: INFO: main: Eval Epoch : batch 85 Loss: 0.0433673380388334]
[2024-04-17 13:04:56,068: INFO: main: Eval Epoch : batch 86 Loss: 0.01658204235692971]
[2024-04-17 13:04:56,271: INFO: main: Eval Epoch : batch 87 Loss: 0.004947054763274636]
[2024-04-17 13:04:56,469: INFO: main: Eval Epoch : batch 88 Loss: 0.006914931198911811]
[2024-04-17 13:04:56,676: INFO: main: Eval Epoch : batch 89 Loss: 0.010995796049106965]
[2024-04-17 13:04:56,891: INFO: main: Eval Epoch : batch 90 Loss: 0.020936574222504504]
[2024-04-17 13:04:57,093: INFO: main: Eval Epoch : batch 91 Loss: 0.007308276509224277]
[2024-04-17 13:04:57,297: INFO: main: Eval Epoch : batch 92 Loss: 0.00033853129604598665]
[2024-04-17 13:04:57,498: INFO: main: Eval Epoch : batch 93 Loss: 0.005221282817571898]
[2024-04-17 13:04:57,706: INFO: main: Eval Epoch : batch 94 Loss: 0.020373346153866705]
[2024-04-17 13:04:57,909: INFO: main: Eval Epoch : batch 95 Loss: 0.007767264589801125]
[2024-04-17 13:04:58,109: INFO: main: Eval Epoch : batch 96 Loss: 0.015238489864792822]
[2024-04-17 13:04:58,308: INFO: main: Eval Epoch : batch 97 Loss: 0.003938551649132298]
[2024-04-17 13:04:58,510: INFO: main: Eval Epoch : batch 98 Loss: 0.03260389979637007]
[2024-04-17 13:04:58,718: INFO: main: Eval Epoch : batch 99 Loss: 0.003700182859712733]
[2024-04-17 13:04:58,918: INFO: main: Eval Epoch : batch 100 Loss: 0.02111108391408331]
[2024-04-17 13:04:59,119: INFO: main: Eval Epoch : batch 101 Loss: 0.01432256037624518]
[2024-04-17 13:04:59,323: INFO: main: Eval Epoch : batch 102 Loss: 0.005997594769282904]
[2024-04-17 13:04:59,524: INFO: main: Eval Epoch : batch 103 Loss: 0.0037857405551179495]
[2024-04-17 13:04:59,732: INFO: main: Eval Epoch : batch 104 Loss: 0.013600546323728022]
[2024-04-17 13:04:59,936: INFO: main: Eval Epoch : batch 105 Loss: 0.0019420203808844573]
[2024-04-17 13:05:00,136: INFO: main: Eval Epoch : batch 106 Loss: 0.011585901276857977]
[2024-04-17 13:05:00,335: INFO: main: Eval Epoch : batch 107 Loss: 0.0015148743412731957]
[2024-04-17 13:05:00,537: INFO: main: Eval Epoch : batch 108 Loss: 0.04074287964254075]
[2024-04-17 13:05:00,742: INFO: main: Eval Epoch : batch 109 Loss: 0.020637561019581762]
[2024-04-17 13:05:00,947: INFO: main: Eval Epoch : batch 110 Loss: 0.01143518173823935]
[2024-04-17 13:05:01,150: INFO: main: Eval Epoch : batch 111 Loss: 0.0028394399061879004]
[2024-04-17 13:05:01,350: INFO: main: Eval Epoch : batch 112 Loss: 0.021574390623689873]
[2024-04-17 13:05:01,556: INFO: main: Eval Epoch : batch 113 Loss: 0.004746814691297212]
[2024-04-17 13:05:01,764: INFO: main: Eval Epoch : batch 114 Loss: 0.024920909582011635]
[2024-04-17 13:05:01,970: INFO: main: Eval Epoch : batch 115 Loss: 0.009858808079524934]
[2024-04-17 13:05:02,174: INFO: main: Eval Epoch : batch 116 Loss: 0.00369644167692444]
[2024-04-17 13:05:02,377: INFO: main: Eval Epoch : batch 117 Loss: 0.0019442534201953878]
[2024-04-17 13:05:02,584: INFO: main: Eval Epoch : batch 118 Loss: 0.0073643475905162846]
[2024-04-17 13:05:02,791: INFO: main: Eval Epoch : batch 119 Loss: 0.02244629603032391]
[2024-04-17 13:05:02,997: INFO: main: Eval Epoch : batch 120 Loss: 0.0040662999015116325]
[2024-04-17 13:05:03,204: INFO: main: Eval Epoch : batch 121 Loss: 0.008684302203477977]
[2024-04-17 13:05:03,409: INFO: main: Eval Epoch : batch 122 Loss: 0.01476594390760359]
[2024-04-17 13:05:03,614: INFO: main: Eval Epoch : batch 123 Loss: 0.020550209861153287]
[2024-04-17 13:05:03,822: INFO: main: Eval Epoch : batch 124 Loss: 0.0024104210525063238]
[2024-04-17 13:05:04,034: INFO: main: Eval Epoch : batch 125 Loss: 0.005128648196458656]
[2024-04-17 13:05:04,243: INFO: main: Eval Epoch : batch 126 Loss: 0.0026542896792343777]
[2024-04-17 13:05:04,445: INFO: main: Eval Epoch : batch 127 Loss: 0.003100578195282615]
[2024-04-17 13:05:04,656: INFO: main: Eval Epoch : batch 128 Loss: 0.011516355144545925]
[2024-04-17 13:05:04,861: INFO: main: Eval Epoch : batch 129 Loss: 0.011328709245752638]
[2024-04-17 13:05:05,069: INFO: main: Eval Epoch : batch 130 Loss: 0.011490787747128181]
[2024-04-17 13:05:05,272: INFO: main: Eval Epoch : batch 131 Loss: 0.010937610357425555]
[2024-04-17 13:05:05,475: INFO: main: Eval Epoch : batch 132 Loss: 0.0020039217929644854]
[2024-04-17 13:05:05,676: INFO: main: Eval Epoch : batch 133 Loss: 0.009815262745099407]
[2024-04-17 13:05:05,878: INFO: main: Eval Epoch : batch 134 Loss: 0.00451886795730708]
[2024-04-17 13:05:06,079: INFO: main: Eval Epoch : batch 135 Loss: 0.0016164068717272543]
[2024-04-17 13:05:06,283: INFO: main: Eval Epoch : batch 136 Loss: 0.028032095085350716]
[2024-04-17 13:05:06,486: INFO: main: Eval Epoch : batch 137 Loss: 0.0175855050044006]
[2024-04-17 13:05:06,687: INFO: main: Eval Epoch : batch 138 Loss: 0.010126111643135467]
[2024-04-17 13:05:06,889: INFO: main: Eval Epoch : batch 139 Loss: 0.0947889415802353]
[2024-04-17 13:05:07,095: INFO: main: Eval Epoch : batch 140 Loss: 0.02832932540309979]
[2024-04-17 13:05:07,298: INFO: main: Eval Epoch : batch 141 Loss: 0.016135698024458904]
[2024-04-17 13:05:07,499: INFO: main: Eval Epoch : batch 142 Loss: 0.0009887884536458297]
[2024-04-17 13:05:07,697: INFO: main: Eval Epoch : batch 143 Loss: 0.02699882799065435]
[2024-04-17 13:05:07,903: INFO: main: Eval Epoch : batch 144 Loss: 0.009915916138526937]
[2024-04-17 13:05:08,104: INFO: main: Eval Epoch : batch 145 Loss: 0.00877151275846552]
[2024-04-17 13:05:08,304: INFO: main: Eval Epoch : batch 146 Loss: 0.006133256529591518]
[2024-04-17 13:05:08,507: INFO: main: Eval Epoch : batch 147 Loss: 0.021609202091095986]
[2024-04-17 13:05:08,708: INFO: main: Eval Epoch : batch 148 Loss: 0.017105561270314963]
[2024-04-17 13:05:08,912: INFO: main: Eval Epoch : batch 149 Loss: 0.005463786567206244]
[2024-04-17 13:05:09,115: INFO: main: Eval Epoch : batch 150 Loss: 0.04293925443054335]
[2024-04-17 13:05:09,314: INFO: main: Eval Epoch : batch 151 Loss: 0.03232007302330438]
[2024-04-17 13:05:09,517: INFO: main: Eval Epoch : batch 152 Loss: 0.0006126734850225975]
[2024-04-17 13:05:09,717: INFO: main: Eval Epoch : batch 153 Loss: 0.0016081391386709282]
[2024-04-17 13:05:09,918: INFO: main: Eval Epoch : batch 154 Loss: 0.008219719129576891]
[2024-04-17 13:05:10,125: INFO: main: Eval Epoch : batch 155 Loss: 0.07823770775465592]
[2024-04-17 13:05:10,326: INFO: main: Eval Epoch : batch 156 Loss: 0.013818639191994988]
[2024-04-17 13:05:10,529: INFO: main: Eval Epoch : batch 157 Loss: 0.009466044948126865]
[2024-04-17 13:05:10,729: INFO: main: Eval Epoch : batch 158 Loss: 0.010518943419854966]
[2024-04-17 13:05:10,931: INFO: main: Eval Epoch : batch 159 Loss: 0.0037186382331579077]
[2024-04-17 13:05:11,132: INFO: main: Eval Epoch : batch 160 Loss: 0.008828251436935015]
[2024-04-17 13:05:11,335: INFO: main: Eval Epoch : batch 161 Loss: 0.017036913193458444]
[2024-04-17 13:05:11,538: INFO: main: Eval Epoch : batch 162 Loss: 0.011008819677548798]
[2024-04-17 13:05:11,740: INFO: main: Eval Epoch : batch 163 Loss: 0.0073558036418944536]
[2024-04-17 13:05:11,942: INFO: main: Eval Epoch : batch 164 Loss: 0.033611799273364905]
[2024-04-17 13:05:12,147: INFO: main: Eval Epoch : batch 165 Loss: 0.012460134679547775]
[2024-04-17 13:05:12,350: INFO: main: Eval Epoch : batch 166 Loss: 0.010034455306833782]
[2024-04-17 13:05:12,553: INFO: main: Eval Epoch : batch 167 Loss: 0.0027927557185511483]
[2024-04-17 13:05:12,754: INFO: main: Eval Epoch : batch 168 Loss: 0.011715294238395263]
[2024-04-17 13:05:12,954: INFO: main: Eval Epoch : batch 169 Loss: 0.012963538797156043]
[2024-04-17 13:05:13,159: INFO: main: Eval Epoch : batch 170 Loss: 0.007247793863674294]
[2024-04-17 13:05:13,359: INFO: main: Eval Epoch : batch 171 Loss: 0.004307842049781906]
[2024-04-17 13:05:13,559: INFO: main: Eval Epoch : batch 172 Loss: 0.01924481959688891]
[2024-04-17 13:05:13,761: INFO: main: Eval Epoch : batch 173 Loss: 0.0002965198803166681]
[2024-04-17 13:05:13,963: INFO: main: Eval Epoch : batch 174 Loss: 0.011962466150333041]
[2024-04-17 13:05:14,166: INFO: main: Eval Epoch : batch 175 Loss: 0.004015944998268321]
[2024-04-17 13:05:14,368: INFO: main: Eval Epoch : batch 176 Loss: 0.006420610462338036]
[2024-04-17 13:05:14,566: INFO: main: Eval Epoch : batch 177 Loss: 0.0012818075996712179]
[2024-04-17 13:05:14,765: INFO: main: Eval Epoch : batch 178 Loss: 0.001190425639856876]
[2024-04-17 13:05:14,963: INFO: main: Eval Epoch : batch 179 Loss: 0.00038194852189516614]
[2024-04-17 13:05:15,164: INFO: main: Eval Epoch : batch 180 Loss: 0.010481359958455054]
[2024-04-17 13:05:15,379: INFO: main: Eval Epoch : batch 181 Loss: 0.0025095514142740244]
[2024-04-17 13:05:15,584: INFO: main: Eval Epoch : batch 182 Loss: 0.011126318540510369]
[2024-04-17 13:05:15,793: INFO: main: Eval Epoch : batch 183 Loss: 0.008367163777673473]
[2024-04-17 13:05:15,996: INFO: main: Eval Epoch : batch 184 Loss: 0.014526592664633669]
[2024-04-17 13:05:16,211: INFO: main: Eval Epoch : batch 185 Loss: 0.0021046067622198534]
[2024-04-17 13:05:16,421: INFO: main: Eval Epoch : batch 186 Loss: 0.003434184831859855]
[2024-04-17 13:05:16,626: INFO: main: Eval Epoch : batch 187 Loss: 0.04993656356121096]
[2024-04-17 13:05:16,831: INFO: main: Eval Epoch : batch 188 Loss: 0.007433582687402425]
[2024-04-17 13:05:17,036: INFO: main: Eval Epoch : batch 189 Loss: 0.017356559328866452]
[2024-04-17 13:05:17,242: INFO: main: Eval Epoch : batch 190 Loss: 0.002841369772785837]
[2024-04-17 13:05:17,448: INFO: main: Eval Epoch : batch 191 Loss: 0.013660270014936192]
[2024-04-17 13:05:17,652: INFO: main: Eval Epoch : batch 192 Loss: 0.0012806941924270168]
[2024-04-17 13:05:17,860: INFO: main: Eval Epoch : batch 193 Loss: 0.006701741158096475]
[2024-04-17 13:05:18,067: INFO: main: Eval Epoch : batch 194 Loss: 0.013766004625285634]
[2024-04-17 13:05:18,270: INFO: main: Eval Epoch : batch 195 Loss: 0.009554845111799904]
[2024-04-17 13:05:18,476: INFO: main: Eval Epoch : batch 196 Loss: 0.005959084890018065]
[2024-04-17 13:05:18,689: INFO: main: Eval Epoch : batch 197 Loss: 0.012922700206172321]
[2024-04-17 13:05:18,906: INFO: main: Eval Epoch : batch 198 Loss: 0.00330614612646681]
[2024-04-17 13:05:19,110: INFO: main: Eval Epoch : batch 199 Loss: 0.005558004502750847]
[2024-04-17 13:05:19,308: INFO: main: Eval Epoch : batch 200 Loss: 0.0008652396670274773]
[2024-04-17 13:05:19,509: INFO: main: Eval Epoch : batch 201 Loss: 0.03129545057359916]
[2024-04-17 13:05:19,720: INFO: main: Eval Epoch : batch 202 Loss: 0.013198187768321892]
[2024-04-17 13:05:19,921: INFO: main: Eval Epoch : batch 203 Loss: 0.0031517555495033625]
[2024-04-17 13:05:20,121: INFO: main: Eval Epoch : batch 204 Loss: 0.010658683159253021]
[2024-04-17 13:05:20,319: INFO: main: Eval Epoch : batch 205 Loss: 0.005237412013161745]
[2024-04-17 13:05:20,519: INFO: main: Eval Epoch : batch 206 Loss: 0.0005251167032898698]
[2024-04-17 13:05:20,729: INFO: main: Eval Epoch : batch 207 Loss: 0.0012034649011819778]
[2024-04-17 13:05:20,931: INFO: main: Eval Epoch : batch 208 Loss: 0.001226830570167316]
[2024-04-17 13:05:21,132: INFO: main: Eval Epoch : batch 209 Loss: 0.002848488614444239]
[2024-04-17 13:05:21,332: INFO: main: Eval Epoch : batch 210 Loss: 0.006056449470287505]
[2024-04-17 13:05:21,531: INFO: main: Eval Epoch : batch 211 Loss: 0.01085537860485556]
[2024-04-17 13:05:21,739: INFO: main: Eval Epoch : batch 212 Loss: 0.006510912979142092]
[2024-04-17 13:05:21,942: INFO: main: Eval Epoch : batch 213 Loss: 0.003346715591409566]
[2024-04-17 13:05:22,145: INFO: main: Eval Epoch : batch 214 Loss: 0.005230876419370483]
[2024-04-17 13:05:22,345: INFO: main: Eval Epoch : batch 215 Loss: 0.02095654247477691]
[2024-04-17 13:05:22,544: INFO: main: Eval Epoch : batch 216 Loss: 0.010016972132651222]
[2024-04-17 13:05:22,749: INFO: main: Eval Epoch : batch 217 Loss: 0.007794886780102271]
[2024-04-17 13:05:22,951: INFO: main: Eval Epoch : batch 218 Loss: 0.013198348234924495]
[2024-04-17 13:05:23,156: INFO: main: Eval Epoch : batch 219 Loss: 0.0004856054904361862]
[2024-04-17 13:05:23,354: INFO: main: Eval Epoch : batch 220 Loss: 0.002147339114129052]
[2024-04-17 13:05:23,552: INFO: main: Eval Epoch : batch 221 Loss: 0.02568952010745196]
[2024-04-17 13:05:23,753: INFO: main: Eval Epoch : batch 222 Loss: 9.486941168571471e-05]
[2024-04-17 13:05:23,958: INFO: main: Eval Epoch : batch 223 Loss: 0.02433970420334075]
[2024-04-17 13:05:24,162: INFO: main: Eval Epoch : batch 224 Loss: 0.0009880939994388698]
[2024-04-17 13:05:24,364: INFO: main: Eval Epoch : batch 225 Loss: 0.015560912321782978]
[2024-04-17 13:05:24,566: INFO: main: Eval Epoch : batch 226 Loss: 0.028684411095689324]
[2024-04-17 13:05:24,766: INFO: main: Eval Epoch : batch 227 Loss: 0.02421719426114495]
[2024-04-17 13:05:24,970: INFO: main: Eval Epoch : batch 228 Loss: 0.0019784481292086627]
[2024-04-17 13:05:25,173: INFO: main: Eval Epoch : batch 229 Loss: 0.0019221863813069884]
[2024-04-17 13:05:25,374: INFO: main: Eval Epoch : batch 230 Loss: 0.00276552374376938]
[2024-04-17 13:05:25,574: INFO: main: Eval Epoch : batch 231 Loss: 0.024707272994960314]
[2024-04-17 13:05:25,778: INFO: main: Eval Epoch : batch 232 Loss: 0.018036615635451517]
[2024-04-17 13:05:25,982: INFO: main: Eval Epoch : batch 233 Loss: 0.011808191429431283]
[2024-04-17 13:05:26,186: INFO: main: Eval Epoch : batch 234 Loss: 0.019262331609312382]
[2024-04-17 13:05:26,386: INFO: main: Eval Epoch : batch 235 Loss: 0.006081186421777652]
[2024-04-17 13:05:26,587: INFO: main: Eval Epoch : batch 236 Loss: 0.023848365695306708]
[2024-04-17 13:05:26,788: INFO: main: Eval Epoch : batch 237 Loss: 0.0002443055362247805]
[2024-04-17 13:05:26,989: INFO: main: Eval Epoch : batch 238 Loss: 0.005407383984564416]
[2024-04-17 13:05:27,189: INFO: main: Eval Epoch : batch 239 Loss: 0.025644651267302712]
[2024-04-17 13:05:27,390: INFO: main: Eval Epoch : batch 240 Loss: 0.007485868140107479]
[2024-04-17 13:05:27,590: INFO: main: Eval Epoch : batch 241 Loss: 0.0047214008692176455]
[2024-04-17 13:05:27,795: INFO: main: Eval Epoch : batch 242 Loss: 0.03426054545010799]
[2024-04-17 13:05:27,994: INFO: main: Eval Epoch : batch 243 Loss: 0.00802408530041704]
[2024-04-17 13:05:28,195: INFO: main: Eval Epoch : batch 244 Loss: 0.003189534792233048]
[2024-04-17 13:05:28,397: INFO: main: Eval Epoch : batch 245 Loss: 0.004438929088847409]
[2024-04-17 13:05:28,597: INFO: main: Eval Epoch : batch 246 Loss: 0.005248436882424488]
[2024-04-17 13:05:28,803: INFO: main: Eval Epoch : batch 247 Loss: 0.01575788175884198]
[2024-04-17 13:05:29,003: INFO: main: Eval Epoch : batch 248 Loss: 0.004462913770389889]
[2024-04-17 13:05:29,209: INFO: main: Eval Epoch : batch 249 Loss: 0.013923427310236387]
[2024-04-17 13:05:29,415: INFO: main: Eval Epoch : batch 250 Loss: 0.0006532180139586484]
[2024-04-17 13:05:29,621: INFO: main: Eval Epoch : batch 251 Loss: 0.005891544129566206]
[2024-04-17 13:05:29,830: INFO: main: Eval Epoch : batch 252 Loss: 0.1197603266064008]
[2024-04-17 13:05:30,037: INFO: main: Eval Epoch : batch 253 Loss: 0.026898469856391046]
[2024-04-17 13:05:30,241: INFO: main: Eval Epoch : batch 254 Loss: 0.0029178557392769704]
[2024-04-17 13:05:30,449: INFO: main: Eval Epoch : batch 255 Loss: 0.01341382693823578]
[2024-04-17 13:05:30,660: INFO: main: Eval Epoch : batch 256 Loss: 0.017314162767178072]
[2024-04-17 13:05:30,867: INFO: main: Eval Epoch : batch 257 Loss: 0.03517757519692497]
[2024-04-17 13:05:31,072: INFO: main: Eval Epoch : batch 258 Loss: 0.045360513042128396]
[2024-04-17 13:05:31,279: INFO: main: Eval Epoch : batch 259 Loss: 0.010246066430036234]
[2024-04-17 13:05:31,491: INFO: main: Eval Epoch : batch 260 Loss: 0.038493150116931926]
[2024-04-17 13:05:31,696: INFO: main: Eval Epoch : batch 261 Loss: 0.009057348439674817]
[2024-04-17 13:05:31,902: INFO: main: Eval Epoch : batch 262 Loss: 0.008094861657326898]
[2024-04-17 13:05:32,108: INFO: main: Eval Epoch : batch 263 Loss: 0.01128960771123889]
[2024-04-17 13:05:32,315: INFO: main: Eval Epoch : batch 264 Loss: 0.07331777150282692]
[2024-04-17 13:05:32,525: INFO: main: Eval Epoch : batch 265 Loss: 0.059248016948809414]
[2024-04-17 13:05:32,732: INFO: main: Eval Epoch : batch 266 Loss: 0.006714394252157459]
[2024-04-17 13:05:32,937: INFO: main: Eval Epoch : batch 267 Loss: 0.009848352833648693]
[2024-04-17 13:05:33,139: INFO: main: Eval Epoch : batch 268 Loss: 0.010192147102730593]
[2024-04-17 13:05:33,344: INFO: main: Eval Epoch : batch 269 Loss: 0.02234719749919355]
[2024-04-17 13:05:33,548: INFO: main: Eval Epoch : batch 270 Loss: 0.028674521111582144]
[2024-04-17 13:05:33,753: INFO: main: Eval Epoch : batch 271 Loss: 0.0026263992421949557]
[2024-04-17 13:05:33,954: INFO: main: Eval Epoch : batch 272 Loss: 0.0195057993070251]
[2024-04-17 13:05:34,155: INFO: main: Eval Epoch : batch 273 Loss: 0.0012318943139045684]
[2024-04-17 13:05:34,356: INFO: main: Eval Epoch : batch 274 Loss: 0.0017473706618382962]
[2024-04-17 13:05:34,561: INFO: main: Eval Epoch : batch 275 Loss: 0.0011456898774492763]
[2024-04-17 13:05:34,765: INFO: main: Eval Epoch : batch 276 Loss: 0.007711594321333654]
[2024-04-17 13:05:34,968: INFO: main: Eval Epoch : batch 277 Loss: 0.00815398500261486]
[2024-04-17 13:05:35,168: INFO: main: Eval Epoch : batch 278 Loss: 0.024342308133668413]
[2024-04-17 13:05:35,368: INFO: main: Eval Epoch : batch 279 Loss: 0.003364925555791046]
[2024-04-17 13:05:35,569: INFO: main: Eval Epoch : batch 280 Loss: 0.00688513569764213]
[2024-04-17 13:05:35,773: INFO: main: Eval Epoch : batch 281 Loss: 0.010978630732177237]
[2024-04-17 13:05:35,978: INFO: main: Eval Epoch : batch 282 Loss: 0.007527786733598966]
[2024-04-17 13:05:36,180: INFO: main: Eval Epoch : batch 283 Loss: 0.010472054817881825]
[2024-04-17 13:05:36,380: INFO: main: Eval Epoch : batch 284 Loss: 0.01829141470255561]
[2024-04-17 13:05:36,587: INFO: main: Eval Epoch : batch 285 Loss: 0.0016862294137951607]
[2024-04-17 13:05:36,788: INFO: main: Eval Epoch : batch 286 Loss: 0.0013960748167032584]
[2024-04-17 13:05:36,994: INFO: main: Eval Epoch : batch 287 Loss: 0.002428679798986764]
[2024-04-17 13:05:37,196: INFO: main: Eval Epoch : batch 288 Loss: 0.0041065013995273]
[2024-04-17 13:05:37,400: INFO: main: Eval Epoch : batch 289 Loss: 0.003755003336598149]
[2024-04-17 13:05:37,598: INFO: main: Eval Epoch : batch 290 Loss: 0.01252396451527558]
[2024-04-17 13:05:37,800: INFO: main: Eval Epoch : batch 291 Loss: 0.0006472845543685713]
[2024-04-17 13:05:38,006: INFO: main: Eval Epoch : batch 292 Loss: 0.013202943783734608]
[2024-04-17 13:05:38,217: INFO: main: Eval Epoch : batch 293 Loss: 0.00431222211548261]
[2024-04-17 13:05:38,422: INFO: main: Eval Epoch : batch 294 Loss: 0.016342142534750605]
[2024-04-17 13:05:38,621: INFO: main: Eval Epoch : batch 295 Loss: 0.002445691849246382]
[2024-04-17 13:05:38,821: INFO: main: Eval Epoch : batch 296 Loss: 0.0300883309953965]
[2024-04-17 13:05:39,023: INFO: main: Eval Epoch : batch 297 Loss: 0.01340544362203747]
[2024-04-17 13:05:39,223: INFO: main: Eval Epoch : batch 298 Loss: 0.015343101466327133]
[2024-04-17 13:05:39,428: INFO: main: Eval Epoch : batch 299 Loss: 0.011120794685082421]
[2024-04-17 13:05:39,628: INFO: main: Eval Epoch : batch 300 Loss: 0.015496196364417373]
[2024-04-17 13:05:39,830: INFO: main: Eval Epoch : batch 301 Loss: 0.0004520067866185939]
[2024-04-17 13:05:40,036: INFO: main: Eval Epoch : batch 302 Loss: 0.01072822439051839]
[2024-04-17 13:05:40,235: INFO: main: Eval Epoch : batch 303 Loss: 0.0002961871742046262]
[2024-04-17 13:05:40,438: INFO: main: Eval Epoch : batch 304 Loss: 0.005291432958230495]
[2024-04-17 13:05:40,639: INFO: main: Eval Epoch : batch 305 Loss: 0.005334552601130563]
[2024-04-17 13:05:40,841: INFO: main: Eval Epoch : batch 306 Loss: 0.006815943747123039]
[2024-04-17 13:05:41,044: INFO: main: Eval Epoch : batch 307 Loss: 0.00516941643133589]
[2024-04-17 13:05:41,248: INFO: main: Eval Epoch : batch 308 Loss: 0.011165079708395005]
[2024-04-17 13:05:41,453: INFO: main: Eval Epoch : batch 309 Loss: 0.02620582683617115]
[2024-04-17 13:05:41,654: INFO: main: Eval Epoch : batch 310 Loss: 0.034363465937337405]
[2024-04-17 13:05:41,856: INFO: main: Eval Epoch : batch 311 Loss: 0.008156633485474591]
[2024-04-17 13:05:42,060: INFO: main: Eval Epoch : batch 312 Loss: 0.002573707544215594]
[2024-04-17 13:05:42,264: INFO: main: Eval Epoch : batch 313 Loss: 0.013327302878115182]
[2024-04-17 13:05:42,465: INFO: main: Eval Epoch : batch 314 Loss: 0.01572381967814302]
[2024-04-17 13:05:42,665: INFO: main: Eval Epoch : batch 315 Loss: 0.001218374770435835]
[2024-04-17 13:05:42,867: INFO: main: Eval Epoch : batch 316 Loss: 0.007680469009475195]
[2024-04-17 13:05:43,079: INFO: main: Eval Epoch : batch 317 Loss: 0.0007476451480663735]
[2024-04-17 13:05:43,285: INFO: main: Eval Epoch : batch 318 Loss: 0.0035535948445729413]
[2024-04-17 13:05:43,489: INFO: main: Eval Epoch : batch 319 Loss: 0.005241812766124037]
[2024-04-17 13:05:43,694: INFO: main: Eval Epoch : batch 320 Loss: 0.0004937511599401948]
[2024-04-17 13:05:43,902: INFO: main: Eval Epoch : batch 321 Loss: 0.005346450297551419]
[2024-04-17 13:05:44,112: INFO: main: Eval Epoch : batch 322 Loss: 0.03538188464317944]
[2024-04-17 13:05:44,320: INFO: main: Eval Epoch : batch 323 Loss: 0.007438975227590915]
[2024-04-17 13:05:44,529: INFO: main: Eval Epoch : batch 324 Loss: 0.012514889857461606]
[2024-04-17 13:05:44,739: INFO: main: Eval Epoch : batch 325 Loss: 0.0029921926844491617]
[2024-04-17 13:05:44,947: INFO: main: Eval Epoch : batch 326 Loss: 0.016327503674073596]
[2024-04-17 13:05:45,150: INFO: main: Eval Epoch : batch 327 Loss: 0.00834052238667336]
[2024-04-17 13:05:45,358: INFO: main: Eval Epoch : batch 328 Loss: 0.001234580458461154]
[2024-04-17 13:05:45,563: INFO: main: Eval Epoch : batch 329 Loss: 0.007161619369721155]
[2024-04-17 13:05:45,770: INFO: main: Eval Epoch : batch 330 Loss: 0.010137481812497389]
[2024-04-17 13:05:45,978: INFO: main: Eval Epoch : batch 331 Loss: 0.016601986446863365]
[2024-04-17 13:05:46,185: INFO: main: Eval Epoch : batch 332 Loss: 0.005247669289744989]
[2024-04-17 13:05:46,394: INFO: main: Eval Epoch : batch 333 Loss: 0.0248980914044539]
[2024-04-17 13:05:46,602: INFO: main: Eval Epoch : batch 334 Loss: 0.04598675813206613]
[2024-04-17 13:05:46,816: INFO: main: Eval Epoch : batch 335 Loss: 0.001939471885646689]
[2024-04-17 13:05:47,017: INFO: main: Eval Epoch : batch 336 Loss: 0.004049860390209766]
[2024-04-17 13:05:47,218: INFO: main: Eval Epoch : batch 337 Loss: 0.036740953257718575]
[2024-04-17 13:05:47,421: INFO: main: Eval Epoch : batch 338 Loss: 0.00039819097441080376]
[2024-04-17 13:05:47,632: INFO: main: Eval Epoch : batch 339 Loss: 0.0011822842228626867]
[2024-04-17 13:05:47,829: INFO: main: Eval Epoch : batch 340 Loss: 0.009575438435303592]
[2024-04-17 13:05:48,030: INFO: main: Eval Epoch : batch 341 Loss: 0.017636993100455214]
[2024-04-17 13:05:48,231: INFO: main: Eval Epoch : batch 342 Loss: 0.00022799946378982168]
[2024-04-17 13:05:48,434: INFO: main: Eval Epoch : batch 343 Loss: 0.014436988987024531]
[2024-04-17 13:05:48,639: INFO: main: Eval Epoch : batch 344 Loss: 0.0228389033231443]
[2024-04-17 13:05:48,840: INFO: main: Eval Epoch : batch 345 Loss: 0.026110692232989896]
[2024-04-17 13:05:49,040: INFO: main: Eval Epoch : batch 346 Loss: 0.0069326765833458035]
[2024-04-17 13:05:49,241: INFO: main: Eval Epoch : batch 347 Loss: 0.014432082382666607]
[2024-04-17 13:05:49,443: INFO: main: Eval Epoch : batch 348 Loss: 0.005476649271827366]
[2024-04-17 13:05:49,651: INFO: main: Eval Epoch : batch 349 Loss: 0.0036640511641612425]
[2024-04-17 13:05:49,852: INFO: main: Eval Epoch : batch 350 Loss: 0.0007202931724927449]
[2024-04-17 13:05:50,052: INFO: main: Eval Epoch : batch 351 Loss: 0.018718484519341886]
[2024-04-17 13:05:50,251: INFO: main: Eval Epoch : batch 352 Loss: 0.004145946402179202]
[2024-04-17 13:05:50,453: INFO: main: Eval Epoch : batch 353 Loss: 0.008015893467180894]
[2024-04-17 13:05:50,660: INFO: main: Eval Epoch : batch 354 Loss: 0.015488965974035535]
[2024-04-17 13:05:50,865: INFO: main: Eval Epoch : batch 355 Loss: 0.027905399907540613]
[2024-04-17 13:05:51,071: INFO: main: Eval Epoch : batch 356 Loss: 0.002727353433946813]
[2024-04-17 13:05:51,170: INFO: main: Eval Epoch : batch 357 Loss: 0.00571784160830425]
[2024-04-17 13:06:05,946: INFO: main: The score of the eval model is {'Accuracy': 0.9938722609989497, 'precision': 0.7552137183205144, 'recall': 0.8660246956421156, 'f1': 0.8068322646694327}]
[2024-04-17 13:06:07,652: INFO: main: Epoch: 5/5]
[2024-04-17 13:06:08,302: INFO: main: Training : batch 0 Loss: 0.0023554347541880068]
[2024-04-17 13:06:08,918: INFO: main: Training : batch 1 Loss: 0.007164263039872074]
[2024-04-17 13:06:09,537: INFO: main: Training : batch 2 Loss: 0.001945257595840059]
[2024-04-17 13:06:10,151: INFO: main: Training : batch 3 Loss: 0.0016658073281151807]
[2024-04-17 13:06:10,770: INFO: main: Training : batch 4 Loss: 0.005158356010143447]
[2024-04-17 13:06:11,392: INFO: main: Training : batch 5 Loss: 0.007786464354405561]
[2024-04-17 13:06:12,016: INFO: main: Training : batch 6 Loss: 0.0018853445865208971]
[2024-04-17 13:06:12,646: INFO: main: Training : batch 7 Loss: 0.002367448159554705]
[2024-04-17 13:06:13,271: INFO: main: Training : batch 8 Loss: 0.009360302999064098]
[2024-04-17 13:06:13,899: INFO: main: Training : batch 9 Loss: 0.0019488312480396743]
[2024-04-17 13:06:14,525: INFO: main: Training : batch 10 Loss: 0.006484618040377875]
[2024-04-17 13:06:15,152: INFO: main: Training : batch 11 Loss: 0.0054713438361310455]
[2024-04-17 13:06:15,778: INFO: main: Training : batch 12 Loss: 0.0048045439123164686]
[2024-04-17 13:06:16,404: INFO: main: Training : batch 13 Loss: 0.0022795351985538947]
[2024-04-17 13:06:17,031: INFO: main: Training : batch 14 Loss: 0.010706066076294282]
[2024-04-17 13:06:17,661: INFO: main: Training : batch 15 Loss: 0.0011231525448245929]
[2024-04-17 13:06:18,299: INFO: main: Training : batch 16 Loss: 0.0007401833882001294]
[2024-04-17 13:06:18,934: INFO: main: Training : batch 17 Loss: 0.010167328050535964]
[2024-04-17 13:06:19,569: INFO: main: Training : batch 18 Loss: 0.0005783136692429356]
[2024-04-17 13:06:20,207: INFO: main: Training : batch 19 Loss: 0.016973026991617405]
[2024-04-17 13:06:20,846: INFO: main: Training : batch 20 Loss: 0.0017446514867001763]
[2024-04-17 13:06:21,484: INFO: main: Training : batch 21 Loss: 0.011055612922990201]
[2024-04-17 13:06:22,115: INFO: main: Training : batch 22 Loss: 0.0005316377973605405]
[2024-04-17 13:06:22,755: INFO: main: Training : batch 23 Loss: 0.0017566029795450953]
[2024-04-17 13:06:23,390: INFO: main: Training : batch 24 Loss: 0.0017909476273992608]
[2024-04-17 13:06:24,032: INFO: main: Training : batch 25 Loss: 0.0032095674295723273]
[2024-04-17 13:06:24,679: INFO: main: Training : batch 26 Loss: 0.012216062695234918]
[2024-04-17 13:06:25,327: INFO: main: Training : batch 27 Loss: 0.027187319297658517]
[2024-04-17 13:06:25,977: INFO: main: Training : batch 28 Loss: 0.0014247417643374876]
[2024-04-17 13:06:26,628: INFO: main: Training : batch 29 Loss: 0.005117380867378607]
[2024-04-17 13:06:27,277: INFO: main: Training : batch 30 Loss: 0.00126284282304145]
[2024-04-17 13:06:27,920: INFO: main: Training : batch 31 Loss: 0.013979571275680829]
[2024-04-17 13:06:28,566: INFO: main: Training : batch 32 Loss: 0.006362322433247913]
[2024-04-17 13:06:29,204: INFO: main: Training : batch 33 Loss: 0.00599319590732997]
[2024-04-17 13:06:29,844: INFO: main: Training : batch 34 Loss: 0.005371961756598504]
[2024-04-17 13:06:30,491: INFO: main: Training : batch 35 Loss: 0.008031518289074134]
[2024-04-17 13:06:31,142: INFO: main: Training : batch 36 Loss: 0.01342328314283197]
[2024-04-17 13:06:31,787: INFO: main: Training : batch 37 Loss: 0.004620319204056909]
[2024-04-17 13:06:32,437: INFO: main: Training : batch 38 Loss: 0.00040308997328110575]
[2024-04-17 13:06:33,085: INFO: main: Training : batch 39 Loss: 0.00913643489633784]
[2024-04-17 13:06:33,732: INFO: main: Training : batch 40 Loss: 0.016223678042149168]
[2024-04-17 13:06:34,383: INFO: main: Training : batch 41 Loss: 0.0075512016051580986]
[2024-04-17 13:06:35,031: INFO: main: Training : batch 42 Loss: 0.001670451850031073]
[2024-04-17 13:06:35,671: INFO: main: Training : batch 43 Loss: 0.009910275713974844]
[2024-04-17 13:06:36,319: INFO: main: Training : batch 44 Loss: 0.00377852236872064]
[2024-04-17 13:06:36,967: INFO: main: Training : batch 45 Loss: 0.005097502807552568]
[2024-04-17 13:06:37,616: INFO: main: Training : batch 46 Loss: 0.011091144301597551]
[2024-04-17 13:06:38,275: INFO: main: Training : batch 47 Loss: 0.011155634249294208]
[2024-04-17 13:06:38,928: INFO: main: Training : batch 48 Loss: 0.0034014470815535835]
[2024-04-17 13:06:39,582: INFO: main: Training : batch 49 Loss: 0.003946244309481721]
[2024-04-17 13:06:40,239: INFO: main: Training : batch 50 Loss: 0.024273877775445978]
[2024-04-17 13:06:40,891: INFO: main: Training : batch 51 Loss: 0.0065581961845595]
[2024-04-17 13:06:41,548: INFO: main: Training : batch 52 Loss: 0.002765172113020304]
[2024-04-17 13:06:42,206: INFO: main: Training : batch 53 Loss: 0.008857222211632251]
[2024-04-17 13:06:42,855: INFO: main: Training : batch 54 Loss: 0.0050744762356836485]
[2024-04-17 13:06:43,505: INFO: main: Training : batch 55 Loss: 0.018150083551905595]
[2024-04-17 13:06:44,152: INFO: main: Training : batch 56 Loss: 0.004723047013018455]
[2024-04-17 13:06:44,802: INFO: main: Training : batch 57 Loss: 0.0026363304749670883]
[2024-04-17 13:06:45,448: INFO: main: Training : batch 58 Loss: 0.01274470139640738]
[2024-04-17 13:06:46,099: INFO: main: Training : batch 59 Loss: 0.004213495734006553]
[2024-04-17 13:06:46,744: INFO: main: Training : batch 60 Loss: 0.0013653788587747761]
[2024-04-17 13:06:47,393: INFO: main: Training : batch 61 Loss: 0.00030906164108807636]
[2024-04-17 13:06:48,042: INFO: main: Training : batch 62 Loss: 0.014261389966391232]
[2024-04-17 13:06:48,688: INFO: main: Training : batch 63 Loss: 0.004472251084108152]
[2024-04-17 13:06:49,338: INFO: main: Training : batch 64 Loss: 0.010000718341765365]
[2024-04-17 13:06:49,987: INFO: main: Training : batch 65 Loss: 0.00510818680672774]
[2024-04-17 13:06:50,631: INFO: main: Training : batch 66 Loss: 0.002840711905743868]
[2024-04-17 13:06:51,277: INFO: main: Training : batch 67 Loss: 0.0069056605996039196]
[2024-04-17 13:06:51,925: INFO: main: Training : batch 68 Loss: 0.0055700689739012895]
[2024-04-17 13:06:52,577: INFO: main: Training : batch 69 Loss: 0.004574148721517564]
[2024-04-17 13:06:53,224: INFO: main: Training : batch 70 Loss: 0.0068742589560874525]
[2024-04-17 13:06:53,870: INFO: main: Training : batch 71 Loss: 0.0041990270068187376]
[2024-04-17 13:06:54,518: INFO: main: Training : batch 72 Loss: 0.0013985357024345428]
[2024-04-17 13:06:55,156: INFO: main: Training : batch 73 Loss: 0.006787995991147842]
[2024-04-17 13:06:55,792: INFO: main: Training : batch 74 Loss: 0.005689616788431913]
[2024-04-17 13:06:56,430: INFO: main: Training : batch 75 Loss: 0.009535832033820774]
[2024-04-17 13:06:57,066: INFO: main: Training : batch 76 Loss: 0.004512389983810928]
[2024-04-17 13:06:57,697: INFO: main: Training : batch 77 Loss: 0.007589315351704719]
[2024-04-17 13:06:58,333: INFO: main: Training : batch 78 Loss: 0.0016120459900617935]
[2024-04-17 13:06:58,963: INFO: main: Training : batch 79 Loss: 0.010348338772246782]
[2024-04-17 13:06:59,593: INFO: main: Training : batch 80 Loss: 0.019831744456669843]
[2024-04-17 13:07:00,224: INFO: main: Training : batch 81 Loss: 0.00015961503371773278]
[2024-04-17 13:07:00,852: INFO: main: Training : batch 82 Loss: 0.009737527455835842]
[2024-04-17 13:07:01,489: INFO: main: Training : batch 83 Loss: 0.006035265559007351]
[2024-04-17 13:07:02,125: INFO: main: Training : batch 84 Loss: 0.00911122573780039]
[2024-04-17 13:07:02,756: INFO: main: Training : batch 85 Loss: 0.003514952199914311]
[2024-04-17 13:07:03,388: INFO: main: Training : batch 86 Loss: 0.0036044315359950395]
[2024-04-17 13:07:04,018: INFO: main: Training : batch 87 Loss: 0.00321100548778513]
[2024-04-17 13:07:04,652: INFO: main: Training : batch 88 Loss: 0.00013825447155598592]
[2024-04-17 13:07:05,292: INFO: main: Training : batch 89 Loss: 0.0006758732642505186]
[2024-04-17 13:07:05,936: INFO: main: Training : batch 90 Loss: 0.007400240929945693]
[2024-04-17 13:07:06,574: INFO: main: Training : batch 91 Loss: 0.01628551226147435]
[2024-04-17 13:07:07,214: INFO: main: Training : batch 92 Loss: 0.003957739184245888]
[2024-04-17 13:07:07,847: INFO: main: Training : batch 93 Loss: 0.0006961937987701579]
[2024-04-17 13:07:08,489: INFO: main: Training : batch 94 Loss: 0.014682475273309917]
[2024-04-17 13:07:09,122: INFO: main: Training : batch 95 Loss: 0.012606208325540592]
[2024-04-17 13:07:09,748: INFO: main: Training : batch 96 Loss: 0.008027876816976101]
[2024-04-17 13:07:10,381: INFO: main: Training : batch 97 Loss: 0.0009584520192902347]
[2024-04-17 13:07:11,007: INFO: main: Training : batch 98 Loss: 0.002673155130212694]
[2024-04-17 13:07:11,631: INFO: main: Training : batch 99 Loss: 0.0052036756219503546]
[2024-04-17 13:07:12,258: INFO: main: Training : batch 100 Loss: 0.00827830667564468]
[2024-04-17 13:07:12,886: INFO: main: Training : batch 101 Loss: 0.004995286951784884]
[2024-04-17 13:07:13,511: INFO: main: Training : batch 102 Loss: 0.00044993883926003315]
[2024-04-17 13:07:14,135: INFO: main: Training : batch 103 Loss: 0.02382140880796425]
[2024-04-17 13:07:14,759: INFO: main: Training : batch 104 Loss: 0.007193146862204897]
[2024-04-17 13:07:15,383: INFO: main: Training : batch 105 Loss: 0.005331371807076006]
[2024-04-17 13:07:16,010: INFO: main: Training : batch 106 Loss: 0.004552144597701811]
[2024-04-17 13:07:16,631: INFO: main: Training : batch 107 Loss: 0.022840956114264056]
[2024-04-17 13:07:17,256: INFO: main: Training : batch 108 Loss: 0.0017291505827393048]
[2024-04-17 13:07:17,881: INFO: main: Training : batch 109 Loss: 0.029150297074038954]
[2024-04-17 13:07:18,503: INFO: main: Training : batch 110 Loss: 0.003918722363903582]
[2024-04-17 13:07:19,140: INFO: main: Training : batch 111 Loss: 0.004011207671107765]
[2024-04-17 13:07:19,772: INFO: main: Training : batch 112 Loss: 0.0015204036358607302]
[2024-04-17 13:07:20,404: INFO: main: Training : batch 113 Loss: 0.01619003477863208]
[2024-04-17 13:07:21,034: INFO: main: Training : batch 114 Loss: 0.036752326912111125]
[2024-04-17 13:07:21,667: INFO: main: Training : batch 115 Loss: 0.005464252548370371]
[2024-04-17 13:07:22,294: INFO: main: Training : batch 116 Loss: 0.0025633485502685794]
[2024-04-17 13:07:22,915: INFO: main: Training : batch 117 Loss: 0.0022826651600884457]
[2024-04-17 13:07:23,542: INFO: main: Training : batch 118 Loss: 0.0008375337834350166]
[2024-04-17 13:07:24,166: INFO: main: Training : batch 119 Loss: 0.0017090614788594926]
[2024-04-17 13:07:24,794: INFO: main: Training : batch 120 Loss: 0.010073333399539497]
[2024-04-17 13:07:25,418: INFO: main: Training : batch 121 Loss: 0.018325681442641448]
[2024-04-17 13:07:26,045: INFO: main: Training : batch 122 Loss: 0.003543568414848291]
[2024-04-17 13:07:26,672: INFO: main: Training : batch 123 Loss: 0.006159499167062952]
[2024-04-17 13:07:27,294: INFO: main: Training : batch 124 Loss: 0.0012493230712016614]
[2024-04-17 13:07:27,920: INFO: main: Training : batch 125 Loss: 0.005273939185292217]
[2024-04-17 13:07:28,546: INFO: main: Training : batch 126 Loss: 0.005826938901913902]
[2024-04-17 13:07:29,168: INFO: main: Training : batch 127 Loss: 0.023104366614281287]
[2024-04-17 13:07:29,787: INFO: main: Training : batch 128 Loss: 0.0003710348499555789]
[2024-04-17 13:07:30,409: INFO: main: Training : batch 129 Loss: 0.009542106097461244]
[2024-04-17 13:07:31,037: INFO: main: Training : batch 130 Loss: 0.0037578471188117898]
[2024-04-17 13:07:31,665: INFO: main: Training : batch 131 Loss: 0.005944564891405141]
[2024-04-17 13:07:32,299: INFO: main: Training : batch 132 Loss: 0.0026114802606364623]
[2024-04-17 13:07:32,925: INFO: main: Training : batch 133 Loss: 0.0038181917287516296]
[2024-04-17 13:07:33,557: INFO: main: Training : batch 134 Loss: 0.0030775437762269693]
[2024-04-17 13:07:34,185: INFO: main: Training : batch 135 Loss: 0.01423041656255347]
[2024-04-17 13:07:34,816: INFO: main: Training : batch 136 Loss: 0.01049360684918975]
[2024-04-17 13:07:35,446: INFO: main: Training : batch 137 Loss: 0.004756653680043671]
[2024-04-17 13:07:36,072: INFO: main: Training : batch 138 Loss: 0.0008592159125378676]
[2024-04-17 13:07:36,706: INFO: main: Training : batch 139 Loss: 0.00798061225231991]
[2024-04-17 13:07:37,329: INFO: main: Training : batch 140 Loss: 0.006417248948326984]
[2024-04-17 13:07:37,950: INFO: main: Training : batch 141 Loss: 0.006107932543732066]
[2024-04-17 13:07:38,579: INFO: main: Training : batch 142 Loss: 0.0061224280659740804]
[2024-04-17 13:07:39,215: INFO: main: Training : batch 143 Loss: 0.004354472323502011]
[2024-04-17 13:07:39,841: INFO: main: Training : batch 144 Loss: 0.007034580024348781]
[2024-04-17 13:07:40,469: INFO: main: Training : batch 145 Loss: 0.00032617766647112127]
[2024-04-17 13:07:41,095: INFO: main: Training : batch 146 Loss: 0.0030826611151762258]
[2024-04-17 13:07:41,725: INFO: main: Training : batch 147 Loss: 0.0017215543240107144]
[2024-04-17 13:07:42,353: INFO: main: Training : batch 148 Loss: 0.0013179482246063986]
[2024-04-17 13:07:42,980: INFO: main: Training : batch 149 Loss: 0.007744185671402262]
[2024-04-17 13:07:43,611: INFO: main: Training : batch 150 Loss: 0.0015251112633188728]
[2024-04-17 13:07:44,238: INFO: main: Training : batch 151 Loss: 0.007347600771693868]
[2024-04-17 13:07:44,868: INFO: main: Training : batch 152 Loss: 0.0010443127520515585]
[2024-04-17 13:07:45,498: INFO: main: Training : batch 153 Loss: 0.0053069828982784135]
[2024-04-17 13:07:46,132: INFO: main: Training : batch 154 Loss: 0.004102719867746361]
[2024-04-17 13:07:46,763: INFO: main: Training : batch 155 Loss: 0.0018159990534799579]
[2024-04-17 13:07:47,403: INFO: main: Training : batch 156 Loss: 0.012036095581207402]
[2024-04-17 13:07:48,041: INFO: main: Training : batch 157 Loss: 0.009447436309619011]
[2024-04-17 13:07:48,684: INFO: main: Training : batch 158 Loss: 0.006131970777599996]
[2024-04-17 13:07:49,321: INFO: main: Training : batch 159 Loss: 0.0009490712692973053]
[2024-04-17 13:07:49,948: INFO: main: Training : batch 160 Loss: 0.006448119981166498]
[2024-04-17 13:07:50,583: INFO: main: Training : batch 161 Loss: 0.009405544832522565]
[2024-04-17 13:07:51,216: INFO: main: Training : batch 162 Loss: 0.01686880352458863]
[2024-04-17 13:07:51,852: INFO: main: Training : batch 163 Loss: 0.002724799082930404]
[2024-04-17 13:07:52,483: INFO: main: Training : batch 164 Loss: 0.009692463050782155]
[2024-04-17 13:07:53,116: INFO: main: Training : batch 165 Loss: 0.00550475527670144]
[2024-04-17 13:07:53,746: INFO: main: Training : batch 166 Loss: 0.010060498294902988]
[2024-04-17 13:07:54,383: INFO: main: Training : batch 167 Loss: 0.00454044174512145]
[2024-04-17 13:07:55,014: INFO: main: Training : batch 168 Loss: 0.002543256650023612]
[2024-04-17 13:07:55,648: INFO: main: Training : batch 169 Loss: 0.002278657680063688]
[2024-04-17 13:07:56,282: INFO: main: Training : batch 170 Loss: 0.009696525721861573]
[2024-04-17 13:07:56,922: INFO: main: Training : batch 171 Loss: 0.007341370428874539]
[2024-04-17 13:07:57,555: INFO: main: Training : batch 172 Loss: 0.0017147207554561881]
[2024-04-17 13:07:58,192: INFO: main: Training : batch 173 Loss: 0.005830428404336369]
[2024-04-17 13:07:58,828: INFO: main: Training : batch 174 Loss: 0.004862286426313504]
[2024-04-17 13:07:59,466: INFO: main: Training : batch 175 Loss: 0.011458315847971318]
[2024-04-17 13:08:00,108: INFO: main: Training : batch 176 Loss: 0.008309454581472791]
[2024-04-17 13:08:00,755: INFO: main: Training : batch 177 Loss: 0.005905967676725865]
[2024-04-17 13:08:01,406: INFO: main: Training : batch 178 Loss: 0.003539572371806607]
[2024-04-17 13:08:02,049: INFO: main: Training : batch 179 Loss: 0.003084135072547841]
[2024-04-17 13:08:02,689: INFO: main: Training : batch 180 Loss: 0.0029949969327764166]
[2024-04-17 13:08:03,323: INFO: main: Training : batch 181 Loss: 0.004187562302538172]
[2024-04-17 13:08:03,957: INFO: main: Training : batch 182 Loss: 0.006469709680810931]
[2024-04-17 13:08:04,593: INFO: main: Training : batch 183 Loss: 0.002010198404284123]
[2024-04-17 13:08:05,231: INFO: main: Training : batch 184 Loss: 0.0029685772993580804]
[2024-04-17 13:08:05,868: INFO: main: Training : batch 185 Loss: 0.0019933987980116385]
[2024-04-17 13:08:06,504: INFO: main: Training : batch 186 Loss: 0.005703864150994166]
[2024-04-17 13:08:07,144: INFO: main: Training : batch 187 Loss: 0.0008523599262623273]
[2024-04-17 13:08:07,777: INFO: main: Training : batch 188 Loss: 0.0014850571238142487]
[2024-04-17 13:08:08,414: INFO: main: Training : batch 189 Loss: 0.0008263621514376091]
[2024-04-17 13:08:09,050: INFO: main: Training : batch 190 Loss: 0.004818944707476784]
[2024-04-17 13:08:09,683: INFO: main: Training : batch 191 Loss: 0.003026276440075545]
[2024-04-17 13:08:10,319: INFO: main: Training : batch 192 Loss: 0.0033744403085556635]
[2024-04-17 13:08:10,953: INFO: main: Training : batch 193 Loss: 0.0030810574311982758]
[2024-04-17 13:08:11,591: INFO: main: Training : batch 194 Loss: 0.021151275591163175]
[2024-04-17 13:08:12,227: INFO: main: Training : batch 195 Loss: 0.010867517397802147]
[2024-04-17 13:08:12,865: INFO: main: Training : batch 196 Loss: 0.003711495154714811]
[2024-04-17 13:08:13,506: INFO: main: Training : batch 197 Loss: 0.007809707725766373]
[2024-04-17 13:08:14,156: INFO: main: Training : batch 198 Loss: 0.005160475435046288]
[2024-04-17 13:08:14,803: INFO: main: Training : batch 199 Loss: 0.005045523911093244]
[2024-04-17 13:08:15,444: INFO: main: Training : batch 200 Loss: 0.0030940443821588126]
[2024-04-17 13:08:16,084: INFO: main: Training : batch 201 Loss: 0.008443051595171504]
[2024-04-17 13:08:16,726: INFO: main: Training : batch 202 Loss: 0.012738364425157874]
[2024-04-17 13:08:17,358: INFO: main: Training : batch 203 Loss: 0.0022877723705027773]
[2024-04-17 13:08:17,995: INFO: main: Training : batch 204 Loss: 0.0067785448427163586]
[2024-04-17 13:08:18,627: INFO: main: Training : batch 205 Loss: 0.0032672189706316546]
[2024-04-17 13:08:19,258: INFO: main: Training : batch 206 Loss: 0.0021006967798575886]
[2024-04-17 13:08:19,891: INFO: main: Training : batch 207 Loss: 0.0005079300582936951]
[2024-04-17 13:08:20,527: INFO: main: Training : batch 208 Loss: 0.005163273481809226]
[2024-04-17 13:08:21,166: INFO: main: Training : batch 209 Loss: 0.000748015297973612]
[2024-04-17 13:08:21,803: INFO: main: Training : batch 210 Loss: 0.002319732730847543]
[2024-04-17 13:08:22,437: INFO: main: Training : batch 211 Loss: 0.002390882936562465]
[2024-04-17 13:08:23,072: INFO: main: Training : batch 212 Loss: 0.0022964181883698056]
[2024-04-17 13:08:23,706: INFO: main: Training : batch 213 Loss: 0.004004365090080268]
[2024-04-17 13:08:24,337: INFO: main: Training : batch 214 Loss: 0.0048728641581994815]
[2024-04-17 13:08:24,972: INFO: main: Training : batch 215 Loss: 0.01747167557355275]
[2024-04-17 13:08:25,606: INFO: main: Training : batch 216 Loss: 0.008337839850796313]
[2024-04-17 13:08:26,246: INFO: main: Training : batch 217 Loss: 0.013885894726998464]
[2024-04-17 13:08:26,886: INFO: main: Training : batch 218 Loss: 0.005229078150068176]
[2024-04-17 13:08:27,526: INFO: main: Training : batch 219 Loss: 0.0021951352740570236]
[2024-04-17 13:08:28,173: INFO: main: Training : batch 220 Loss: 0.0014479808367696557]
[2024-04-17 13:08:28,826: INFO: main: Training : batch 221 Loss: 0.0009013572480367353]
[2024-04-17 13:08:29,471: INFO: main: Training : batch 222 Loss: 6.582356469087922e-05]
[2024-04-17 13:08:30,105: INFO: main: Training : batch 223 Loss: 0.0038666921879907242]
[2024-04-17 13:08:30,735: INFO: main: Training : batch 224 Loss: 0.0052764091172468565]
[2024-04-17 13:08:31,368: INFO: main: Training : batch 225 Loss: 0.015747092475882803]
[2024-04-17 13:08:32,002: INFO: main: Training : batch 226 Loss: 0.001933416606674937]
[2024-04-17 13:08:32,630: INFO: main: Training : batch 227 Loss: 0.0038948558581709826]
[2024-04-17 13:08:33,262: INFO: main: Training : batch 228 Loss: 0.006286085512351969]
[2024-04-17 13:08:33,893: INFO: main: Training : batch 229 Loss: 0.018408637538002957]
[2024-04-17 13:08:34,527: INFO: main: Training : batch 230 Loss: 0.01620899531475975]
[2024-04-17 13:08:35,156: INFO: main: Training : batch 231 Loss: 0.006079675396051271]
[2024-04-17 13:08:35,792: INFO: main: Training : batch 232 Loss: 0.0035794963153657753]
[2024-04-17 13:08:36,423: INFO: main: Training : batch 233 Loss: 0.003402492503605021]
[2024-04-17 13:08:37,055: INFO: main: Training : batch 234 Loss: 0.002382504122054083]
[2024-04-17 13:08:37,686: INFO: main: Training : batch 235 Loss: 0.009166929772613583]
[2024-04-17 13:08:38,316: INFO: main: Training : batch 236 Loss: 0.004890083904688733]
[2024-04-17 13:08:38,946: INFO: main: Training : batch 237 Loss: 0.007561324519492759]
[2024-04-17 13:08:39,580: INFO: main: Training : batch 238 Loss: 0.017127333710742428]
[2024-04-17 13:08:40,217: INFO: main: Training : batch 239 Loss: 0.005899714695628888]
[2024-04-17 13:08:40,855: INFO: main: Training : batch 240 Loss: 0.0019721980768204303]
[2024-04-17 13:08:41,488: INFO: main: Training : batch 241 Loss: 0.004468630828066923]
[2024-04-17 13:08:42,130: INFO: main: Training : batch 242 Loss: 0.0009913563167718489]
[2024-04-17 13:08:42,767: INFO: main: Training : batch 243 Loss: 0.006355374335589909]
[2024-04-17 13:08:43,397: INFO: main: Training : batch 244 Loss: 0.002055069605560539]
[2024-04-17 13:08:44,026: INFO: main: Training : batch 245 Loss: 0.007348451608553328]
[2024-04-17 13:08:44,654: INFO: main: Training : batch 246 Loss: 0.006545060463271973]
[2024-04-17 13:08:45,286: INFO: main: Training : batch 247 Loss: 0.006617603914396261]
[2024-04-17 13:08:45,921: INFO: main: Training : batch 248 Loss: 0.003668870356859346]
[2024-04-17 13:08:46,544: INFO: main: Training : batch 249 Loss: 0.026394014129708557]
[2024-04-17 13:08:47,177: INFO: main: Training : batch 250 Loss: 0.01443375809572104]
[2024-04-17 13:08:47,806: INFO: main: Training : batch 251 Loss: 0.0019769069951119794]
[2024-04-17 13:08:48,436: INFO: main: Training : batch 252 Loss: 0.01293237991433981]
[2024-04-17 13:08:49,062: INFO: main: Training : batch 253 Loss: 0.02472048435811183]
[2024-04-17 13:08:49,693: INFO: main: Training : batch 254 Loss: 0.014237451431262644]
[2024-04-17 13:08:50,320: INFO: main: Training : batch 255 Loss: 0.00874872961372388]
[2024-04-17 13:08:50,945: INFO: main: Training : batch 256 Loss: 0.00406245690951786]
[2024-04-17 13:08:51,571: INFO: main: Training : batch 257 Loss: 0.00717298252278955]
[2024-04-17 13:08:52,201: INFO: main: Training : batch 258 Loss: 0.0009721064602066347]
[2024-04-17 13:08:52,834: INFO: main: Training : batch 259 Loss: 0.008256186916803883]
[2024-04-17 13:08:53,470: INFO: main: Training : batch 260 Loss: 0.008433890272893568]
[2024-04-17 13:08:54,111: INFO: main: Training : batch 261 Loss: 0.0115437580610183]
[2024-04-17 13:08:54,746: INFO: main: Training : batch 262 Loss: 0.0023209579157356997]
[2024-04-17 13:08:55,379: INFO: main: Training : batch 263 Loss: 0.001631104237624208]
[2024-04-17 13:08:56,012: INFO: main: Training : batch 264 Loss: 0.012978981136812067]
[2024-04-17 13:08:56,649: INFO: main: Training : batch 265 Loss: 0.002574053587696619]
[2024-04-17 13:08:57,277: INFO: main: Training : batch 266 Loss: 0.0060033051673391334]
[2024-04-17 13:08:57,911: INFO: main: Training : batch 267 Loss: 0.03197433975195964]
[2024-04-17 13:08:58,541: INFO: main: Training : batch 268 Loss: 0.004859132199913012]
[2024-04-17 13:08:59,170: INFO: main: Training : batch 269 Loss: 0.004716277428954286]
[2024-04-17 13:08:59,798: INFO: main: Training : batch 270 Loss: 0.00648232436632541]
[2024-04-17 13:09:00,423: INFO: main: Training : batch 271 Loss: 0.003920022421175304]
[2024-04-17 13:09:01,049: INFO: main: Training : batch 272 Loss: 0.0047042341404407014]
[2024-04-17 13:09:01,676: INFO: main: Training : batch 273 Loss: 0.001168013247717379]
[2024-04-17 13:09:02,307: INFO: main: Training : batch 274 Loss: 0.0012862654397010145]
[2024-04-17 13:09:02,937: INFO: main: Training : batch 275 Loss: 0.004444462027489256]
[2024-04-17 13:09:03,568: INFO: main: Training : batch 276 Loss: 0.00393624172347335]
[2024-04-17 13:09:04,194: INFO: main: Training : batch 277 Loss: 0.011484639978643347]
[2024-04-17 13:09:04,821: INFO: main: Training : batch 278 Loss: 0.00803695144682949]
[2024-04-17 13:09:05,447: INFO: main: Training : batch 279 Loss: 0.006433854844512571]
[2024-04-17 13:09:06,076: INFO: main: Training : batch 280 Loss: 0.004429054229415407]
[2024-04-17 13:09:06,709: INFO: main: Training : batch 281 Loss: 0.0006485204940872657]
[2024-04-17 13:09:07,344: INFO: main: Training : batch 282 Loss: 0.0028301601881214416]
[2024-04-17 13:09:07,980: INFO: main: Training : batch 283 Loss: 0.010215149928587316]
[2024-04-17 13:09:08,617: INFO: main: Training : batch 284 Loss: 0.00587059023835393]
[2024-04-17 13:09:09,259: INFO: main: Training : batch 285 Loss: 0.005896662938136599]
[2024-04-17 13:09:09,896: INFO: main: Training : batch 286 Loss: 0.004188409573044307]
[2024-04-17 13:09:10,526: INFO: main: Training : batch 287 Loss: 0.003005972314300326]
[2024-04-17 13:09:11,154: INFO: main: Training : batch 288 Loss: 0.008029706337986364]
[2024-04-17 13:09:11,787: INFO: main: Training : batch 289 Loss: 0.0008806604548021629]
[2024-04-17 13:09:12,420: INFO: main: Training : batch 290 Loss: 0.007620596919565728]
[2024-04-17 13:09:13,050: INFO: main: Training : batch 291 Loss: 0.0155469708723565]
[2024-04-17 13:09:13,676: INFO: main: Training : batch 292 Loss: 0.0049017745046454255]
[2024-04-17 13:09:14,318: INFO: main: Training : batch 293 Loss: 0.0034416773432294533]
[2024-04-17 13:09:14,950: INFO: main: Training : batch 294 Loss: 0.005375082349190666]
[2024-04-17 13:09:15,580: INFO: main: Training : batch 295 Loss: 0.008857224529014534]
[2024-04-17 13:09:16,213: INFO: main: Training : batch 296 Loss: 0.01620278276722699]
[2024-04-17 13:09:16,844: INFO: main: Training : batch 297 Loss: 0.011170591468259898]
[2024-04-17 13:09:17,477: INFO: main: Training : batch 298 Loss: 0.008837149801688182]
[2024-04-17 13:09:18,109: INFO: main: Training : batch 299 Loss: 0.019740215035685862]
[2024-04-17 13:09:18,742: INFO: main: Training : batch 300 Loss: 0.0072425521561665825]
[2024-04-17 13:09:19,373: INFO: main: Training : batch 301 Loss: 0.007187907304566169]
[2024-04-17 13:09:20,004: INFO: main: Training : batch 302 Loss: 0.013906524401741824]
[2024-04-17 13:09:20,645: INFO: main: Training : batch 303 Loss: 0.0026946442197866263]
[2024-04-17 13:09:21,279: INFO: main: Training : batch 304 Loss: 0.004774688356694841]
[2024-04-17 13:09:21,919: INFO: main: Training : batch 305 Loss: 0.009968067760447402]
[2024-04-17 13:09:22,554: INFO: main: Training : batch 306 Loss: 0.018324841721036992]
[2024-04-17 13:09:23,196: INFO: main: Training : batch 307 Loss: 0.004632775892257858]
[2024-04-17 13:09:23,833: INFO: main: Training : batch 308 Loss: 0.018784442249405318]
[2024-04-17 13:09:24,465: INFO: main: Training : batch 309 Loss: 0.006235502981000921]
[2024-04-17 13:09:25,096: INFO: main: Training : batch 310 Loss: 0.0008605740247500652]
[2024-04-17 13:09:25,728: INFO: main: Training : batch 311 Loss: 0.006656200919014546]
[2024-04-17 13:09:26,359: INFO: main: Training : batch 312 Loss: 0.0062392502079263315]
[2024-04-17 13:09:26,993: INFO: main: Training : batch 313 Loss: 0.0025829315742680083]
[2024-04-17 13:09:27,626: INFO: main: Training : batch 314 Loss: 0.001422526580236054]
[2024-04-17 13:09:28,261: INFO: main: Training : batch 315 Loss: 0.0019089372889937717]
[2024-04-17 13:09:28,892: INFO: main: Training : batch 316 Loss: 0.009615701028622761]
[2024-04-17 13:09:29,524: INFO: main: Training : batch 317 Loss: 0.0018002018387217147]
[2024-04-17 13:09:30,161: INFO: main: Training : batch 318 Loss: 0.0012662927660681146]
[2024-04-17 13:09:30,790: INFO: main: Training : batch 319 Loss: 0.012629257284749347]
[2024-04-17 13:09:31,424: INFO: main: Training : batch 320 Loss: 0.010839773061261842]
[2024-04-17 13:09:32,057: INFO: main: Training : batch 321 Loss: 0.013048377137022981]
[2024-04-17 13:09:32,688: INFO: main: Training : batch 322 Loss: 0.0021535011389659697]
[2024-04-17 13:09:33,321: INFO: main: Training : batch 323 Loss: 0.00046988380210178727]
[2024-04-17 13:09:33,962: INFO: main: Training : batch 324 Loss: 0.003077748356984563]
[2024-04-17 13:09:34,600: INFO: main: Training : batch 325 Loss: 0.011967355447663282]
[2024-04-17 13:09:35,234: INFO: main: Training : batch 326 Loss: 0.007171143356241439]
[2024-04-17 13:09:35,876: INFO: main: Training : batch 327 Loss: 0.011269449044352854]
[2024-04-17 13:09:36,511: INFO: main: Training : batch 328 Loss: 0.010117638405074545]
[2024-04-17 13:09:37,143: INFO: main: Training : batch 329 Loss: 0.014094700099593935]
[2024-04-17 13:09:37,777: INFO: main: Training : batch 330 Loss: 0.0024163097124816377]
[2024-04-17 13:09:38,413: INFO: main: Training : batch 331 Loss: 0.012026217449303482]
[2024-04-17 13:09:39,047: INFO: main: Training : batch 332 Loss: 0.009733367958608036]
[2024-04-17 13:09:39,676: INFO: main: Training : batch 333 Loss: 0.006384296795497072]
[2024-04-17 13:09:40,309: INFO: main: Training : batch 334 Loss: 0.01836888605446031]
[2024-04-17 13:09:40,937: INFO: main: Training : batch 335 Loss: 0.007258036797769379]
[2024-04-17 13:09:41,572: INFO: main: Training : batch 336 Loss: 0.009991229859592814]
[2024-04-17 13:09:42,204: INFO: main: Training : batch 337 Loss: 0.005323083459066456]
[2024-04-17 13:09:42,836: INFO: main: Training : batch 338 Loss: 0.024844261835243443]
[2024-04-17 13:09:43,468: INFO: main: Training : batch 339 Loss: 0.0040089925403420995]
[2024-04-17 13:09:44,098: INFO: main: Training : batch 340 Loss: 0.010901565300119265]
[2024-04-17 13:09:44,732: INFO: main: Training : batch 341 Loss: 0.004928950303559735]
[2024-04-17 13:09:45,363: INFO: main: Training : batch 342 Loss: 0.008059571862118023]
[2024-04-17 13:09:45,996: INFO: main: Training : batch 343 Loss: 0.0065681145660921765]
[2024-04-17 13:09:46,624: INFO: main: Training : batch 344 Loss: 0.009772209464210358]
[2024-04-17 13:09:47,263: INFO: main: Training : batch 345 Loss: 0.005752817815477063]
[2024-04-17 13:09:47,899: INFO: main: Training : batch 346 Loss: 0.0026033187397523083]
[2024-04-17 13:09:48,536: INFO: main: Training : batch 347 Loss: 0.004505767938063533]
[2024-04-17 13:09:49,171: INFO: main: Training : batch 348 Loss: 0.00786864996781343]
[2024-04-17 13:09:49,808: INFO: main: Training : batch 349 Loss: 0.006052229994923306]
[2024-04-17 13:09:50,447: INFO: main: Training : batch 350 Loss: 0.007156629295779582]
[2024-04-17 13:09:51,082: INFO: main: Training : batch 351 Loss: 0.0017145586551833615]
[2024-04-17 13:09:51,713: INFO: main: Training : batch 352 Loss: 0.001157939693213173]
[2024-04-17 13:09:52,345: INFO: main: Training : batch 353 Loss: 0.008039023929068906]
[2024-04-17 13:09:52,983: INFO: main: Training : batch 354 Loss: 0.005658673145137764]
[2024-04-17 13:09:53,618: INFO: main: Training : batch 355 Loss: 0.009980955565949752]
[2024-04-17 13:09:54,252: INFO: main: Training : batch 356 Loss: 0.0011113933184674976]
[2024-04-17 13:09:54,886: INFO: main: Training : batch 357 Loss: 0.0013192298061726914]
[2024-04-17 13:09:55,512: INFO: main: Training : batch 358 Loss: 0.0066852703016373255]
[2024-04-17 13:09:56,144: INFO: main: Training : batch 359 Loss: 0.0015642223230945599]
[2024-04-17 13:09:56,776: INFO: main: Training : batch 360 Loss: 0.0020757204476432277]
[2024-04-17 13:09:57,411: INFO: main: Training : batch 361 Loss: 0.007881247767874553]
[2024-04-17 13:09:58,047: INFO: main: Training : batch 362 Loss: 0.005001679493936103]
[2024-04-17 13:09:58,681: INFO: main: Training : batch 363 Loss: 0.003276171605488893]
[2024-04-17 13:09:59,317: INFO: main: Training : batch 364 Loss: 0.015796854802193314]
[2024-04-17 13:09:59,953: INFO: main: Training : batch 365 Loss: 0.006600322293200202]
[2024-04-17 13:10:00,584: INFO: main: Training : batch 366 Loss: 0.009312175295262713]
[2024-04-17 13:10:01,221: INFO: main: Training : batch 367 Loss: 0.0016467136609081706]
[2024-04-17 13:10:01,869: INFO: main: Training : batch 368 Loss: 0.006991105560153089]
[2024-04-17 13:10:02,511: INFO: main: Training : batch 369 Loss: 0.010118091703424542]
[2024-04-17 13:10:03,150: INFO: main: Training : batch 370 Loss: 0.01081856411586205]
[2024-04-17 13:10:03,797: INFO: main: Training : batch 371 Loss: 0.015089025457948558]
[2024-04-17 13:10:04,427: INFO: main: Training : batch 372 Loss: 0.013006825182664364]
[2024-04-17 13:10:05,059: INFO: main: Training : batch 373 Loss: 0.002078686880438693]
[2024-04-17 13:10:05,694: INFO: main: Training : batch 374 Loss: 0.00812726068629811]
[2024-04-17 13:10:06,327: INFO: main: Training : batch 375 Loss: 0.004128182216463642]
[2024-04-17 13:10:06,957: INFO: main: Training : batch 376 Loss: 0.0012796985211345512]
[2024-04-17 13:10:07,588: INFO: main: Training : batch 377 Loss: 0.005224154872393403]
[2024-04-17 13:10:08,218: INFO: main: Training : batch 378 Loss: 0.0012750489894816294]
[2024-04-17 13:10:08,851: INFO: main: Training : batch 379 Loss: 0.00812788670574106]
[2024-04-17 13:10:09,483: INFO: main: Training : batch 380 Loss: 0.007145045688961087]
[2024-04-17 13:10:10,116: INFO: main: Training : batch 381 Loss: 0.010308547352509015]
[2024-04-17 13:10:10,751: INFO: main: Training : batch 382 Loss: 0.00946846627155135]
[2024-04-17 13:10:11,377: INFO: main: Training : batch 383 Loss: 0.0014265141441288816]
[2024-04-17 13:10:12,010: INFO: main: Training : batch 384 Loss: 0.004681656778685163]
[2024-04-17 13:10:12,645: INFO: main: Training : batch 385 Loss: 0.0016357292101519428]
[2024-04-17 13:10:13,280: INFO: main: Training : batch 386 Loss: 0.0033131863977078897]
[2024-04-17 13:10:13,913: INFO: main: Training : batch 387 Loss: 0.004165687649272367]
[2024-04-17 13:10:14,551: INFO: main: Training : batch 388 Loss: 0.003490602757521543]
[2024-04-17 13:10:15,191: INFO: main: Training : batch 389 Loss: 0.014128849906497586]
[2024-04-17 13:10:15,836: INFO: main: Training : batch 390 Loss: 0.005785258379123686]
[2024-04-17 13:10:16,476: INFO: main: Training : batch 391 Loss: 0.0007314431974281657]
[2024-04-17 13:10:17,117: INFO: main: Training : batch 392 Loss: 0.01799797062161073]
[2024-04-17 13:10:17,748: INFO: main: Training : batch 393 Loss: 0.002232130828690724]
[2024-04-17 13:10:18,382: INFO: main: Training : batch 394 Loss: 0.006769780165122094]
[2024-04-17 13:10:19,011: INFO: main: Training : batch 395 Loss: 0.0012913880708941305]
[2024-04-17 13:10:19,641: INFO: main: Training : batch 396 Loss: 0.012536769339874678]
[2024-04-17 13:10:20,279: INFO: main: Training : batch 397 Loss: 0.002884200086140563]
[2024-04-17 13:10:20,909: INFO: main: Training : batch 398 Loss: 0.022264560926294093]
[2024-04-17 13:10:21,542: INFO: main: Training : batch 399 Loss: 0.0006255947466277616]
[2024-04-17 13:10:22,171: INFO: main: Training : batch 400 Loss: 0.004673267833518438]
[2024-04-17 13:10:22,807: INFO: main: Training : batch 401 Loss: 0.002834166236580978]
[2024-04-17 13:10:23,446: INFO: main: Training : batch 402 Loss: 0.006012335336820947]
[2024-04-17 13:10:24,080: INFO: main: Training : batch 403 Loss: 0.0013695943587537856]
[2024-04-17 13:10:24,719: INFO: main: Training : batch 404 Loss: 0.001924744261012322]
[2024-04-17 13:10:25,353: INFO: main: Training : batch 405 Loss: 0.0048268655708957565]
[2024-04-17 13:10:25,987: INFO: main: Training : batch 406 Loss: 0.006322596989346602]
[2024-04-17 13:10:26,623: INFO: main: Training : batch 407 Loss: 0.007505736121320632]
[2024-04-17 13:10:27,254: INFO: main: Training : batch 408 Loss: 0.008631364709330569]
[2024-04-17 13:10:27,899: INFO: main: Training : batch 409 Loss: 0.000791801946993325]
[2024-04-17 13:10:28,545: INFO: main: Training : batch 410 Loss: 0.015161977679815164]
[2024-04-17 13:10:29,188: INFO: main: Training : batch 411 Loss: 0.00915412168633483]
[2024-04-17 13:10:29,825: INFO: main: Training : batch 412 Loss: 0.005570013846878193]
[2024-04-17 13:10:30,467: INFO: main: Training : batch 413 Loss: 0.005245731513003537]
[2024-04-17 13:10:31,105: INFO: main: Training : batch 414 Loss: 0.003348744798955634]
[2024-04-17 13:10:31,734: INFO: main: Training : batch 415 Loss: 0.005231238864190883]
[2024-04-17 13:10:32,369: INFO: main: Training : batch 416 Loss: 0.007730626576015145]
[2024-04-17 13:10:33,000: INFO: main: Training : batch 417 Loss: 0.0021877126976504006]
[2024-04-17 13:10:33,630: INFO: main: Training : batch 418 Loss: 0.0101650802015176]
[2024-04-17 13:10:34,262: INFO: main: Training : batch 419 Loss: 0.000953267267158536]
[2024-04-17 13:10:34,893: INFO: main: Training : batch 420 Loss: 0.0017440078514047435]
[2024-04-17 13:10:35,524: INFO: main: Training : batch 421 Loss: 0.015856348029547076]
[2024-04-17 13:10:36,168: INFO: main: Training : batch 422 Loss: 0.020891887990178347]
[2024-04-17 13:10:36,799: INFO: main: Training : batch 423 Loss: 0.011558112154308586]
[2024-04-17 13:10:37,434: INFO: main: Training : batch 424 Loss: 0.004839018034381524]
[2024-04-17 13:10:38,064: INFO: main: Training : batch 425 Loss: 0.03799509253461269]
[2024-04-17 13:10:38,698: INFO: main: Training : batch 426 Loss: 0.015440598403374993]
[2024-04-17 13:10:39,330: INFO: main: Training : batch 427 Loss: 0.014207786693054676]
[2024-04-17 13:10:39,963: INFO: main: Training : batch 428 Loss: 0.006074783107636611]
[2024-04-17 13:10:40,593: INFO: main: Training : batch 429 Loss: 0.0017138557458275133]
[2024-04-17 13:10:41,233: INFO: main: Training : batch 430 Loss: 0.03127025495630014]
[2024-04-17 13:10:41,872: INFO: main: Training : batch 431 Loss: 0.007567456197398202]
[2024-04-17 13:10:42,514: INFO: main: Training : batch 432 Loss: 0.002682901238149861]
[2024-04-17 13:10:43,156: INFO: main: Training : batch 433 Loss: 0.0021985591475171926]
[2024-04-17 13:10:43,804: INFO: main: Training : batch 434 Loss: 0.004900754165797744]
[2024-04-17 13:10:44,451: INFO: main: Training : batch 435 Loss: 0.006064642610174541]
[2024-04-17 13:10:45,085: INFO: main: Training : batch 436 Loss: 0.009840934860499416]
[2024-04-17 13:10:45,707: INFO: main: Training : batch 437 Loss: 0.002292635487930981]
[2024-04-17 13:10:46,339: INFO: main: Training : batch 438 Loss: 0.010869898764106375]
[2024-04-17 13:10:46,969: INFO: main: Training : batch 439 Loss: 0.005960551105263406]
[2024-04-17 13:10:47,597: INFO: main: Training : batch 440 Loss: 0.002414674014729617]
[2024-04-17 13:10:48,232: INFO: main: Training : batch 441 Loss: 0.003505159126069532]
[2024-04-17 13:10:48,861: INFO: main: Training : batch 442 Loss: 0.018862443687946758]
[2024-04-17 13:10:49,498: INFO: main: Training : batch 443 Loss: 0.004031931438675113]
[2024-04-17 13:10:50,136: INFO: main: Training : batch 444 Loss: 0.004503452905209548]
[2024-04-17 13:10:50,767: INFO: main: Training : batch 445 Loss: 0.0030869183676388657]
[2024-04-17 13:10:51,402: INFO: main: Training : batch 446 Loss: 0.013097752386368034]
[2024-04-17 13:10:52,033: INFO: main: Training : batch 447 Loss: 0.012434308805360296]
[2024-04-17 13:10:52,658: INFO: main: Training : batch 448 Loss: 0.0053420617324894705]
[2024-04-17 13:10:53,289: INFO: main: Training : batch 449 Loss: 0.0010671812690038643]
[2024-04-17 13:10:53,917: INFO: main: Training : batch 450 Loss: 0.013438920787539044]
[2024-04-17 13:10:54,548: INFO: main: Training : batch 451 Loss: 0.008943662123127404]
[2024-04-17 13:10:55,189: INFO: main: Training : batch 452 Loss: 0.0014245381036827136]
[2024-04-17 13:10:55,825: INFO: main: Training : batch 453 Loss: 0.009985927061099171]
[2024-04-17 13:10:56,466: INFO: main: Training : batch 454 Loss: 0.005195766538282969]
[2024-04-17 13:10:57,109: INFO: main: Training : batch 455 Loss: 0.002346003502468928]
[2024-04-17 13:10:57,744: INFO: main: Training : batch 456 Loss: 0.005465576360627899]
[2024-04-17 13:10:58,379: INFO: main: Training : batch 457 Loss: 0.011349370419871945]
[2024-04-17 13:10:59,011: INFO: main: Training : batch 458 Loss: 0.0029662797894357824]
[2024-04-17 13:10:59,637: INFO: main: Training : batch 459 Loss: 0.011732260789510349]
[2024-04-17 13:11:00,271: INFO: main: Training : batch 460 Loss: 0.00591381269390616]
[2024-04-17 13:11:00,902: INFO: main: Training : batch 461 Loss: 0.0010514238485930067]
[2024-04-17 13:11:01,538: INFO: main: Training : batch 462 Loss: 0.0009533704225514186]
[2024-04-17 13:11:02,172: INFO: main: Training : batch 463 Loss: 0.002386874224048888]
[2024-04-17 13:11:02,807: INFO: main: Training : batch 464 Loss: 0.0009822755053400726]
[2024-04-17 13:11:03,444: INFO: main: Training : batch 465 Loss: 0.00842263929337832]
[2024-04-17 13:11:04,073: INFO: main: Training : batch 466 Loss: 0.0017828652633296994]
[2024-04-17 13:11:04,703: INFO: main: Training : batch 467 Loss: 0.007362477779947167]
[2024-04-17 13:11:05,335: INFO: main: Training : batch 468 Loss: 0.002791080021342871]
[2024-04-17 13:11:05,967: INFO: main: Training : batch 469 Loss: 0.004736721840632802]
[2024-04-17 13:11:06,599: INFO: main: Training : batch 470 Loss: 0.004300657282361368]
[2024-04-17 13:11:07,234: INFO: main: Training : batch 471 Loss: 0.0012165152776211384]
[2024-04-17 13:11:07,869: INFO: main: Training : batch 472 Loss: 0.0009227767365666271]
[2024-04-17 13:11:08,506: INFO: main: Training : batch 473 Loss: 0.008067272950441611]
[2024-04-17 13:11:09,140: INFO: main: Training : batch 474 Loss: 0.007990137897572368]
[2024-04-17 13:11:09,779: INFO: main: Training : batch 475 Loss: 0.0056879158112853575]
[2024-04-17 13:11:10,422: INFO: main: Training : batch 476 Loss: 0.013643690836908931]
[2024-04-17 13:11:11,069: INFO: main: Training : batch 477 Loss: 0.00821527635663573]
[2024-04-17 13:11:11,713: INFO: main: Training : batch 478 Loss: 0.019533326236049097]
[2024-04-17 13:11:12,347: INFO: main: Training : batch 479 Loss: 0.005044617011122446]
[2024-04-17 13:11:12,977: INFO: main: Training : batch 480 Loss: 0.00036956551907689754]
[2024-04-17 13:11:13,605: INFO: main: Training : batch 481 Loss: 0.001947096399358934]
[2024-04-17 13:11:14,236: INFO: main: Training : batch 482 Loss: 0.0029338654751058075]
[2024-04-17 13:11:14,865: INFO: main: Training : batch 483 Loss: 0.0033893858661632626]
[2024-04-17 13:11:15,492: INFO: main: Training : batch 484 Loss: 0.003256562052967484]
[2024-04-17 13:11:16,122: INFO: main: Training : batch 485 Loss: 0.008137163879282311]
[2024-04-17 13:11:16,753: INFO: main: Training : batch 486 Loss: 0.01565545914073538]
[2024-04-17 13:11:17,381: INFO: main: Training : batch 487 Loss: 0.004718347153710805]
[2024-04-17 13:11:18,013: INFO: main: Training : batch 488 Loss: 0.011390590842317205]
[2024-04-17 13:11:18,646: INFO: main: Training : batch 489 Loss: 0.0018921621233431844]
[2024-04-17 13:11:19,277: INFO: main: Training : batch 490 Loss: 0.0025506999555063316]
[2024-04-17 13:11:19,907: INFO: main: Training : batch 491 Loss: 0.0002538992575653246]
[2024-04-17 13:11:20,534: INFO: main: Training : batch 492 Loss: 0.0010645547345692447]
[2024-04-17 13:11:21,166: INFO: main: Training : batch 493 Loss: 0.005574667746908796]
[2024-04-17 13:11:21,795: INFO: main: Training : batch 494 Loss: 0.005720337025224484]
[2024-04-17 13:11:22,429: INFO: main: Training : batch 495 Loss: 0.00034193892595807244]
[2024-04-17 13:11:23,069: INFO: main: Training : batch 496 Loss: 0.002811480514152219]
[2024-04-17 13:11:23,704: INFO: main: Training : batch 497 Loss: 0.0037050541227006783]
[2024-04-17 13:11:24,341: INFO: main: Training : batch 498 Loss: 0.002397633432606859]
[2024-04-17 13:11:24,977: INFO: main: Training : batch 499 Loss: 0.0004968541157831705]
[2024-04-17 13:11:25,608: INFO: main: Training : batch 500 Loss: 0.003675542838959827]
[2024-04-17 13:11:26,237: INFO: main: Training : batch 501 Loss: 0.011686697588797848]
[2024-04-17 13:11:26,869: INFO: main: Training : batch 502 Loss: 0.0065626299064117785]
[2024-04-17 13:11:27,501: INFO: main: Training : batch 503 Loss: 0.010626366070933488]
[2024-04-17 13:11:28,132: INFO: main: Training : batch 504 Loss: 0.011634332432848416]
[2024-04-17 13:11:28,767: INFO: main: Training : batch 505 Loss: 0.002151607186058218]
[2024-04-17 13:11:29,397: INFO: main: Training : batch 506 Loss: 0.017555146155182683]
[2024-04-17 13:11:30,024: INFO: main: Training : batch 507 Loss: 0.006328161487470496]
[2024-04-17 13:11:30,651: INFO: main: Training : batch 508 Loss: 0.001502335460608418]
[2024-04-17 13:11:31,284: INFO: main: Training : batch 509 Loss: 0.0007438237004326949]
[2024-04-17 13:11:31,918: INFO: main: Training : batch 510 Loss: 0.005422833823650885]
[2024-04-17 13:11:32,545: INFO: main: Training : batch 511 Loss: 0.011264846485001643]
[2024-04-17 13:11:33,175: INFO: main: Training : batch 512 Loss: 0.0007393119854896212]
[2024-04-17 13:11:33,804: INFO: main: Training : batch 513 Loss: 0.018653546660776]
[2024-04-17 13:11:34,435: INFO: main: Training : batch 514 Loss: 0.010046943768509087]
[2024-04-17 13:11:35,065: INFO: main: Training : batch 515 Loss: 0.004303342832273798]
[2024-04-17 13:11:35,700: INFO: main: Training : batch 516 Loss: 0.007004609181236635]
[2024-04-17 13:11:36,346: INFO: main: Training : batch 517 Loss: 0.0022159103921001867]
[2024-04-17 13:11:36,986: INFO: main: Training : batch 518 Loss: 0.0005389290224788149]
[2024-04-17 13:11:37,622: INFO: main: Training : batch 519 Loss: 0.018612516234360325]
[2024-04-17 13:11:38,265: INFO: main: Training : batch 520 Loss: 0.007595205821362944]
[2024-04-17 13:11:38,897: INFO: main: Training : batch 521 Loss: 0.013875488918866773]
[2024-04-17 13:11:39,526: INFO: main: Training : batch 522 Loss: 0.004722158093959375]
[2024-04-17 13:11:40,157: INFO: main: Training : batch 523 Loss: 0.003218431632347717]
[2024-04-17 13:11:40,788: INFO: main: Training : batch 524 Loss: 0.019617483175243193]
[2024-04-17 13:11:41,421: INFO: main: Training : batch 525 Loss: 0.0025163130455902903]
[2024-04-17 13:11:42,048: INFO: main: Training : batch 526 Loss: 0.007047935465353965]
[2024-04-17 13:11:42,676: INFO: main: Training : batch 527 Loss: 0.015894731685061773]
[2024-04-17 13:11:43,308: INFO: main: Training : batch 528 Loss: 0.005908251817756826]
[2024-04-17 13:11:43,935: INFO: main: Training : batch 529 Loss: 0.004518547112663609]
[2024-04-17 13:11:44,562: INFO: main: Training : batch 530 Loss: 0.0008489011462837065]
[2024-04-17 13:11:45,190: INFO: main: Training : batch 531 Loss: 0.01110734087856454]
[2024-04-17 13:11:45,820: INFO: main: Training : batch 532 Loss: 0.008793541858490685]
[2024-04-17 13:11:46,446: INFO: main: Training : batch 533 Loss: 0.010682986181076859]
[2024-04-17 13:11:47,080: INFO: main: Training : batch 534 Loss: 0.004768863850447167]
[2024-04-17 13:11:47,713: INFO: main: Training : batch 535 Loss: 0.003929977137826903]
[2024-04-17 13:11:48,349: INFO: main: Training : batch 536 Loss: 0.0004922832690222727]
[2024-04-17 13:11:48,988: INFO: main: Training : batch 537 Loss: 0.01039779456662114]
[2024-04-17 13:11:49,630: INFO: main: Training : batch 538 Loss: 0.011724186005458043]
[2024-04-17 13:11:50,265: INFO: main: Training : batch 539 Loss: 0.003116183236921672]
[2024-04-17 13:11:50,909: INFO: main: Training : batch 540 Loss: 0.008501184690548738]
[2024-04-17 13:11:51,546: INFO: main: Training : batch 541 Loss: 0.0007251853510537389]
[2024-04-17 13:11:52,188: INFO: main: Training : batch 542 Loss: 0.0024170054394843537]
[2024-04-17 13:11:52,831: INFO: main: Training : batch 543 Loss: 0.004989385863660983]
[2024-04-17 13:11:53,467: INFO: main: Training : batch 544 Loss: 0.0065073247151342635]
[2024-04-17 13:11:54,100: INFO: main: Training : batch 545 Loss: 0.002442435785406602]
[2024-04-17 13:11:54,732: INFO: main: Training : batch 546 Loss: 0.009702033305957656]
[2024-04-17 13:11:55,363: INFO: main: Training : batch 547 Loss: 0.004562342664824517]
[2024-04-17 13:11:55,998: INFO: main: Training : batch 548 Loss: 0.015282822567827972]
[2024-04-17 13:11:56,630: INFO: main: Training : batch 549 Loss: 0.0013735831008465287]
[2024-04-17 13:11:57,264: INFO: main: Training : batch 550 Loss: 0.010335475797854205]
[2024-04-17 13:11:57,897: INFO: main: Training : batch 551 Loss: 0.010657428050229371]
[2024-04-17 13:11:58,526: INFO: main: Training : batch 552 Loss: 0.006901061510832106]
[2024-04-17 13:11:59,158: INFO: main: Training : batch 553 Loss: 0.005287032786839298]
[2024-04-17 13:11:59,794: INFO: main: Training : batch 554 Loss: 0.003645853160609763]
[2024-04-17 13:12:00,429: INFO: main: Training : batch 555 Loss: 0.01328831136125576]
[2024-04-17 13:12:01,066: INFO: main: Training : batch 556 Loss: 0.0006314713750312849]
[2024-04-17 13:12:01,700: INFO: main: Training : batch 557 Loss: 0.0012814761246907546]
[2024-04-17 13:12:02,339: INFO: main: Training : batch 558 Loss: 0.0070171996620718435]
[2024-04-17 13:12:02,976: INFO: main: Training : batch 559 Loss: 0.005993885742969833]
[2024-04-17 13:12:03,612: INFO: main: Training : batch 560 Loss: 0.002136910077429179]
[2024-04-17 13:12:04,251: INFO: main: Training : batch 561 Loss: 0.010390268814787669]
[2024-04-17 13:12:04,891: INFO: main: Training : batch 562 Loss: 0.001087928531207385]
[2024-04-17 13:12:05,536: INFO: main: Training : batch 563 Loss: 0.007337223765927709]
[2024-04-17 13:12:06,172: INFO: main: Training : batch 564 Loss: 0.0039065394787088205]
[2024-04-17 13:12:06,802: INFO: main: Training : batch 565 Loss: 0.003963970461454016]
[2024-04-17 13:12:07,438: INFO: main: Training : batch 566 Loss: 0.011998760450351061]
[2024-04-17 13:12:08,075: INFO: main: Training : batch 567 Loss: 0.0058429411254209244]
[2024-04-17 13:12:08,714: INFO: main: Training : batch 568 Loss: 0.002993257102971615]
[2024-04-17 13:12:09,349: INFO: main: Training : batch 569 Loss: 0.005045351970391689]
[2024-04-17 13:12:09,986: INFO: main: Training : batch 570 Loss: 0.011633455804785408]
[2024-04-17 13:12:10,618: INFO: main: Training : batch 571 Loss: 0.0066899305705445606]
[2024-04-17 13:12:11,255: INFO: main: Training : batch 572 Loss: 0.009594339978061963]
[2024-04-17 13:12:11,892: INFO: main: Training : batch 573 Loss: 0.003746796969247415]
[2024-04-17 13:12:12,526: INFO: main: Training : batch 574 Loss: 0.004341424658836517]
[2024-04-17 13:12:13,156: INFO: main: Training : batch 575 Loss: 0.0076573198947814045]
[2024-04-17 13:12:13,788: INFO: main: Training : batch 576 Loss: 0.0072657535943622086]
[2024-04-17 13:12:14,418: INFO: main: Training : batch 577 Loss: 0.022309605549679215]
[2024-04-17 13:12:15,054: INFO: main: Training : batch 578 Loss: 0.003422243974312115]
[2024-04-17 13:12:15,697: INFO: main: Training : batch 579 Loss: 0.006717618030454932]
[2024-04-17 13:12:16,332: INFO: main: Training : batch 580 Loss: 0.0004622664867704141]
[2024-04-17 13:12:16,975: INFO: main: Training : batch 581 Loss: 0.002682087082617937]
[2024-04-17 13:12:17,625: INFO: main: Training : batch 582 Loss: 0.004105183338402021]
[2024-04-17 13:12:18,267: INFO: main: Training : batch 583 Loss: 0.0029423658186245698]
[2024-04-17 13:12:18,908: INFO: main: Training : batch 584 Loss: 0.010566381667508512]
[2024-04-17 13:12:19,537: INFO: main: Training : batch 585 Loss: 0.018008369493594707]
[2024-04-17 13:12:20,167: INFO: main: Training : batch 586 Loss: 0.0028610075926679707]
[2024-04-17 13:12:20,795: INFO: main: Training : batch 587 Loss: 0.0257046955386824]
[2024-04-17 13:12:21,417: INFO: main: Training : batch 588 Loss: 0.008815692378166047]
[2024-04-17 13:12:22,050: INFO: main: Training : batch 589 Loss: 0.0015075719046056418]
[2024-04-17 13:12:22,680: INFO: main: Training : batch 590 Loss: 0.0007827374694358428]
[2024-04-17 13:12:23,308: INFO: main: Training : batch 591 Loss: 0.007169287265612172]
[2024-04-17 13:12:23,939: INFO: main: Training : batch 592 Loss: 0.007288452116043043]
[2024-04-17 13:12:24,571: INFO: main: Training : batch 593 Loss: 0.011182239623329501]
[2024-04-17 13:12:25,197: INFO: main: Training : batch 594 Loss: 0.0018129800786610877]
[2024-04-17 13:12:25,831: INFO: main: Training : batch 595 Loss: 0.00398051679684317]
[2024-04-17 13:12:26,463: INFO: main: Training : batch 596 Loss: 0.0014501832870938698]
[2024-04-17 13:12:27,093: INFO: main: Training : batch 597 Loss: 0.0036568512005596175]
[2024-04-17 13:12:27,724: INFO: main: Training : batch 598 Loss: 0.019655626862111468]
[2024-04-17 13:12:28,348: INFO: main: Training : batch 599 Loss: 0.010478199527533042]
[2024-04-17 13:12:28,976: INFO: main: Training : batch 600 Loss: 0.0075154171012335805]
[2024-04-17 13:12:29,614: INFO: main: Training : batch 601 Loss: 0.0017232242262501447]
[2024-04-17 13:12:30,251: INFO: main: Training : batch 602 Loss: 0.015012154527147475]
[2024-04-17 13:12:30,892: INFO: main: Training : batch 603 Loss: 0.009800914254447027]
[2024-04-17 13:12:31,528: INFO: main: Training : batch 604 Loss: 0.0010026555228799733]
[2024-04-17 13:12:32,165: INFO: main: Training : batch 605 Loss: 0.005187732760359283]
[2024-04-17 13:12:32,794: INFO: main: Training : batch 606 Loss: 0.0026435862487655505]
[2024-04-17 13:12:33,419: INFO: main: Training : batch 607 Loss: 0.0020455922610987426]
[2024-04-17 13:12:34,045: INFO: main: Training : batch 608 Loss: 0.003527068603902675]
[2024-04-17 13:12:34,672: INFO: main: Training : batch 609 Loss: 0.007208801438934066]
[2024-04-17 13:12:35,302: INFO: main: Training : batch 610 Loss: 0.002423968172622694]
[2024-04-17 13:12:35,931: INFO: main: Training : batch 611 Loss: 0.01178510593207329]
[2024-04-17 13:12:36,562: INFO: main: Training : batch 612 Loss: 0.003143331296630664]
[2024-04-17 13:12:37,191: INFO: main: Training : batch 613 Loss: 0.005732420683130968]
[2024-04-17 13:12:37,816: INFO: main: Training : batch 614 Loss: 0.014164751422730758]
[2024-04-17 13:12:38,445: INFO: main: Training : batch 615 Loss: 0.0020345057022939017]
[2024-04-17 13:12:39,075: INFO: main: Training : batch 616 Loss: 0.005144913180299569]
[2024-04-17 13:12:39,702: INFO: main: Training : batch 617 Loss: 0.004991542269082997]
[2024-04-17 13:12:40,329: INFO: main: Training : batch 618 Loss: 0.008502311264433522]
[2024-04-17 13:12:40,953: INFO: main: Training : batch 619 Loss: 0.003440112842722078]
[2024-04-17 13:12:41,581: INFO: main: Training : batch 620 Loss: 0.0032174032476152098]
[2024-04-17 13:12:42,209: INFO: main: Training : batch 621 Loss: 0.002757183653073046]
[2024-04-17 13:12:42,844: INFO: main: Training : batch 622 Loss: 0.003469605704430201]
[2024-04-17 13:12:43,477: INFO: main: Training : batch 623 Loss: 0.0059453314828429695]
[2024-04-17 13:12:44,120: INFO: main: Training : batch 624 Loss: 0.002837720338006527]
[2024-04-17 13:12:44,753: INFO: main: Training : batch 625 Loss: 0.01482764477592091]
[2024-04-17 13:12:45,386: INFO: main: Training : batch 626 Loss: 0.0099246694318069]
[2024-04-17 13:12:46,020: INFO: main: Training : batch 627 Loss: 0.011390146346218266]
[2024-04-17 13:12:46,641: INFO: main: Training : batch 628 Loss: 0.006384118067883241]
[2024-04-17 13:12:47,278: INFO: main: Training : batch 629 Loss: 0.010992918513293523]
[2024-04-17 13:12:47,905: INFO: main: Training : batch 630 Loss: 0.01037082466534645]
[2024-04-17 13:12:48,536: INFO: main: Training : batch 631 Loss: 0.007530785824340253]
[2024-04-17 13:12:49,166: INFO: main: Training : batch 632 Loss: 9.610353101714452e-05]
[2024-04-17 13:12:49,799: INFO: main: Training : batch 633 Loss: 0.001004714713911283]
[2024-04-17 13:12:50,430: INFO: main: Training : batch 634 Loss: 0.028799988545902277]
[2024-04-17 13:12:51,061: INFO: main: Training : batch 635 Loss: 0.004135730517438728]
[2024-04-17 13:12:51,695: INFO: main: Training : batch 636 Loss: 0.0016443979103120945]
[2024-04-17 13:12:52,328: INFO: main: Training : batch 637 Loss: 0.003451358992548903]
[2024-04-17 13:12:52,961: INFO: main: Training : batch 638 Loss: 0.006125213886242082]
[2024-04-17 13:12:53,596: INFO: main: Training : batch 639 Loss: 0.00043590890751280925]
[2024-04-17 13:12:54,225: INFO: main: Training : batch 640 Loss: 0.0036013789243585473]
[2024-04-17 13:12:54,858: INFO: main: Training : batch 641 Loss: 0.002473873015814829]
[2024-04-17 13:12:55,492: INFO: main: Training : batch 642 Loss: 0.00601313178338295]
[2024-04-17 13:12:56,132: INFO: main: Training : batch 643 Loss: 0.0023013298382447973]
[2024-04-17 13:12:56,769: INFO: main: Training : batch 644 Loss: 0.025137052705154997]
[2024-04-17 13:12:57,413: INFO: main: Training : batch 645 Loss: 0.00451769648020485]
[2024-04-17 13:12:58,052: INFO: main: Training : batch 646 Loss: 0.026369848942471456]
[2024-04-17 13:12:58,688: INFO: main: Training : batch 647 Loss: 0.008527632048480317]
[2024-04-17 13:12:59,326: INFO: main: Training : batch 648 Loss: 0.002910347539466777]
[2024-04-17 13:12:59,962: INFO: main: Training : batch 649 Loss: 0.002393582675101336]
[2024-04-17 13:13:00,596: INFO: main: Training : batch 650 Loss: 0.0026346292233256725]
[2024-04-17 13:13:01,227: INFO: main: Training : batch 651 Loss: 0.005643970036831956]
[2024-04-17 13:13:01,860: INFO: main: Training : batch 652 Loss: 0.003268760674344552]
[2024-04-17 13:13:02,495: INFO: main: Training : batch 653 Loss: 0.0029087214392199776]
[2024-04-17 13:13:03,129: INFO: main: Training : batch 654 Loss: 0.007544771491096764]
[2024-04-17 13:13:03,767: INFO: main: Training : batch 655 Loss: 0.006143196791022983]
[2024-04-17 13:13:04,402: INFO: main: Training : batch 656 Loss: 0.0023264718975793885]
[2024-04-17 13:13:05,035: INFO: main: Training : batch 657 Loss: 0.008380388314441559]
[2024-04-17 13:13:05,668: INFO: main: Training : batch 658 Loss: 0.0030533225716613973]
[2024-04-17 13:13:06,301: INFO: main: Training : batch 659 Loss: 0.006426451413761338]
[2024-04-17 13:13:06,936: INFO: main: Training : batch 660 Loss: 0.00489323967584864]
[2024-04-17 13:13:07,570: INFO: main: Training : batch 661 Loss: 0.0007136434109981189]
[2024-04-17 13:13:08,202: INFO: main: Training : batch 662 Loss: 0.004731634308261721]
[2024-04-17 13:13:08,834: INFO: main: Training : batch 663 Loss: 0.002488662083814545]
[2024-04-17 13:13:09,478: INFO: main: Training : batch 664 Loss: 0.0020668919534812]
[2024-04-17 13:13:10,128: INFO: main: Training : batch 665 Loss: 0.0019649612421114423]
[2024-04-17 13:13:10,778: INFO: main: Training : batch 666 Loss: 0.004545757361851636]
[2024-04-17 13:13:11,424: INFO: main: Training : batch 667 Loss: 0.004922381245732939]
[2024-04-17 13:13:12,066: INFO: main: Training : batch 668 Loss: 0.0013256549842873716]
[2024-04-17 13:13:12,709: INFO: main: Training : batch 669 Loss: 0.003283464295168536]
[2024-04-17 13:13:13,346: INFO: main: Training : batch 670 Loss: 0.0025890272991063315]
[2024-04-17 13:13:13,978: INFO: main: Training : batch 671 Loss: 0.009645164506130566]
[2024-04-17 13:13:14,610: INFO: main: Training : batch 672 Loss: 0.0030893943985910183]
[2024-04-17 13:13:15,247: INFO: main: Training : batch 673 Loss: 0.00887161551864781]
[2024-04-17 13:13:15,883: INFO: main: Training : batch 674 Loss: 0.02072927900882593]
[2024-04-17 13:13:16,519: INFO: main: Training : batch 675 Loss: 0.0032324195286111343]
[2024-04-17 13:13:17,149: INFO: main: Training : batch 676 Loss: 0.0043232390198729865]
[2024-04-17 13:13:17,785: INFO: main: Training : batch 677 Loss: 0.010522697174001207]
[2024-04-17 13:13:18,419: INFO: main: Training : batch 678 Loss: 0.0007568488971495233]
[2024-04-17 13:13:19,054: INFO: main: Training : batch 679 Loss: 0.0005125225179985353]
[2024-04-17 13:13:19,688: INFO: main: Training : batch 680 Loss: 0.0030613392356073233]
[2024-04-17 13:13:20,327: INFO: main: Training : batch 681 Loss: 0.001699178431021069]
[2024-04-17 13:13:20,964: INFO: main: Training : batch 682 Loss: 0.008312020312654543]
[2024-04-17 13:13:21,599: INFO: main: Training : batch 683 Loss: 0.012141599561034356]
[2024-04-17 13:13:22,235: INFO: main: Training : batch 684 Loss: 0.016400711312918786]
[2024-04-17 13:13:22,869: INFO: main: Training : batch 685 Loss: 0.00460803800297763]
[2024-04-17 13:13:23,509: INFO: main: Training : batch 686 Loss: 0.02494933362003657]
[2024-04-17 13:13:24,159: INFO: main: Training : batch 687 Loss: 0.005319698238786764]
[2024-04-17 13:13:24,801: INFO: main: Training : batch 688 Loss: 0.0025990684923846335]
[2024-04-17 13:13:25,440: INFO: main: Training : batch 689 Loss: 0.001841939821539589]
[2024-04-17 13:13:26,076: INFO: main: Training : batch 690 Loss: 0.002146754243331902]
[2024-04-17 13:13:26,710: INFO: main: Training : batch 691 Loss: 0.012617008927126897]
[2024-04-17 13:13:27,343: INFO: main: Training : batch 692 Loss: 0.0014242435404231322]
[2024-04-17 13:13:27,975: INFO: main: Training : batch 693 Loss: 0.005888014134754974]
[2024-04-17 13:13:28,610: INFO: main: Training : batch 694 Loss: 0.00515820169120908]
[2024-04-17 13:13:29,247: INFO: main: Training : batch 695 Loss: 0.00032858710172850976]
[2024-04-17 13:13:29,882: INFO: main: Training : batch 696 Loss: 0.004730551630451672]
[2024-04-17 13:13:30,517: INFO: main: Training : batch 697 Loss: 0.007458817114389267]
[2024-04-17 13:13:31,154: INFO: main: Training : batch 698 Loss: 0.004532674638862185]
[2024-04-17 13:13:31,782: INFO: main: Training : batch 699 Loss: 0.004690884108731722]
[2024-04-17 13:13:32,415: INFO: main: Training : batch 700 Loss: 0.0009206753772469487]
[2024-04-17 13:13:33,047: INFO: main: Training : batch 701 Loss: 0.003700236813778867]
[2024-04-17 13:13:33,679: INFO: main: Training : batch 702 Loss: 0.014876216275614263]
[2024-04-17 13:13:34,307: INFO: main: Training : batch 703 Loss: 0.001835838404067847]
[2024-04-17 13:13:34,941: INFO: main: Training : batch 704 Loss: 0.0002745090706018287]
[2024-04-17 13:13:35,573: INFO: main: Training : batch 705 Loss: 0.014376552232858034]
[2024-04-17 13:13:36,211: INFO: main: Training : batch 706 Loss: 0.0015265112850662147]
[2024-04-17 13:13:36,846: INFO: main: Training : batch 707 Loss: 0.0005173263776726442]
[2024-04-17 13:13:37,487: INFO: main: Training : batch 708 Loss: 0.006106961383813259]
[2024-04-17 13:13:38,128: INFO: main: Training : batch 709 Loss: 0.0041941275093560415]
[2024-04-17 13:13:38,763: INFO: main: Training : batch 710 Loss: 0.002075058191299924]
[2024-04-17 13:13:39,403: INFO: main: Training : batch 711 Loss: 0.029257837935336106]
[2024-04-17 13:13:40,037: INFO: main: Training : batch 712 Loss: 0.011278051019762233]
[2024-04-17 13:13:40,671: INFO: main: Training : batch 713 Loss: 0.003479583321073895]
[2024-04-17 13:13:41,306: INFO: main: Training : batch 714 Loss: 0.002537387978028514]
[2024-04-17 13:13:41,938: INFO: main: Training : batch 715 Loss: 0.0037825075238326162]
[2024-04-17 13:13:42,570: INFO: main: Training : batch 716 Loss: 0.0028138070161797253]
[2024-04-17 13:13:43,200: INFO: main: Training : batch 717 Loss: 0.0037612590008599327]
[2024-04-17 13:13:43,829: INFO: main: Training : batch 718 Loss: 0.006499355801538753]
[2024-04-17 13:13:44,465: INFO: main: Training : batch 719 Loss: 0.015372025753255877]
[2024-04-17 13:13:45,097: INFO: main: Training : batch 720 Loss: 0.005014333240875055]
[2024-04-17 13:13:45,728: INFO: main: Training : batch 721 Loss: 0.0035356901870092277]
[2024-04-17 13:13:46,364: INFO: main: Training : batch 722 Loss: 0.010641160191809702]
[2024-04-17 13:13:46,995: INFO: main: Training : batch 723 Loss: 0.0015899674311775022]
[2024-04-17 13:13:47,625: INFO: main: Training : batch 724 Loss: 0.0005806674547446295]
[2024-04-17 13:13:48,253: INFO: main: Training : batch 725 Loss: 0.01706707964939088]
[2024-04-17 13:13:48,887: INFO: main: Training : batch 726 Loss: 0.0023212358042753066]
[2024-04-17 13:13:49,520: INFO: main: Training : batch 727 Loss: 0.0045584000311476545]
[2024-04-17 13:13:50,161: INFO: main: Training : batch 728 Loss: 0.0059938531338857215]
[2024-04-17 13:13:50,799: INFO: main: Training : batch 729 Loss: 0.002199874849267632]
[2024-04-17 13:13:51,435: INFO: main: Training : batch 730 Loss: 0.014254419223223378]
[2024-04-17 13:13:52,061: INFO: main: Training : batch 731 Loss: 0.004883625339197321]
[2024-04-17 13:13:52,699: INFO: main: Training : batch 732 Loss: 0.005010582155407391]
[2024-04-17 13:13:53,326: INFO: main: Training : batch 733 Loss: 0.0010010513733935253]
[2024-04-17 13:13:53,954: INFO: main: Training : batch 734 Loss: 0.010807419525944995]
[2024-04-17 13:13:54,585: INFO: main: Training : batch 735 Loss: 0.00518740298158441]
[2024-04-17 13:13:55,210: INFO: main: Training : batch 736 Loss: 0.0026642338103503795]
[2024-04-17 13:13:55,843: INFO: main: Training : batch 737 Loss: 0.0016234166694108932]
[2024-04-17 13:13:56,467: INFO: main: Training : batch 738 Loss: 0.0038835504717634906]
[2024-04-17 13:13:57,097: INFO: main: Training : batch 739 Loss: 0.017166935307628202]
[2024-04-17 13:13:57,726: INFO: main: Training : batch 740 Loss: 0.00537715615205181]
[2024-04-17 13:13:58,353: INFO: main: Training : batch 741 Loss: 0.004137954561117488]
[2024-04-17 13:13:58,980: INFO: main: Training : batch 742 Loss: 0.0071899876376809125]
[2024-04-17 13:13:59,610: INFO: main: Training : batch 743 Loss: 0.003476455478735073]
[2024-04-17 13:14:00,239: INFO: main: Training : batch 744 Loss: 0.0011381550688687918]
[2024-04-17 13:14:00,866: INFO: main: Training : batch 745 Loss: 0.0005349000176878113]
[2024-04-17 13:14:01,495: INFO: main: Training : batch 746 Loss: 0.0037589558387303487]
[2024-04-17 13:14:02,126: INFO: main: Training : batch 747 Loss: 0.013633255265138044]
[2024-04-17 13:14:02,756: INFO: main: Training : batch 748 Loss: 0.00342593499322289]
[2024-04-17 13:14:03,393: INFO: main: Training : batch 749 Loss: 0.01236204894821905]
[2024-04-17 13:14:04,027: INFO: main: Training : batch 750 Loss: 0.0017559901343766047]
[2024-04-17 13:14:04,662: INFO: main: Training : batch 751 Loss: 0.003935357373605341]
[2024-04-17 13:14:05,295: INFO: main: Training : batch 752 Loss: 0.004114289535505851]
[2024-04-17 13:14:05,930: INFO: main: Training : batch 753 Loss: 0.002447293690563695]
[2024-04-17 13:14:06,567: INFO: main: Training : batch 754 Loss: 0.00016269090634143222]
[2024-04-17 13:14:07,200: INFO: main: Training : batch 755 Loss: 0.029347888365289104]
[2024-04-17 13:14:07,826: INFO: main: Training : batch 756 Loss: 0.004893509682904637]
[2024-04-17 13:14:08,454: INFO: main: Training : batch 757 Loss: 0.006190716311746445]
[2024-04-17 13:14:09,088: INFO: main: Training : batch 758 Loss: 0.017131880733958837]
[2024-04-17 13:14:09,722: INFO: main: Training : batch 759 Loss: 0.0013065156122004334]
[2024-04-17 13:14:10,355: INFO: main: Training : batch 760 Loss: 0.0024903399192390424]
[2024-04-17 13:14:10,991: INFO: main: Training : batch 761 Loss: 0.0027164603444388784]
[2024-04-17 13:14:11,621: INFO: main: Training : batch 762 Loss: 0.0016724651057659166]
[2024-04-17 13:14:12,253: INFO: main: Training : batch 763 Loss: 0.004672582645844574]
[2024-04-17 13:14:12,883: INFO: main: Training : batch 764 Loss: 0.008879949159013701]
[2024-04-17 13:14:13,515: INFO: main: Training : batch 765 Loss: 0.0031219625040700303]
[2024-04-17 13:14:14,146: INFO: main: Training : batch 766 Loss: 0.005771401341949905]
[2024-04-17 13:14:14,781: INFO: main: Training : batch 767 Loss: 0.0037188321409853237]
[2024-04-17 13:14:15,416: INFO: main: Training : batch 768 Loss: 0.0004721191562966819]
[2024-04-17 13:14:16,046: INFO: main: Training : batch 769 Loss: 0.0034940171040304317]
[2024-04-17 13:14:16,677: INFO: main: Training : batch 770 Loss: 0.006531276564705059]
[2024-04-17 13:14:17,324: INFO: main: Training : batch 771 Loss: 0.0027542824639480608]
[2024-04-17 13:14:17,970: INFO: main: Training : batch 772 Loss: 0.0016957203153790197]
[2024-04-17 13:14:18,609: INFO: main: Training : batch 773 Loss: 0.006347307211068635]
[2024-04-17 13:14:19,244: INFO: main: Training : batch 774 Loss: 0.0046843127970488925]
[2024-04-17 13:14:19,887: INFO: main: Training : batch 775 Loss: 0.0065971461483765636]
[2024-04-17 13:14:20,519: INFO: main: Training : batch 776 Loss: 0.007917450306459562]
[2024-04-17 13:14:21,146: INFO: main: Training : batch 777 Loss: 0.0016666638732649559]
[2024-04-17 13:14:21,782: INFO: main: Training : batch 778 Loss: 0.004650569385962885]
[2024-04-17 13:14:22,416: INFO: main: Training : batch 779 Loss: 0.0005795661687594668]
[2024-04-17 13:14:23,050: INFO: main: Training : batch 780 Loss: 0.013554508107267426]
[2024-04-17 13:14:23,684: INFO: main: Training : batch 781 Loss: 0.004404908809597325]
[2024-04-17 13:14:24,319: INFO: main: Training : batch 782 Loss: 0.0022720792768013355]
[2024-04-17 13:14:24,948: INFO: main: Training : batch 783 Loss: 0.00618638813486214]
[2024-04-17 13:14:25,577: INFO: main: Training : batch 784 Loss: 0.006881636056757337]
[2024-04-17 13:14:26,206: INFO: main: Training : batch 785 Loss: 0.0015531474843789028]
[2024-04-17 13:14:26,837: INFO: main: Training : batch 786 Loss: 0.024611607930146235]
[2024-04-17 13:14:27,472: INFO: main: Training : batch 787 Loss: 0.005393062603668249]
[2024-04-17 13:14:28,103: INFO: main: Training : batch 788 Loss: 0.0007664429601746268]
[2024-04-17 13:14:28,732: INFO: main: Training : batch 789 Loss: 0.00202166966629154]
[2024-04-17 13:14:29,371: INFO: main: Training : batch 790 Loss: 0.00621456010552545]
[2024-04-17 13:14:30,005: INFO: main: Training : batch 791 Loss: 0.0021457770383548325]
[2024-04-17 13:14:30,639: INFO: main: Training : batch 792 Loss: 0.009251835094291573]
[2024-04-17 13:14:31,281: INFO: main: Training : batch 793 Loss: 0.011622187465145532]
[2024-04-17 13:14:31,920: INFO: main: Training : batch 794 Loss: 0.0016468857823604833]
[2024-04-17 13:14:32,558: INFO: main: Training : batch 795 Loss: 0.0020068950570889988]
[2024-04-17 13:14:33,200: INFO: main: Training : batch 796 Loss: 0.0040067565783298]
[2024-04-17 13:14:33,836: INFO: main: Training : batch 797 Loss: 0.011577201574949543]
[2024-04-17 13:14:34,467: INFO: main: Training : batch 798 Loss: 0.010413070128472613]
[2024-04-17 13:14:35,099: INFO: main: Training : batch 799 Loss: 0.009053285235100156]
[2024-04-17 13:14:35,732: INFO: main: Training : batch 800 Loss: 0.0036505477606050326]
[2024-04-17 13:14:36,365: INFO: main: Training : batch 801 Loss: 0.00038785857666832343]
[2024-04-17 13:14:36,999: INFO: main: Training : batch 802 Loss: 0.004997253062778836]
[2024-04-17 13:14:37,633: INFO: main: Training : batch 803 Loss: 0.0026330850222629643]
[2024-04-17 13:14:38,270: INFO: main: Training : batch 804 Loss: 0.0032048143988655173]
[2024-04-17 13:14:38,901: INFO: main: Training : batch 805 Loss: 0.0006445529780949802]
[2024-04-17 13:14:39,535: INFO: main: Training : batch 806 Loss: 0.012998437219898703]
[2024-04-17 13:14:40,168: INFO: main: Training : batch 807 Loss: 0.019841271749726344]
[2024-04-17 13:14:40,804: INFO: main: Training : batch 808 Loss: 0.006325671843103202]
[2024-04-17 13:14:41,437: INFO: main: Training : batch 809 Loss: 0.00615419802271515]
[2024-04-17 13:14:42,073: INFO: main: Training : batch 810 Loss: 0.014462279002329633]
[2024-04-17 13:14:42,704: INFO: main: Training : batch 811 Loss: 0.006619589553955438]
[2024-04-17 13:14:43,335: INFO: main: Training : batch 812 Loss: 0.0023910871796413127]
[2024-04-17 13:14:43,976: INFO: main: Training : batch 813 Loss: 0.006191076228409514]
[2024-04-17 13:14:44,619: INFO: main: Training : batch 814 Loss: 0.003839450855062787]
[2024-04-17 13:14:45,257: INFO: main: Training : batch 815 Loss: 0.011341590071701152]
[2024-04-17 13:14:45,900: INFO: main: Training : batch 816 Loss: 0.009181277883128235]
[2024-04-17 13:14:46,549: INFO: main: Training : batch 817 Loss: 0.0018352061220118683]
[2024-04-17 13:14:47,195: INFO: main: Training : batch 818 Loss: 0.006856183218629398]
[2024-04-17 13:14:47,823: INFO: main: Training : batch 819 Loss: 0.005666276156550296]
[2024-04-17 13:14:48,453: INFO: main: Training : batch 820 Loss: 0.011544414687062061]
[2024-04-17 13:14:49,086: INFO: main: Training : batch 821 Loss: 0.019647807094285832]
[2024-04-17 13:14:49,718: INFO: main: Training : batch 822 Loss: 0.01021152915891097]
[2024-04-17 13:14:50,351: INFO: main: Training : batch 823 Loss: 0.01864421848089389]
[2024-04-17 13:14:50,983: INFO: main: Training : batch 824 Loss: 0.012539619555018647]
[2024-04-17 13:14:51,615: INFO: main: Training : batch 825 Loss: 0.005731593426410795]
[2024-04-17 13:14:52,249: INFO: main: Training : batch 826 Loss: 0.002729678922937956]
[2024-04-17 13:14:52,881: INFO: main: Training : batch 827 Loss: 0.002885360200392094]
[2024-04-17 13:14:53,516: INFO: main: Training : batch 828 Loss: 0.018752847213954146]
[2024-04-17 13:14:54,153: INFO: main: Training : batch 829 Loss: 0.00313244973435998]
[2024-04-17 13:14:54,785: INFO: main: Training : batch 830 Loss: 0.006722085189757301]
[2024-04-17 13:14:55,419: INFO: main: Training : batch 831 Loss: 0.0061447698948632395]
[2024-04-17 13:14:56,051: INFO: main: Training : batch 832 Loss: 0.009842888267251585]
[2024-04-17 13:14:56,681: INFO: main: Training : batch 833 Loss: 0.002085001005685655]
[2024-04-17 13:14:57,320: INFO: main: Training : batch 834 Loss: 0.006069623488050273]
[2024-04-17 13:14:57,961: INFO: main: Training : batch 835 Loss: 0.0066801270348977775]
[2024-04-17 13:14:58,598: INFO: main: Training : batch 836 Loss: 0.004052448156813218]
[2024-04-17 13:14:59,238: INFO: main: Training : batch 837 Loss: 0.0006027802134543368]
[2024-04-17 13:14:59,880: INFO: main: Training : batch 838 Loss: 0.001989434738565671]
[2024-04-17 13:15:00,530: INFO: main: Training : batch 839 Loss: 0.006434710897869471]
[2024-04-17 13:15:01,160: INFO: main: Training : batch 840 Loss: 0.001037351783012116]
[2024-04-17 13:15:01,790: INFO: main: Training : batch 841 Loss: 0.03479188016266625]
[2024-04-17 13:15:02,424: INFO: main: Training : batch 842 Loss: 0.0038922046123348332]
[2024-04-17 13:15:03,060: INFO: main: Training : batch 843 Loss: 0.006966249784303316]
[2024-04-17 13:15:03,693: INFO: main: Training : batch 844 Loss: 0.003921917235381012]
[2024-04-17 13:15:04,325: INFO: main: Training : batch 845 Loss: 0.05059744928809702]
[2024-04-17 13:15:04,956: INFO: main: Training : batch 846 Loss: 0.005243620883753318]
[2024-04-17 13:15:05,593: INFO: main: Training : batch 847 Loss: 0.008019916249047226]
[2024-04-17 13:15:06,230: INFO: main: Training : batch 848 Loss: 0.07982981562334919]
[2024-04-17 13:15:06,864: INFO: main: Training : batch 849 Loss: 0.008738305590340134]
[2024-04-17 13:15:07,496: INFO: main: Training : batch 850 Loss: 0.014584134575735127]
[2024-04-17 13:15:08,133: INFO: main: Training : batch 851 Loss: 0.01369349352481759]
[2024-04-17 13:15:08,767: INFO: main: Training : batch 852 Loss: 0.003067979398903812]
[2024-04-17 13:15:09,396: INFO: main: Training : batch 853 Loss: 0.016596402240103426]
[2024-04-17 13:15:10,025: INFO: main: Training : batch 854 Loss: 0.005848645026815032]
[2024-04-17 13:15:10,664: INFO: main: Training : batch 855 Loss: 0.006928004601288015]
[2024-04-17 13:15:11,298: INFO: main: Training : batch 856 Loss: 0.0010521407332391261]
[2024-04-17 13:15:11,942: INFO: main: Training : batch 857 Loss: 0.016396439832486762]
[2024-04-17 13:15:12,580: INFO: main: Training : batch 858 Loss: 0.0016840876848462727]
[2024-04-17 13:15:13,216: INFO: main: Training : batch 859 Loss: 0.009373800125943108]
[2024-04-17 13:15:13,854: INFO: main: Training : batch 860 Loss: 0.004353472169878808]
[2024-04-17 13:15:14,489: INFO: main: Training : batch 861 Loss: 0.0037324003545346715]
[2024-04-17 13:15:15,120: INFO: main: Training : batch 862 Loss: 0.011550721642383448]
[2024-04-17 13:15:15,757: INFO: main: Training : batch 863 Loss: 0.002405149677947595]
[2024-04-17 13:15:16,390: INFO: main: Training : batch 864 Loss: 0.006792503464272789]
[2024-04-17 13:15:17,022: INFO: main: Training : batch 865 Loss: 0.004457696072222461]
[2024-04-17 13:15:17,654: INFO: main: Training : batch 866 Loss: 0.009108737313236456]
[2024-04-17 13:15:18,287: INFO: main: Training : batch 867 Loss: 0.0014643599038581163]
[2024-04-17 13:15:18,919: INFO: main: Training : batch 868 Loss: 0.001511731521349708]
[2024-04-17 13:15:19,551: INFO: main: Training : batch 869 Loss: 0.008514146999186631]
[2024-04-17 13:15:20,180: INFO: main: Training : batch 870 Loss: 0.014782561172122512]
[2024-04-17 13:15:20,815: INFO: main: Training : batch 871 Loss: 0.010951287798306633]
[2024-04-17 13:15:21,448: INFO: main: Training : batch 872 Loss: 0.0022301760479263273]
[2024-04-17 13:15:22,073: INFO: main: Training : batch 873 Loss: 0.019021713369505082]
[2024-04-17 13:15:22,705: INFO: main: Training : batch 874 Loss: 0.009574618766389242]
[2024-04-17 13:15:23,338: INFO: main: Training : batch 875 Loss: 0.0035880932568107867]
[2024-04-17 13:15:23,964: INFO: main: Training : batch 876 Loss: 0.01050268921184844]
[2024-04-17 13:15:24,600: INFO: main: Training : batch 877 Loss: 0.0046827189385491835]
[2024-04-17 13:15:25,237: INFO: main: Training : batch 878 Loss: 0.013462565072787603]
[2024-04-17 13:15:25,867: INFO: main: Training : batch 879 Loss: 0.0018536422835416938]
[2024-04-17 13:15:26,507: INFO: main: Training : batch 880 Loss: 0.002456552266486183]
[2024-04-17 13:15:27,145: INFO: main: Training : batch 881 Loss: 0.009079225364490585]
[2024-04-17 13:15:27,778: INFO: main: Training : batch 882 Loss: 0.001781878996573766]
[2024-04-17 13:15:28,406: INFO: main: Training : batch 883 Loss: 0.010849709378016836]
[2024-04-17 13:15:29,040: INFO: main: Training : batch 884 Loss: 0.010208879403754161]
[2024-04-17 13:15:29,670: INFO: main: Training : batch 885 Loss: 0.002090788411578407]
[2024-04-17 13:15:30,304: INFO: main: Training : batch 886 Loss: 0.004583770709012257]
[2024-04-17 13:15:30,936: INFO: main: Training : batch 887 Loss: 0.002253080976596229]
[2024-04-17 13:15:31,566: INFO: main: Training : batch 888 Loss: 0.0012882111831990781]
[2024-04-17 13:15:32,195: INFO: main: Training : batch 889 Loss: 0.0001891749361667393]
[2024-04-17 13:15:32,826: INFO: main: Training : batch 890 Loss: 0.0013512002001509417]
[2024-04-17 13:15:33,458: INFO: main: Training : batch 891 Loss: 0.018643603598719128]
[2024-04-17 13:15:34,089: INFO: main: Training : batch 892 Loss: 0.0017287181145114222]
[2024-04-17 13:15:34,722: INFO: main: Training : batch 893 Loss: 0.010461532914258161]
[2024-04-17 13:15:35,354: INFO: main: Training : batch 894 Loss: 0.0021961917771859455]
[2024-04-17 13:15:35,989: INFO: main: Training : batch 895 Loss: 0.0024177801587520285]
[2024-04-17 13:15:36,620: INFO: main: Training : batch 896 Loss: 0.004459681169810188]
[2024-04-17 13:15:37,254: INFO: main: Training : batch 897 Loss: 0.008146809561691726]
[2024-04-17 13:15:37,899: INFO: main: Training : batch 898 Loss: 0.0027684873215254174]
[2024-04-17 13:15:38,540: INFO: main: Training : batch 899 Loss: 0.0007060954205151121]
[2024-04-17 13:15:39,175: INFO: main: Training : batch 900 Loss: 0.001190320142134643]
[2024-04-17 13:15:39,817: INFO: main: Training : batch 901 Loss: 0.0009269646988928055]
[2024-04-17 13:15:40,450: INFO: main: Training : batch 902 Loss: 0.010318536881999322]
[2024-04-17 13:15:41,083: INFO: main: Training : batch 903 Loss: 0.0054142713537140945]
[2024-04-17 13:15:41,718: INFO: main: Training : batch 904 Loss: 0.004541957820932678]
[2024-04-17 13:15:42,354: INFO: main: Training : batch 905 Loss: 0.00339317710331384]
[2024-04-17 13:15:42,986: INFO: main: Training : batch 906 Loss: 0.006584128327350331]
[2024-04-17 13:15:43,617: INFO: main: Training : batch 907 Loss: 0.005532779030735979]
[2024-04-17 13:15:44,251: INFO: main: Training : batch 908 Loss: 0.0022268671137899206]
[2024-04-17 13:15:44,882: INFO: main: Training : batch 909 Loss: 0.01556885881349422]
[2024-04-17 13:15:45,514: INFO: main: Training : batch 910 Loss: 0.009498846701986278]
[2024-04-17 13:15:46,149: INFO: main: Training : batch 911 Loss: 0.003268474431961917]
[2024-04-17 13:15:46,780: INFO: main: Training : batch 912 Loss: 0.0038111268979610768]
[2024-04-17 13:15:47,414: INFO: main: Training : batch 913 Loss: 0.006183833106412149]
[2024-04-17 13:15:48,046: INFO: main: Training : batch 914 Loss: 0.008768387606072505]
[2024-04-17 13:15:48,678: INFO: main: Training : batch 915 Loss: 0.0042416180936928755]
[2024-04-17 13:15:49,313: INFO: main: Training : batch 916 Loss: 0.000885091450548453]
[2024-04-17 13:15:49,950: INFO: main: Training : batch 917 Loss: 0.0027298477791657945]
[2024-04-17 13:15:50,583: INFO: main: Training : batch 918 Loss: 0.020197237962938243]
[2024-04-17 13:15:51,222: INFO: main: Training : batch 919 Loss: 0.003210350268340611]
[2024-04-17 13:15:51,863: INFO: main: Training : batch 920 Loss: 0.006988986049350589]
[2024-04-17 13:15:52,502: INFO: main: Training : batch 921 Loss: 0.003389904232378368]
[2024-04-17 13:15:53,142: INFO: main: Training : batch 922 Loss: 0.00767833691192504]
[2024-04-17 13:15:53,780: INFO: main: Training : batch 923 Loss: 0.007583521398275747]
[2024-04-17 13:15:54,415: INFO: main: Training : batch 924 Loss: 0.005183966212124563]
[2024-04-17 13:15:55,047: INFO: main: Training : batch 925 Loss: 0.019105109308054476]
[2024-04-17 13:15:55,680: INFO: main: Training : batch 926 Loss: 0.0011876503784101048]
[2024-04-17 13:15:56,311: INFO: main: Training : batch 927 Loss: 0.0032174468669040764]
[2024-04-17 13:15:56,943: INFO: main: Training : batch 928 Loss: 0.016570921860039847]
[2024-04-17 13:15:57,580: INFO: main: Training : batch 929 Loss: 0.005932108402189773]
[2024-04-17 13:15:58,215: INFO: main: Training : batch 930 Loss: 0.0029931123676229456]
[2024-04-17 13:15:58,846: INFO: main: Training : batch 931 Loss: 0.0021501492416840366]
[2024-04-17 13:15:59,480: INFO: main: Training : batch 932 Loss: 0.004331452977886541]
[2024-04-17 13:16:00,114: INFO: main: Training : batch 933 Loss: 0.005733611560745359]
[2024-04-17 13:16:00,745: INFO: main: Training : batch 934 Loss: 0.007119049857433108]
[2024-04-17 13:16:01,375: INFO: main: Training : batch 935 Loss: 0.0023112047516836622]
[2024-04-17 13:16:02,005: INFO: main: Training : batch 936 Loss: 0.009491331130821477]
[2024-04-17 13:16:02,640: INFO: main: Training : batch 937 Loss: 0.0052358440794694]
[2024-04-17 13:16:03,274: INFO: main: Training : batch 938 Loss: 0.009354274435907141]
[2024-04-17 13:16:03,907: INFO: main: Training : batch 939 Loss: 0.003230557238663822]
[2024-04-17 13:16:04,544: INFO: main: Training : batch 940 Loss: 0.007234338135566759]
[2024-04-17 13:16:05,192: INFO: main: Training : batch 941 Loss: 0.00916174344583931]
[2024-04-17 13:16:05,831: INFO: main: Training : batch 942 Loss: 0.0029192216632223846]
[2024-04-17 13:16:06,468: INFO: main: Training : batch 943 Loss: 0.0026262392897725985]
[2024-04-17 13:16:07,110: INFO: main: Training : batch 944 Loss: 0.003972939377265741]
[2024-04-17 13:16:07,748: INFO: main: Training : batch 945 Loss: 0.003395872507322717]
[2024-04-17 13:16:08,383: INFO: main: Training : batch 946 Loss: 0.006398826820517636]
[2024-04-17 13:16:09,012: INFO: main: Training : batch 947 Loss: 0.00038938108897829674]
[2024-04-17 13:16:09,640: INFO: main: Training : batch 948 Loss: 0.0005775919549398608]
[2024-04-17 13:16:10,269: INFO: main: Training : batch 949 Loss: 0.004817444612292101]
[2024-04-17 13:16:10,905: INFO: main: Training : batch 950 Loss: 0.005885939396432315]
[2024-04-17 13:16:11,532: INFO: main: Training : batch 951 Loss: 0.001675207433372257]
[2024-04-17 13:16:12,161: INFO: main: Training : batch 952 Loss: 0.0018512875071909857]
[2024-04-17 13:16:12,791: INFO: main: Training : batch 953 Loss: 0.009064434642915916]
[2024-04-17 13:16:13,420: INFO: main: Training : batch 954 Loss: 0.006922780190279135]
[2024-04-17 13:16:14,049: INFO: main: Training : batch 955 Loss: 0.0034362522447311315]
[2024-04-17 13:16:14,683: INFO: main: Training : batch 956 Loss: 0.009246897521174647]
[2024-04-17 13:16:15,312: INFO: main: Training : batch 957 Loss: 0.002268797497872263]
[2024-04-17 13:16:15,947: INFO: main: Training : batch 958 Loss: 0.010834419598264386]
[2024-04-17 13:16:16,575: INFO: main: Training : batch 959 Loss: 0.009697271465168984]
[2024-04-17 13:16:17,210: INFO: main: Training : batch 960 Loss: 0.00725393496485279]
[2024-04-17 13:16:17,855: INFO: main: Training : batch 961 Loss: 0.0034320952123165892]
[2024-04-17 13:16:18,494: INFO: main: Training : batch 962 Loss: 0.0023268676443728917]
[2024-04-17 13:16:19,130: INFO: main: Training : batch 963 Loss: 0.0012954172223450416]
[2024-04-17 13:16:19,763: INFO: main: Training : batch 964 Loss: 0.006308660589031899]
[2024-04-17 13:16:20,401: INFO: main: Training : batch 965 Loss: 0.005401576970020649]
[2024-04-17 13:16:21,037: INFO: main: Training : batch 966 Loss: 0.0004575094501151473]
[2024-04-17 13:16:21,667: INFO: main: Training : batch 967 Loss: 0.007708429124003105]
[2024-04-17 13:16:22,297: INFO: main: Training : batch 968 Loss: 0.004280030068255102]
[2024-04-17 13:16:22,929: INFO: main: Training : batch 969 Loss: 0.014172138295969463]
[2024-04-17 13:16:23,559: INFO: main: Training : batch 970 Loss: 0.002704347240753576]
[2024-04-17 13:16:24,195: INFO: main: Training : batch 971 Loss: 0.01995274870793378]
[2024-04-17 13:16:24,819: INFO: main: Training : batch 972 Loss: 0.004040217434108507]
[2024-04-17 13:16:25,445: INFO: main: Training : batch 973 Loss: 0.00407898986857383]
[2024-04-17 13:16:26,074: INFO: main: Training : batch 974 Loss: 0.002717014938342095]
[2024-04-17 13:16:26,705: INFO: main: Training : batch 975 Loss: 0.04288723216088904]
[2024-04-17 13:16:27,339: INFO: main: Training : batch 976 Loss: 0.0034272814011418835]
[2024-04-17 13:16:27,970: INFO: main: Training : batch 977 Loss: 0.0029940742969067824]
[2024-04-17 13:16:28,603: INFO: main: Training : batch 978 Loss: 0.009460728341441625]
[2024-04-17 13:16:29,237: INFO: main: Training : batch 979 Loss: 0.011815010558794279]
[2024-04-17 13:16:29,873: INFO: main: Training : batch 980 Loss: 0.008755783447801322]
[2024-04-17 13:16:30,506: INFO: main: Training : batch 981 Loss: 0.0035316885463920603]
[2024-04-17 13:16:31,135: INFO: main: Training : batch 982 Loss: 0.012347016593224609]
[2024-04-17 13:16:31,776: INFO: main: Training : batch 983 Loss: 0.005523215346665824]
[2024-04-17 13:16:32,411: INFO: main: Training : batch 984 Loss: 0.007010678291498017]
[2024-04-17 13:16:33,047: INFO: main: Training : batch 985 Loss: 0.0008258062056602616]
[2024-04-17 13:16:33,688: INFO: main: Training : batch 986 Loss: 0.012666904016508957]
[2024-04-17 13:16:34,327: INFO: main: Training : batch 987 Loss: 0.004709537511000371]
[2024-04-17 13:16:34,960: INFO: main: Training : batch 988 Loss: 0.0030648527205683744]
[2024-04-17 13:16:35,591: INFO: main: Training : batch 989 Loss: 0.008896066212855658]
[2024-04-17 13:16:36,221: INFO: main: Training : batch 990 Loss: 0.0021564022796162472]
[2024-04-17 13:16:36,854: INFO: main: Training : batch 991 Loss: 0.0032055341619281865]
[2024-04-17 13:16:37,489: INFO: main: Training : batch 992 Loss: 0.014020643779965036]
[2024-04-17 13:16:38,122: INFO: main: Training : batch 993 Loss: 0.006261512420953932]
[2024-04-17 13:16:38,756: INFO: main: Training : batch 994 Loss: 0.001915740566464649]
[2024-04-17 13:16:39,387: INFO: main: Training : batch 995 Loss: 0.00876267496503037]
[2024-04-17 13:16:40,017: INFO: main: Training : batch 996 Loss: 0.002972086306250057]
[2024-04-17 13:16:40,646: INFO: main: Training : batch 997 Loss: 0.005865239860653266]
[2024-04-17 13:16:41,281: INFO: main: Training : batch 998 Loss: 0.0034886923445635156]
[2024-04-17 13:16:41,911: INFO: main: Training : batch 999 Loss: 0.0014402923293693134]
[2024-04-17 13:16:42,548: INFO: main: Training : batch 1000 Loss: 0.0032193124824791038]
[2024-04-17 13:16:43,177: INFO: main: Training : batch 1001 Loss: 0.008407706065897304]
[2024-04-17 13:16:43,809: INFO: main: Training : batch 1002 Loss: 0.0031290168864023743]
[2024-04-17 13:16:44,440: INFO: main: Training : batch 1003 Loss: 0.010934077692545067]
[2024-04-17 13:16:45,073: INFO: main: Training : batch 1004 Loss: 0.003404014574195338]
[2024-04-17 13:16:45,710: INFO: main: Training : batch 1005 Loss: 0.0053952047621967135]
[2024-04-17 13:16:46,347: INFO: main: Training : batch 1006 Loss: 0.00400148851848844]
[2024-04-17 13:16:46,982: INFO: main: Training : batch 1007 Loss: 0.011136592696135057]
[2024-04-17 13:16:47,617: INFO: main: Training : batch 1008 Loss: 0.0011181808205191258]
[2024-04-17 13:16:48,246: INFO: main: Training : batch 1009 Loss: 0.0033563597387469887]
[2024-04-17 13:16:48,885: INFO: main: Training : batch 1010 Loss: 0.0021649596883217053]
[2024-04-17 13:16:49,519: INFO: main: Training : batch 1011 Loss: 0.006476807738741938]
[2024-04-17 13:16:50,151: INFO: main: Training : batch 1012 Loss: 0.01404826649271117]
[2024-04-17 13:16:50,782: INFO: main: Training : batch 1013 Loss: 0.0028746534170286985]
[2024-04-17 13:16:51,411: INFO: main: Training : batch 1014 Loss: 0.004469977443158506]
[2024-04-17 13:16:52,047: INFO: main: Training : batch 1015 Loss: 0.012346957151885848]
[2024-04-17 13:16:52,676: INFO: main: Training : batch 1016 Loss: 0.0023256048743612377]
[2024-04-17 13:16:53,303: INFO: main: Training : batch 1017 Loss: 0.0031100919664661334]
[2024-04-17 13:16:53,936: INFO: main: Training : batch 1018 Loss: 0.010112492022117005]
[2024-04-17 13:16:54,568: INFO: main: Training : batch 1019 Loss: 0.003288466938257199]
[2024-04-17 13:16:55,200: INFO: main: Training : batch 1020 Loss: 0.009588361790810743]
[2024-04-17 13:16:55,831: INFO: main: Training : batch 1021 Loss: 0.011912839593058049]
[2024-04-17 13:16:56,462: INFO: main: Training : batch 1022 Loss: 0.015624404421521115]
[2024-04-17 13:16:57,089: INFO: main: Training : batch 1023 Loss: 0.004535747526213927]
[2024-04-17 13:16:57,719: INFO: main: Training : batch 1024 Loss: 0.006676298018836186]
[2024-04-17 13:16:58,355: INFO: main: Training : batch 1025 Loss: 0.0027832157139766767]
[2024-04-17 13:16:58,991: INFO: main: Training : batch 1026 Loss: 0.004460091848019338]
[2024-04-17 13:16:59,625: INFO: main: Training : batch 1027 Loss: 0.0006562202143078977]
[2024-04-17 13:17:00,261: INFO: main: Training : batch 1028 Loss: 0.0003962146523621757]
[2024-04-17 13:17:00,900: INFO: main: Training : batch 1029 Loss: 0.002507203078133099]
[2024-04-17 13:17:01,531: INFO: main: Training : batch 1030 Loss: 0.002376856582463347]
[2024-04-17 13:17:02,168: INFO: main: Training : batch 1031 Loss: 0.004066191977346145]
[2024-04-17 13:17:02,795: INFO: main: Training : batch 1032 Loss: 0.008202719624829605]
[2024-04-17 13:17:03,427: INFO: main: Training : batch 1033 Loss: 0.009390470616333088]
[2024-04-17 13:17:04,058: INFO: main: Training : batch 1034 Loss: 0.007608477322334849]
[2024-04-17 13:17:04,686: INFO: main: Training : batch 1035 Loss: 0.002380671442828431]
[2024-04-17 13:17:05,318: INFO: main: Training : batch 1036 Loss: 0.0033644681779239796]
[2024-04-17 13:17:05,950: INFO: main: Training : batch 1037 Loss: 0.005248455635032333]
[2024-04-17 13:17:06,582: INFO: main: Training : batch 1038 Loss: 0.01021829964445728]
[2024-04-17 13:17:07,215: INFO: main: Training : batch 1039 Loss: 0.0016972305999800685]
[2024-04-17 13:17:07,845: INFO: main: Training : batch 1040 Loss: 0.001041826629527952]
[2024-04-17 13:17:08,472: INFO: main: Training : batch 1041 Loss: 0.00791826852077879]
[2024-04-17 13:17:09,105: INFO: main: Training : batch 1042 Loss: 0.0012589358866231899]
[2024-04-17 13:17:09,734: INFO: main: Training : batch 1043 Loss: 0.0005995375494109789]
[2024-04-17 13:17:10,362: INFO: main: Training : batch 1044 Loss: 0.008808915899126885]
[2024-04-17 13:17:10,995: INFO: main: Training : batch 1045 Loss: 0.0029632914421399764]
[2024-04-17 13:17:11,630: INFO: main: Training : batch 1046 Loss: 0.0055104412899068525]
[2024-04-17 13:17:12,270: INFO: main: Training : batch 1047 Loss: 0.0030978810146786834]
[2024-04-17 13:17:12,906: INFO: main: Training : batch 1048 Loss: 0.015137239180150652]
[2024-04-17 13:17:13,538: INFO: main: Training : batch 1049 Loss: 0.0056780541992349646]
[2024-04-17 13:17:14,179: INFO: main: Training : batch 1050 Loss: 0.004734965474561598]
[2024-04-17 13:17:14,815: INFO: main: Training : batch 1051 Loss: 0.0019403647235012274]
[2024-04-17 13:17:15,445: INFO: main: Training : batch 1052 Loss: 0.010521409286537503]
[2024-04-17 13:17:16,072: INFO: main: Training : batch 1053 Loss: 0.00880180894525328]
[2024-04-17 13:17:16,703: INFO: main: Training : batch 1054 Loss: 0.010316930732530543]
[2024-04-17 13:17:17,333: INFO: main: Training : batch 1055 Loss: 0.006510507688436395]
[2024-04-17 13:17:17,963: INFO: main: Training : batch 1056 Loss: 0.004159335658981114]
[2024-04-17 13:17:18,597: INFO: main: Training : batch 1057 Loss: 0.006868088624184185]
[2024-04-17 13:17:19,229: INFO: main: Training : batch 1058 Loss: 0.00516724593709648]
[2024-04-17 13:17:19,865: INFO: main: Training : batch 1059 Loss: 0.0034565418286953064]
[2024-04-17 13:17:20,498: INFO: main: Training : batch 1060 Loss: 0.002654898623955313]
[2024-04-17 13:17:21,128: INFO: main: Training : batch 1061 Loss: 0.0034685661590682956]
[2024-04-17 13:17:21,760: INFO: main: Training : batch 1062 Loss: 0.0009595428620765842]
[2024-04-17 13:17:22,392: INFO: main: Training : batch 1063 Loss: 0.0035668013608651183]
[2024-04-17 13:17:23,026: INFO: main: Training : batch 1064 Loss: 0.010351212995124062]
[2024-04-17 13:17:23,658: INFO: main: Training : batch 1065 Loss: 0.009269162902410219]
[2024-04-17 13:17:24,288: INFO: main: Training : batch 1066 Loss: 0.0008360019140983648]
[2024-04-17 13:17:24,921: INFO: main: Training : batch 1067 Loss: 0.003176661804049944]
[2024-04-17 13:17:25,564: INFO: main: Training : batch 1068 Loss: 0.0009116336780366349]
[2024-04-17 13:17:26,204: INFO: main: Training : batch 1069 Loss: 0.019686257585358097]
[2024-04-17 13:17:26,850: INFO: main: Training : batch 1070 Loss: 0.008088608669909618]
[2024-04-17 13:17:27,488: INFO: main: Training : batch 1071 Loss: 0.0019019409697297574]
[2024-04-17 13:17:28,131: INFO: main: Training : batch 1072 Loss: 0.002856954035174939]
[2024-04-17 13:17:28,759: INFO: main: Training : batch 1073 Loss: 0.004274969200846198]
[2024-04-17 13:17:29,381: INFO: main: Training : batch 1074 Loss: 0.0024159154417671387]
[2024-04-17 13:17:30,015: INFO: main: Training : batch 1075 Loss: 0.0027316844536176997]
[2024-04-17 13:17:30,643: INFO: main: Training : batch 1076 Loss: 0.003213142647135731]
[2024-04-17 13:17:31,277: INFO: main: Training : batch 1077 Loss: 0.005820894002815979]
[2024-04-17 13:17:31,910: INFO: main: Training : batch 1078 Loss: 0.0031643081752677636]
[2024-04-17 13:17:32,541: INFO: main: Training : batch 1079 Loss: 0.01200087757896003]
[2024-04-17 13:17:33,175: INFO: main: Training : batch 1080 Loss: 0.003517604723894372]
[2024-04-17 13:17:33,812: INFO: main: Training : batch 1081 Loss: 0.004450734982924771]
[2024-04-17 13:17:34,451: INFO: main: Training : batch 1082 Loss: 0.00674052144921867]
[2024-04-17 13:17:35,082: INFO: main: Training : batch 1083 Loss: 0.001953176945778221]
[2024-04-17 13:17:35,715: INFO: main: Training : batch 1084 Loss: 0.0014653066879289516]
[2024-04-17 13:17:36,342: INFO: main: Training : batch 1085 Loss: 0.003437225562421677]
[2024-04-17 13:17:36,971: INFO: main: Training : batch 1086 Loss: 0.01487574369728647]
[2024-04-17 13:17:37,606: INFO: main: Training : batch 1087 Loss: 0.003568841711970325]
[2024-04-17 13:17:38,239: INFO: main: Training : batch 1088 Loss: 0.005985685632098305]
[2024-04-17 13:17:38,880: INFO: main: Training : batch 1089 Loss: 0.00248523872943274]
[2024-04-17 13:17:39,517: INFO: main: Training : batch 1090 Loss: 0.0190364092085162]
[2024-04-17 13:17:40,156: INFO: main: Training : batch 1091 Loss: 0.006749170260569445]
[2024-04-17 13:17:40,792: INFO: main: Training : batch 1092 Loss: 0.006916115357773533]
[2024-04-17 13:17:41,430: INFO: main: Training : batch 1093 Loss: 0.0044849659617617775]
[2024-04-17 13:17:42,064: INFO: main: Training : batch 1094 Loss: 0.0016242749997630694]
[2024-04-17 13:17:42,698: INFO: main: Training : batch 1095 Loss: 0.002474629782397586]
[2024-04-17 13:17:43,330: INFO: main: Training : batch 1096 Loss: 0.013200876184916907]
[2024-04-17 13:17:43,964: INFO: main: Training : batch 1097 Loss: 0.0033958059262603224]
[2024-04-17 13:17:44,599: INFO: main: Training : batch 1098 Loss: 0.009124746165853494]
[2024-04-17 13:17:45,227: INFO: main: Training : batch 1099 Loss: 0.010584069119583385]
[2024-04-17 13:17:45,858: INFO: main: Training : batch 1100 Loss: 0.003322192279871562]
[2024-04-17 13:17:46,489: INFO: main: Training : batch 1101 Loss: 0.0023245524656243517]
[2024-04-17 13:17:47,124: INFO: main: Training : batch 1102 Loss: 0.012288369774387345]
[2024-04-17 13:17:47,757: INFO: main: Training : batch 1103 Loss: 0.0019312327936907815]
[2024-04-17 13:17:48,390: INFO: main: Training : batch 1104 Loss: 0.0028040751119862493]
[2024-04-17 13:17:49,026: INFO: main: Training : batch 1105 Loss: 0.004437916826637899]
[2024-04-17 13:17:49,661: INFO: main: Training : batch 1106 Loss: 0.0037882491296635295]
[2024-04-17 13:17:50,291: INFO: main: Training : batch 1107 Loss: 0.002047811973199227]
[2024-04-17 13:17:50,922: INFO: main: Training : batch 1108 Loss: 0.01593238006137932]
[2024-04-17 13:17:51,551: INFO: main: Training : batch 1109 Loss: 0.003679189621353078]
[2024-04-17 13:17:52,188: INFO: main: Training : batch 1110 Loss: 0.00252457630963656]
[2024-04-17 13:17:52,828: INFO: main: Training : batch 1111 Loss: 0.0014130746756317192]
[2024-04-17 13:17:53,473: INFO: main: Training : batch 1112 Loss: 0.0020175556868213865]
[2024-04-17 13:17:54,108: INFO: main: Training : batch 1113 Loss: 0.0015150608646647462]
[2024-04-17 13:17:54,747: INFO: main: Training : batch 1114 Loss: 0.004019513372265824]
[2024-04-17 13:17:55,380: INFO: main: Training : batch 1115 Loss: 0.0007140684103648104]
[2024-04-17 13:17:56,003: INFO: main: Training : batch 1116 Loss: 0.0058081529382863475]
[2024-04-17 13:17:56,634: INFO: main: Training : batch 1117 Loss: 0.0011304245011717223]
[2024-04-17 13:17:57,263: INFO: main: Training : batch 1118 Loss: 0.0015552592772603705]
[2024-04-17 13:17:57,895: INFO: main: Training : batch 1119 Loss: 0.008054655089584343]
[2024-04-17 13:17:58,531: INFO: main: Training : batch 1120 Loss: 0.010966648193230242]
[2024-04-17 13:17:59,159: INFO: main: Training : batch 1121 Loss: 0.023912846672911468]
[2024-04-17 13:17:59,787: INFO: main: Training : batch 1122 Loss: 0.014810518324913022]
[2024-04-17 13:18:00,418: INFO: main: Training : batch 1123 Loss: 0.0019777932679219327]
[2024-04-17 13:18:01,048: INFO: main: Training : batch 1124 Loss: 0.0031881197533760947]
[2024-04-17 13:18:01,677: INFO: main: Training : batch 1125 Loss: 0.0036205203467934016]
[2024-04-17 13:18:02,309: INFO: main: Training : batch 1126 Loss: 0.0023810879194751393]
[2024-04-17 13:18:02,938: INFO: main: Training : batch 1127 Loss: 0.0032370265418220643]
[2024-04-17 13:18:03,570: INFO: main: Training : batch 1128 Loss: 0.0016973679438579041]
[2024-04-17 13:18:04,201: INFO: main: Training : batch 1129 Loss: 0.006415122085411935]
[2024-04-17 13:18:04,834: INFO: main: Training : batch 1130 Loss: 0.007489286330325434]
[2024-04-17 13:18:05,469: INFO: main: Training : batch 1131 Loss: 0.0039047855694551775]
[2024-04-17 13:18:06,104: INFO: main: Training : batch 1132 Loss: 0.007559658597974259]
[2024-04-17 13:18:06,747: INFO: main: Training : batch 1133 Loss: 0.004394409463120329]
[2024-04-17 13:18:07,380: INFO: main: Training : batch 1134 Loss: 0.00860545610345513]
[2024-04-17 13:18:08,023: INFO: main: Training : batch 1135 Loss: 0.015862146620953227]
[2024-04-17 13:18:08,668: INFO: main: Training : batch 1136 Loss: 0.004175069596631926]
[2024-04-17 13:18:09,300: INFO: main: Training : batch 1137 Loss: 0.005306571437106367]
[2024-04-17 13:18:09,935: INFO: main: Training : batch 1138 Loss: 0.009379020257738884]
[2024-04-17 13:18:10,564: INFO: main: Training : batch 1139 Loss: 0.0019015764468459562]
[2024-04-17 13:18:11,196: INFO: main: Training : batch 1140 Loss: 0.016414666458079485]
[2024-04-17 13:18:11,823: INFO: main: Training : batch 1141 Loss: 0.0027105506410270526]
[2024-04-17 13:18:12,452: INFO: main: Training : batch 1142 Loss: 0.0034907048886043753]
[2024-04-17 13:18:13,083: INFO: main: Training : batch 1143 Loss: 0.0015800274578531052]
[2024-04-17 13:18:13,717: INFO: main: Training : batch 1144 Loss: 0.0031316566911240634]
[2024-04-17 13:18:14,345: INFO: main: Training : batch 1145 Loss: 0.015715667584274173]
[2024-04-17 13:18:14,978: INFO: main: Training : batch 1146 Loss: 0.010007131603837385]
[2024-04-17 13:18:15,609: INFO: main: Training : batch 1147 Loss: 0.002337450564017589]
[2024-04-17 13:18:16,239: INFO: main: Training : batch 1148 Loss: 0.0001944042432375682]
[2024-04-17 13:18:16,872: INFO: main: Training : batch 1149 Loss: 0.002336414110534702]
[2024-04-17 13:18:17,496: INFO: main: Training : batch 1150 Loss: 0.003369462301361103]
[2024-04-17 13:18:18,128: INFO: main: Training : batch 1151 Loss: 0.008685324641221364]
[2024-04-17 13:18:18,760: INFO: main: Training : batch 1152 Loss: 0.012883619692287337]
[2024-04-17 13:18:19,402: INFO: main: Training : batch 1153 Loss: 0.008043991406068844]
[2024-04-17 13:18:20,039: INFO: main: Training : batch 1154 Loss: 0.012363010995081478]
[2024-04-17 13:18:20,680: INFO: main: Training : batch 1155 Loss: 0.010673090614031986]
[2024-04-17 13:18:21,311: INFO: main: Training : batch 1156 Loss: 0.014899674571380474]
[2024-04-17 13:18:21,952: INFO: main: Training : batch 1157 Loss: 0.0034957810452169074]
[2024-04-17 13:18:22,582: INFO: main: Training : batch 1158 Loss: 0.0072003736485849006]
[2024-04-17 13:18:23,211: INFO: main: Training : batch 1159 Loss: 0.0032017625730093304]
[2024-04-17 13:18:23,845: INFO: main: Training : batch 1160 Loss: 0.014419546827828682]
[2024-04-17 13:18:24,475: INFO: main: Training : batch 1161 Loss: 0.01247955653572703]
[2024-04-17 13:18:25,107: INFO: main: Training : batch 1162 Loss: 0.0007523870090745032]
[2024-04-17 13:18:25,740: INFO: main: Training : batch 1163 Loss: 0.002596893529996113]
[2024-04-17 13:18:26,372: INFO: main: Training : batch 1164 Loss: 0.011867705005729047]
[2024-04-17 13:18:27,004: INFO: main: Training : batch 1165 Loss: 0.009313773340163714]
[2024-04-17 13:18:27,633: INFO: main: Training : batch 1166 Loss: 0.00494141052857566]
[2024-04-17 13:18:28,264: INFO: main: Training : batch 1167 Loss: 0.0009161091903721229]
[2024-04-17 13:18:28,897: INFO: main: Training : batch 1168 Loss: 0.010829081899389982]
[2024-04-17 13:18:29,529: INFO: main: Training : batch 1169 Loss: 0.00247415011022752]
[2024-04-17 13:18:30,158: INFO: main: Training : batch 1170 Loss: 0.00745477259366403]
[2024-04-17 13:18:30,788: INFO: main: Training : batch 1171 Loss: 0.00505380511277848]
[2024-04-17 13:18:31,419: INFO: main: Training : batch 1172 Loss: 0.001137567082913826]
[2024-04-17 13:18:32,054: INFO: main: Training : batch 1173 Loss: 0.0015903470350932284]
[2024-04-17 13:18:32,685: INFO: main: Training : batch 1174 Loss: 0.01591012445868465]
[2024-04-17 13:18:33,320: INFO: main: Training : batch 1175 Loss: 0.011418409769468965]
[2024-04-17 13:18:33,966: INFO: main: Training : batch 1176 Loss: 0.010608906822481994]
[2024-04-17 13:18:34,611: INFO: main: Training : batch 1177 Loss: 0.004967477259760758]
[2024-04-17 13:18:35,260: INFO: main: Training : batch 1178 Loss: 0.014382721440169105]
[2024-04-17 13:18:35,891: INFO: main: Training : batch 1179 Loss: 0.008939331720016139]
[2024-04-17 13:18:36,524: INFO: main: Training : batch 1180 Loss: 0.002476207949727104]
[2024-04-17 13:18:37,156: INFO: main: Training : batch 1181 Loss: 0.002225328963166785]
[2024-04-17 13:18:37,790: INFO: main: Training : batch 1182 Loss: 0.003460263149919577]
[2024-04-17 13:18:38,424: INFO: main: Training : batch 1183 Loss: 0.007445763206610333]
[2024-04-17 13:18:39,053: INFO: main: Training : batch 1184 Loss: 0.0033849615900332234]
[2024-04-17 13:18:39,682: INFO: main: Training : batch 1185 Loss: 0.005636169968432647]
[2024-04-17 13:18:40,310: INFO: main: Training : batch 1186 Loss: 0.0014185886603779573]
[2024-04-17 13:18:40,943: INFO: main: Training : batch 1187 Loss: 0.028364202268359954]
[2024-04-17 13:18:41,577: INFO: main: Training : batch 1188 Loss: 0.005890377606642289]
[2024-04-17 13:18:42,205: INFO: main: Training : batch 1189 Loss: 0.004443978754759362]
[2024-04-17 13:18:42,841: INFO: main: Training : batch 1190 Loss: 0.008889349294862008]
[2024-04-17 13:18:43,473: INFO: main: Training : batch 1191 Loss: 0.001533921784005029]
[2024-04-17 13:18:44,110: INFO: main: Training : batch 1192 Loss: 0.003330582556911047]
[2024-04-17 13:18:44,744: INFO: main: Training : batch 1193 Loss: 0.007187708996790277]
[2024-04-17 13:18:45,375: INFO: main: Training : batch 1194 Loss: 0.009233175389920035]
[2024-04-17 13:18:46,019: INFO: main: Training : batch 1195 Loss: 0.00827338810391095]
[2024-04-17 13:18:46,653: INFO: main: Training : batch 1196 Loss: 0.004757416128854622]
[2024-04-17 13:18:47,291: INFO: main: Training : batch 1197 Loss: 0.010453627947553447]
[2024-04-17 13:18:47,928: INFO: main: Training : batch 1198 Loss: 0.004190073660685557]
[2024-04-17 13:18:48,567: INFO: main: Training : batch 1199 Loss: 0.0025972240951114985]
[2024-04-17 13:18:49,204: INFO: main: Training : batch 1200 Loss: 0.017044900096642595]
[2024-04-17 13:18:49,835: INFO: main: Training : batch 1201 Loss: 0.003357416621993684]
[2024-04-17 13:18:50,470: INFO: main: Training : batch 1202 Loss: 0.007653147023140397]
[2024-04-17 13:18:51,104: INFO: main: Training : batch 1203 Loss: 0.002525197924057859]
[2024-04-17 13:18:51,736: INFO: main: Training : batch 1204 Loss: 0.0035801375349005407]
[2024-04-17 13:18:52,360: INFO: main: Training : batch 1205 Loss: 0.007140360445123944]
[2024-04-17 13:18:52,990: INFO: main: Training : batch 1206 Loss: 0.005429378176463955]
[2024-04-17 13:18:53,616: INFO: main: Training : batch 1207 Loss: 0.005310692780082833]
[2024-04-17 13:18:54,246: INFO: main: Training : batch 1208 Loss: 0.0009671533801436624]
[2024-04-17 13:18:54,879: INFO: main: Training : batch 1209 Loss: 0.004071470151303039]
[2024-04-17 13:18:55,512: INFO: main: Training : batch 1210 Loss: 0.003000702744985227]
[2024-04-17 13:18:56,144: INFO: main: Training : batch 1211 Loss: 0.0037007161592160307]
[2024-04-17 13:18:56,776: INFO: main: Training : batch 1212 Loss: 0.003963926485220203]
[2024-04-17 13:18:57,413: INFO: main: Training : batch 1213 Loss: 0.002020383414095022]
[2024-04-17 13:18:58,047: INFO: main: Training : batch 1214 Loss: 0.005446301142378537]
[2024-04-17 13:18:58,679: INFO: main: Training : batch 1215 Loss: 0.0013968226824662246]
[2024-04-17 13:18:59,312: INFO: main: Training : batch 1216 Loss: 0.013533936311234266]
[2024-04-17 13:18:59,947: INFO: main: Training : batch 1217 Loss: 0.030914504803924006]
[2024-04-17 13:19:00,587: INFO: main: Training : batch 1218 Loss: 0.003713060052442248]
[2024-04-17 13:19:01,225: INFO: main: Training : batch 1219 Loss: 0.011717696126531313]
[2024-04-17 13:19:01,864: INFO: main: Training : batch 1220 Loss: 0.01958896824104285]
[2024-04-17 13:19:02,502: INFO: main: Training : batch 1221 Loss: 0.002003051999830636]
[2024-04-17 13:19:03,134: INFO: main: Training : batch 1222 Loss: 0.007014263883603609]
[2024-04-17 13:19:03,765: INFO: main: Training : batch 1223 Loss: 0.010774489369685256]
[2024-04-17 13:19:04,398: INFO: main: Training : batch 1224 Loss: 0.009834354388455722]
[2024-04-17 13:19:05,031: INFO: main: Training : batch 1225 Loss: 0.0012940544240756264]
[2024-04-17 13:19:05,662: INFO: main: Training : batch 1226 Loss: 0.017774227202659806]
[2024-04-17 13:19:06,290: INFO: main: Training : batch 1227 Loss: 0.00246546385060698]
[2024-04-17 13:19:06,920: INFO: main: Training : batch 1228 Loss: 0.007245029971841032]
[2024-04-17 13:19:07,554: INFO: main: Training : batch 1229 Loss: 0.005094133910268586]
[2024-04-17 13:19:08,188: INFO: main: Training : batch 1230 Loss: 0.006095915748462179]
[2024-04-17 13:19:08,820: INFO: main: Training : batch 1231 Loss: 0.006851087655191971]
[2024-04-17 13:19:09,451: INFO: main: Training : batch 1232 Loss: 0.02443633879601257]
[2024-04-17 13:19:10,082: INFO: main: Training : batch 1233 Loss: 0.0014608873293009358]
[2024-04-17 13:19:10,714: INFO: main: Training : batch 1234 Loss: 0.0031379734973572587]
[2024-04-17 13:19:11,346: INFO: main: Training : batch 1235 Loss: 0.005839123006965555]
[2024-04-17 13:19:11,979: INFO: main: Training : batch 1236 Loss: 0.009332216283901493]
[2024-04-17 13:19:12,614: INFO: main: Training : batch 1237 Loss: 0.0006063937317933311]
[2024-04-17 13:19:13,251: INFO: main: Training : batch 1238 Loss: 0.00458031610240088]
[2024-04-17 13:19:13,889: INFO: main: Training : batch 1239 Loss: 0.00480304063063119]
[2024-04-17 13:19:14,524: INFO: main: Training : batch 1240 Loss: 0.0024004538625808074]
[2024-04-17 13:19:15,162: INFO: main: Training : batch 1241 Loss: 0.006384046086428928]
[2024-04-17 13:19:15,798: INFO: main: Training : batch 1242 Loss: 0.001077818531495508]
[2024-04-17 13:19:16,430: INFO: main: Training : batch 1243 Loss: 0.004811056629157383]
[2024-04-17 13:19:17,065: INFO: main: Training : batch 1244 Loss: 0.003789902667607908]
[2024-04-17 13:19:17,696: INFO: main: Training : batch 1245 Loss: 0.005435595927428867]
[2024-04-17 13:19:18,328: INFO: main: Training : batch 1246 Loss: 0.0027569663962644436]
[2024-04-17 13:19:18,963: INFO: main: Training : batch 1247 Loss: 0.0018113462321478618]
[2024-04-17 13:19:19,593: INFO: main: Training : batch 1248 Loss: 0.00026691649212575314]
[2024-04-17 13:19:20,223: INFO: main: Training : batch 1249 Loss: 0.002886194189060471]
[2024-04-17 13:19:20,851: INFO: main: Training : batch 1250 Loss: 0.01200700332737436]
[2024-04-17 13:19:21,487: INFO: main: Training : batch 1251 Loss: 0.002047634359475599]
[2024-04-17 13:19:22,120: INFO: main: Training : batch 1252 Loss: 0.002796114302282025]
[2024-04-17 13:19:22,752: INFO: main: Training : batch 1253 Loss: 0.030583771098767986]
[2024-04-17 13:19:23,387: INFO: main: Training : batch 1254 Loss: 0.0013332891954915198]
[2024-04-17 13:19:24,021: INFO: main: Training : batch 1255 Loss: 0.0029831953672754116]
[2024-04-17 13:19:24,652: INFO: main: Training : batch 1256 Loss: 0.0006562919540473815]
[2024-04-17 13:19:25,281: INFO: main: Training : batch 1257 Loss: 0.014269577672801462]
[2024-04-17 13:19:25,914: INFO: main: Training : batch 1258 Loss: 0.0030563565432884486]
[2024-04-17 13:19:26,556: INFO: main: Training : batch 1259 Loss: 0.008924650354792505]
[2024-04-17 13:19:27,191: INFO: main: Training : batch 1260 Loss: 0.06021613816296783]
[2024-04-17 13:19:27,835: INFO: main: Training : batch 1261 Loss: 0.011076751149477886]
[2024-04-17 13:19:28,472: INFO: main: Training : batch 1262 Loss: 0.0008195189924752479]
[2024-04-17 13:19:29,116: INFO: main: Training : batch 1263 Loss: 0.004310223968001343]
[2024-04-17 13:19:29,748: INFO: main: Training : batch 1264 Loss: 0.002072645401757751]
[2024-04-17 13:19:30,380: INFO: main: Training : batch 1265 Loss: 0.006498150442637195]
[2024-04-17 13:19:31,009: INFO: main: Training : batch 1266 Loss: 0.0037165071001059985]
[2024-04-17 13:19:31,641: INFO: main: Training : batch 1267 Loss: 0.0060196493750576075]
[2024-04-17 13:19:32,271: INFO: main: Training : batch 1268 Loss: 0.005703616968656385]
[2024-04-17 13:19:32,903: INFO: main: Training : batch 1269 Loss: 0.002642342034982723]
[2024-04-17 13:19:33,534: INFO: main: Training : batch 1270 Loss: 0.003100008994731225]
[2024-04-17 13:19:34,164: INFO: main: Training : batch 1271 Loss: 0.0020103080738266776]
[2024-04-17 13:19:34,803: INFO: main: Training : batch 1272 Loss: 0.008352056909810373]
[2024-04-17 13:19:35,436: INFO: main: Training : batch 1273 Loss: 0.003428419051229217]
[2024-04-17 13:19:36,065: INFO: main: Training : batch 1274 Loss: 0.0014708698776320513]
[2024-04-17 13:19:36,702: INFO: main: Training : batch 1275 Loss: 0.006471939689984475]
[2024-04-17 13:19:37,333: INFO: main: Training : batch 1276 Loss: 0.001735784474366412]
[2024-04-17 13:19:37,966: INFO: main: Training : batch 1277 Loss: 0.00031878626014156334]
[2024-04-17 13:19:38,590: INFO: main: Training : batch 1278 Loss: 0.0014990318301252696]
[2024-04-17 13:19:39,225: INFO: main: Training : batch 1279 Loss: 0.0009034630710256638]
[2024-04-17 13:19:39,864: INFO: main: Training : batch 1280 Loss: 0.0014204914013519953]
[2024-04-17 13:19:40,505: INFO: main: Training : batch 1281 Loss: 0.0010688366153204937]
[2024-04-17 13:19:41,153: INFO: main: Training : batch 1282 Loss: 0.003929957963117428]
[2024-04-17 13:19:41,796: INFO: main: Training : batch 1283 Loss: 0.005384544859535803]
[2024-04-17 13:19:42,432: INFO: main: Training : batch 1284 Loss: 0.0031128773474259704]
[2024-04-17 13:19:43,072: INFO: main: Training : batch 1285 Loss: 0.023163180830075257]
[2024-04-17 13:19:43,705: INFO: main: Training : batch 1286 Loss: 0.005740327865606812]
[2024-04-17 13:19:44,335: INFO: main: Training : batch 1287 Loss: 0.0013804734388746112]
[2024-04-17 13:19:44,966: INFO: main: Training : batch 1288 Loss: 0.002348899754541377]
[2024-04-17 13:19:45,599: INFO: main: Training : batch 1289 Loss: 0.015039656729485632]
[2024-04-17 13:19:46,230: INFO: main: Training : batch 1290 Loss: 0.004249900649722194]
[2024-04-17 13:19:46,863: INFO: main: Training : batch 1291 Loss: 0.010985572220025526]
[2024-04-17 13:19:47,496: INFO: main: Training : batch 1292 Loss: 0.016330777242691594]
[2024-04-17 13:19:48,128: INFO: main: Training : batch 1293 Loss: 0.007509329607488813]
[2024-04-17 13:19:48,757: INFO: main: Training : batch 1294 Loss: 0.0034386065699064917]
[2024-04-17 13:19:49,387: INFO: main: Training : batch 1295 Loss: 0.011720144555298262]
[2024-04-17 13:19:50,016: INFO: main: Training : batch 1296 Loss: 0.002203592861151077]
[2024-04-17 13:19:50,649: INFO: main: Training : batch 1297 Loss: 0.00815715407618405]
[2024-04-17 13:19:51,280: INFO: main: Training : batch 1298 Loss: 0.005967061544122504]
[2024-04-17 13:19:51,910: INFO: main: Training : batch 1299 Loss: 0.009073570082370726]
[2024-04-17 13:19:52,541: INFO: main: Training : batch 1300 Loss: 0.010285482714294575]
[2024-04-17 13:19:53,175: INFO: main: Training : batch 1301 Loss: 0.0026734408598000727]
[2024-04-17 13:19:53,816: INFO: main: Training : batch 1302 Loss: 0.007131492802134732]
[2024-04-17 13:19:54,455: INFO: main: Training : batch 1303 Loss: 0.0027250759865970497]
[2024-04-17 13:19:55,093: INFO: main: Training : batch 1304 Loss: 0.007121867938824221]
[2024-04-17 13:19:55,730: INFO: main: Training : batch 1305 Loss: 0.008380760713788548]
[2024-04-17 13:19:56,371: INFO: main: Training : batch 1306 Loss: 0.0008242294775380945]
[2024-04-17 13:19:57,002: INFO: main: Training : batch 1307 Loss: 0.004164189723154706]
[2024-04-17 13:19:57,631: INFO: main: Training : batch 1308 Loss: 0.002742971737639423]
[2024-04-17 13:19:58,258: INFO: main: Training : batch 1309 Loss: 0.005727062652862705]
[2024-04-17 13:19:58,896: INFO: main: Training : batch 1310 Loss: 0.01128443048420863]
[2024-04-17 13:19:59,525: INFO: main: Training : batch 1311 Loss: 0.003275540827842622]
[2024-04-17 13:20:00,162: INFO: main: Training : batch 1312 Loss: 0.0015148080411235742]
[2024-04-17 13:20:00,795: INFO: main: Training : batch 1313 Loss: 0.010508635312834148]
[2024-04-17 13:20:01,428: INFO: main: Training : batch 1314 Loss: 0.0013962839442656018]
[2024-04-17 13:20:02,062: INFO: main: Training : batch 1315 Loss: 0.003877375027749527]
[2024-04-17 13:20:02,690: INFO: main: Training : batch 1316 Loss: 0.008362695530493706]
[2024-04-17 13:20:03,322: INFO: main: Training : batch 1317 Loss: 0.008745566230518656]
[2024-04-17 13:20:03,953: INFO: main: Training : batch 1318 Loss: 0.007248505215330355]
[2024-04-17 13:20:04,584: INFO: main: Training : batch 1319 Loss: 0.01719514550505863]
[2024-04-17 13:20:05,219: INFO: main: Training : batch 1320 Loss: 0.003841274144853784]
[2024-04-17 13:20:05,849: INFO: main: Training : batch 1321 Loss: 0.0037991969372669055]
[2024-04-17 13:20:06,484: INFO: main: Training : batch 1322 Loss: 0.005241927826026465]
[2024-04-17 13:20:07,126: INFO: main: Training : batch 1323 Loss: 0.018077254065844307]
[2024-04-17 13:20:07,765: INFO: main: Training : batch 1324 Loss: 0.017999538806011784]
[2024-04-17 13:20:08,409: INFO: main: Training : batch 1325 Loss: 0.014261703882746966]
[2024-04-17 13:20:09,057: INFO: main: Training : batch 1326 Loss: 0.00445873233757215]
[2024-04-17 13:20:09,697: INFO: main: Training : batch 1327 Loss: 0.0012670806359626797]
[2024-04-17 13:20:10,336: INFO: main: Training : batch 1328 Loss: 0.009570808845799566]
[2024-04-17 13:20:10,967: INFO: main: Training : batch 1329 Loss: 0.0014768846423748698]
[2024-04-17 13:20:11,595: INFO: main: Training : batch 1330 Loss: 0.0061167017752815234]
[2024-04-17 13:20:12,226: INFO: main: Training : batch 1331 Loss: 0.009210038984804672]
[2024-04-17 13:20:12,858: INFO: main: Training : batch 1332 Loss: 0.0004905753980181742]
[2024-04-17 13:20:13,490: INFO: main: Training : batch 1333 Loss: 0.007632833801425231]
[2024-04-17 13:20:14,122: INFO: main: Training : batch 1334 Loss: 0.001731031263676952]
[2024-04-17 13:20:14,752: INFO: main: Training : batch 1335 Loss: 0.002916915824353449]
[2024-04-17 13:20:15,385: INFO: main: Training : batch 1336 Loss: 0.0016420863325867872]
[2024-04-17 13:20:16,015: INFO: main: Training : batch 1337 Loss: 0.0013423141564409863]
[2024-04-17 13:20:16,646: INFO: main: Training : batch 1338 Loss: 0.008617823955453121]
[2024-04-17 13:20:17,278: INFO: main: Training : batch 1339 Loss: 0.004089729456882357]
[2024-04-17 13:20:17,910: INFO: main: Training : batch 1340 Loss: 0.0008310541994332065]
[2024-04-17 13:20:18,543: INFO: main: Training : batch 1341 Loss: 0.007920759469397364]
[2024-04-17 13:20:19,172: INFO: main: Training : batch 1342 Loss: 0.017132122912064972]
[2024-04-17 13:20:19,807: INFO: main: Training : batch 1343 Loss: 0.003911575609532088]
[2024-04-17 13:20:20,442: INFO: main: Training : batch 1344 Loss: 0.0037200398548350192]
[2024-04-17 13:20:21,074: INFO: main: Training : batch 1345 Loss: 0.003998212011403189]
[2024-04-17 13:20:21,712: INFO: main: Training : batch 1346 Loss: 0.003153549855954151]
[2024-04-17 13:20:22,357: INFO: main: Training : batch 1347 Loss: 0.0030493572898577085]
[2024-04-17 13:20:22,995: INFO: main: Training : batch 1348 Loss: 0.005855417341350425]
[2024-04-17 13:20:23,631: INFO: main: Training : batch 1349 Loss: 0.007797152686548876]
[2024-04-17 13:20:24,263: INFO: main: Training : batch 1350 Loss: 0.005007600072286522]
[2024-04-17 13:20:24,887: INFO: main: Training : batch 1351 Loss: 0.01001462754671742]
[2024-04-17 13:20:25,518: INFO: main: Training : batch 1352 Loss: 0.006438245818868937]
[2024-04-17 13:20:26,149: INFO: main: Training : batch 1353 Loss: 0.006592034828107917]
[2024-04-17 13:20:26,782: INFO: main: Training : batch 1354 Loss: 0.00917841240860681]
[2024-04-17 13:20:27,417: INFO: main: Training : batch 1355 Loss: 0.0009114823688490657]
[2024-04-17 13:20:28,049: INFO: main: Training : batch 1356 Loss: 0.0024431177855123738]
[2024-04-17 13:20:28,681: INFO: main: Training : batch 1357 Loss: 0.004724173826280542]
[2024-04-17 13:20:29,317: INFO: main: Training : batch 1358 Loss: 0.01006453409914136]
[2024-04-17 13:20:29,951: INFO: main: Training : batch 1359 Loss: 0.0013474800125511566]
[2024-04-17 13:20:30,584: INFO: main: Training : batch 1360 Loss: 0.01634409069413591]
[2024-04-17 13:20:31,219: INFO: main: Training : batch 1361 Loss: 0.006784790164061415]
[2024-04-17 13:20:31,853: INFO: main: Training : batch 1362 Loss: 0.0036630779778272556]
[2024-04-17 13:20:32,486: INFO: main: Training : batch 1363 Loss: 0.027057006798969996]
[2024-04-17 13:20:33,114: INFO: main: Training : batch 1364 Loss: 0.006894069962678865]
[2024-04-17 13:20:33,746: INFO: main: Training : batch 1365 Loss: 0.007379510634590224]
[2024-04-17 13:20:34,383: INFO: main: Training : batch 1366 Loss: 0.00737064992237084]
[2024-04-17 13:20:35,022: INFO: main: Training : batch 1367 Loss: 0.009512235181575462]
[2024-04-17 13:20:35,665: INFO: main: Training : batch 1368 Loss: 0.0007115283420707261]
[2024-04-17 13:20:36,299: INFO: main: Training : batch 1369 Loss: 0.000874860240717848]
[2024-04-17 13:20:36,940: INFO: main: Training : batch 1370 Loss: 0.008974533121561145]
[2024-04-17 13:20:37,578: INFO: main: Training : batch 1371 Loss: 0.008417518661733308]
[2024-04-17 13:20:38,216: INFO: main: Training : batch 1372 Loss: 0.002415373882834587]
[2024-04-17 13:20:38,846: INFO: main: Training : batch 1373 Loss: 0.010456847850232332]
[2024-04-17 13:20:39,480: INFO: main: Training : batch 1374 Loss: 0.0183130536660856]
[2024-04-17 13:20:40,114: INFO: main: Training : batch 1375 Loss: 0.0002874157934503388]
[2024-04-17 13:20:40,747: INFO: main: Training : batch 1376 Loss: 0.018895949474326403]
[2024-04-17 13:20:41,382: INFO: main: Training : batch 1377 Loss: 0.006613000512804205]
[2024-04-17 13:20:42,013: INFO: main: Training : batch 1378 Loss: 0.018460495799006743]
[2024-04-17 13:20:42,642: INFO: main: Training : batch 1379 Loss: 0.008233854881987326]
[2024-04-17 13:20:43,274: INFO: main: Training : batch 1380 Loss: 0.005101959068600512]
[2024-04-17 13:20:43,909: INFO: main: Training : batch 1381 Loss: 0.013658720065632044]
[2024-04-17 13:20:44,538: INFO: main: Training : batch 1382 Loss: 0.009925066725790395]
[2024-04-17 13:20:45,172: INFO: main: Training : batch 1383 Loss: 0.008234263536311385]
[2024-04-17 13:20:45,794: INFO: main: Training : batch 1384 Loss: 0.0006477101689942777]
[2024-04-17 13:20:46,426: INFO: main: Training : batch 1385 Loss: 0.005988866999873011]
[2024-04-17 13:20:47,058: INFO: main: Training : batch 1386 Loss: 0.0016821642124265008]
[2024-04-17 13:20:47,704: INFO: main: Training : batch 1387 Loss: 0.001774406060645468]
[2024-04-17 13:20:48,341: INFO: main: Training : batch 1388 Loss: 0.0022998576280480864]
[2024-04-17 13:20:48,972: INFO: main: Training : batch 1389 Loss: 0.0012487311988820832]
[2024-04-17 13:20:49,607: INFO: main: Training : batch 1390 Loss: 0.007489488801284547]
[2024-04-17 13:20:50,252: INFO: main: Training : batch 1391 Loss: 0.0019527582238812935]
[2024-04-17 13:20:50,891: INFO: main: Training : batch 1392 Loss: 0.0048062560782960355]
[2024-04-17 13:20:51,520: INFO: main: Training : batch 1393 Loss: 0.01984319433643963]
[2024-04-17 13:20:52,152: INFO: main: Training : batch 1394 Loss: 0.0148122860025281]
[2024-04-17 13:20:52,786: INFO: main: Training : batch 1395 Loss: 0.005854835435410966]
[2024-04-17 13:20:53,413: INFO: main: Training : batch 1396 Loss: 0.003819524094441875]
[2024-04-17 13:20:54,043: INFO: main: Training : batch 1397 Loss: 0.002641618027372742]
[2024-04-17 13:20:54,678: INFO: main: Training : batch 1398 Loss: 0.0036742255506600893]
[2024-04-17 13:20:55,306: INFO: main: Training : batch 1399 Loss: 0.003681729089257516]
[2024-04-17 13:20:55,937: INFO: main: Training : batch 1400 Loss: 0.01174105461546192]
[2024-04-17 13:20:56,573: INFO: main: Training : batch 1401 Loss: 0.011176400557555054]
[2024-04-17 13:20:57,200: INFO: main: Training : batch 1402 Loss: 0.0009821678172423978]
[2024-04-17 13:20:57,831: INFO: main: Training : batch 1403 Loss: 0.02096058180030519]
[2024-04-17 13:20:58,468: INFO: main: Training : batch 1404 Loss: 0.0014891584152311162]
[2024-04-17 13:20:59,099: INFO: main: Training : batch 1405 Loss: 0.004768334256296694]
[2024-04-17 13:20:59,731: INFO: main: Training : batch 1406 Loss: 0.007771041519321294]
[2024-04-17 13:21:00,364: INFO: main: Training : batch 1407 Loss: 0.0012406316604517817]
[2024-04-17 13:21:00,994: INFO: main: Training : batch 1408 Loss: 0.018385279985198597]
[2024-04-17 13:21:01,636: INFO: main: Training : batch 1409 Loss: 0.0038123405956342893]
[2024-04-17 13:21:02,274: INFO: main: Training : batch 1410 Loss: 0.016015633704046062]
[2024-04-17 13:21:02,913: INFO: main: Training : batch 1411 Loss: 0.0084558540947934]
[2024-04-17 13:21:03,550: INFO: main: Training : batch 1412 Loss: 0.0028576579973551086]
[2024-04-17 13:21:04,189: INFO: main: Training : batch 1413 Loss: 0.0012452439731817492]
[2024-04-17 13:21:04,821: INFO: main: Training : batch 1414 Loss: 0.010590868135107975]
[2024-04-17 13:21:05,451: INFO: main: Training : batch 1415 Loss: 0.004338661330547724]
[2024-04-17 13:21:06,081: INFO: main: Training : batch 1416 Loss: 0.01473883826331427]
[2024-04-17 13:21:06,712: INFO: main: Training : batch 1417 Loss: 0.008250612266020798]
[2024-04-17 13:21:07,344: INFO: main: Training : batch 1418 Loss: 0.00142674446432749]
[2024-04-17 13:21:07,975: INFO: main: Training : batch 1419 Loss: 0.0019717981312349386]
[2024-04-17 13:21:08,604: INFO: main: Training : batch 1420 Loss: 0.01627539179454502]
[2024-04-17 13:21:09,230: INFO: main: Training : batch 1421 Loss: 0.004746769584765397]
[2024-04-17 13:21:09,861: INFO: main: Training : batch 1422 Loss: 0.01881196058211141]
[2024-04-17 13:21:10,491: INFO: main: Training : batch 1423 Loss: 0.01156687750152883]
[2024-04-17 13:21:11,117: INFO: main: Training : batch 1424 Loss: 0.002193566315275696]
[2024-04-17 13:21:11,745: INFO: main: Training : batch 1425 Loss: 0.006738454234513904]
[2024-04-17 13:21:12,374: INFO: main: Training : batch 1426 Loss: 0.0034493075875469594]
[2024-04-17 13:21:13,003: INFO: main: Training : batch 1427 Loss: 0.0046316443371746015]
[2024-04-17 13:21:13,642: INFO: main: Training : batch 1428 Loss: 0.01219391256703279]
[2024-04-17 13:21:14,274: INFO: main: Training : batch 1429 Loss: 0.007890203126897493]
[2024-04-17 13:21:14,483: INFO: main: Eval Epoch : batch 0 Loss: 0.0020053724916879114]
[2024-04-17 13:21:14,689: INFO: main: Eval Epoch : batch 1 Loss: 0.0023551649220091286]
[2024-04-17 13:21:14,895: INFO: main: Eval Epoch : batch 2 Loss: 0.03120229538700364]
[2024-04-17 13:21:15,098: INFO: main: Eval Epoch : batch 3 Loss: 0.014850350011613805]
[2024-04-17 13:21:15,310: INFO: main: Eval Epoch : batch 4 Loss: 0.0034805298772879493]
[2024-04-17 13:21:15,516: INFO: main: Eval Epoch : batch 5 Loss: 0.011959656207955631]
[2024-04-17 13:21:15,721: INFO: main: Eval Epoch : batch 6 Loss: 0.007631063087508685]
[2024-04-17 13:21:15,922: INFO: main: Eval Epoch : batch 7 Loss: 0.004038049624822822]
[2024-04-17 13:21:16,124: INFO: main: Eval Epoch : batch 8 Loss: 0.008961916140454104]
[2024-04-17 13:21:16,331: INFO: main: Eval Epoch : batch 9 Loss: 0.026995303437721732]
[2024-04-17 13:21:16,539: INFO: main: Eval Epoch : batch 10 Loss: 0.012589729092445816]
[2024-04-17 13:21:16,749: INFO: main: Eval Epoch : batch 11 Loss: 0.04294990788341819]
[2024-04-17 13:21:16,954: INFO: main: Eval Epoch : batch 12 Loss: 0.012035008175303948]
[2024-04-17 13:21:17,157: INFO: main: Eval Epoch : batch 13 Loss: 0.013593582829241854]
[2024-04-17 13:21:17,371: INFO: main: Eval Epoch : batch 14 Loss: 0.003297755424288708]
[2024-04-17 13:21:17,575: INFO: main: Eval Epoch : batch 15 Loss: 0.020841467387550335]
[2024-04-17 13:21:17,783: INFO: main: Eval Epoch : batch 16 Loss: 0.020744661011392402]
[2024-04-17 13:21:17,986: INFO: main: Eval Epoch : batch 17 Loss: 0.02137291797065563]
[2024-04-17 13:21:18,188: INFO: main: Eval Epoch : batch 18 Loss: 0.014277086679277023]
[2024-04-17 13:21:18,389: INFO: main: Eval Epoch : batch 19 Loss: 0.009821021015667687]
[2024-04-17 13:21:18,594: INFO: main: Eval Epoch : batch 20 Loss: 0.0013657343516061862]
[2024-04-17 13:21:18,797: INFO: main: Eval Epoch : batch 21 Loss: 0.007451555573716483]
[2024-04-17 13:21:18,999: INFO: main: Eval Epoch : batch 22 Loss: 0.011131942959078637]
[2024-04-17 13:21:19,202: INFO: main: Eval Epoch : batch 23 Loss: 0.011810987648195001]
[2024-04-17 13:21:19,405: INFO: main: Eval Epoch : batch 24 Loss: 0.02741407445605232]
[2024-04-17 13:21:19,607: INFO: main: Eval Epoch : batch 25 Loss: 0.006678508224922005]
[2024-04-17 13:21:19,809: INFO: main: Eval Epoch : batch 26 Loss: 0.009358646912122202]
[2024-04-17 13:21:20,011: INFO: main: Eval Epoch : batch 27 Loss: 0.010383562884555128]
[2024-04-17 13:21:20,211: INFO: main: Eval Epoch : batch 28 Loss: 0.00678288741795414]
[2024-04-17 13:21:20,412: INFO: main: Eval Epoch : batch 29 Loss: 0.004744410640850432]
[2024-04-17 13:21:20,614: INFO: main: Eval Epoch : batch 30 Loss: 0.0071752265360591734]
[2024-04-17 13:21:20,816: INFO: main: Eval Epoch : batch 31 Loss: 0.03667081336211428]
[2024-04-17 13:21:21,014: INFO: main: Eval Epoch : batch 32 Loss: 0.011029109921465243]
[2024-04-17 13:21:21,212: INFO: main: Eval Epoch : batch 33 Loss: 0.005815906543548467]
[2024-04-17 13:21:21,411: INFO: main: Eval Epoch : batch 34 Loss: 0.017174917338856125]
[2024-04-17 13:21:21,614: INFO: main: Eval Epoch : batch 35 Loss: 0.01642972703149788]
[2024-04-17 13:21:21,816: INFO: main: Eval Epoch : batch 36 Loss: 0.006616462030606523]
[2024-04-17 13:21:22,021: INFO: main: Eval Epoch : batch 37 Loss: 0.023485985750002714]
[2024-04-17 13:21:22,220: INFO: main: Eval Epoch : batch 38 Loss: 0.008904853488448048]
[2024-04-17 13:21:22,422: INFO: main: Eval Epoch : batch 39 Loss: 0.0034717697596288346]
[2024-04-17 13:21:22,625: INFO: main: Eval Epoch : batch 40 Loss: 0.003210043528045672]
[2024-04-17 13:21:22,828: INFO: main: Eval Epoch : batch 41 Loss: 0.014111979113998755]
[2024-04-17 13:21:23,032: INFO: main: Eval Epoch : batch 42 Loss: 0.001416309293371304]
[2024-04-17 13:21:23,235: INFO: main: Eval Epoch : batch 43 Loss: 0.005705573071156312]
[2024-04-17 13:21:23,435: INFO: main: Eval Epoch : batch 44 Loss: 0.02103435064798643]
[2024-04-17 13:21:23,637: INFO: main: Eval Epoch : batch 45 Loss: 0.004511537529903758]
[2024-04-17 13:21:23,838: INFO: main: Eval Epoch : batch 46 Loss: 0.0073455403294212]
[2024-04-17 13:21:24,041: INFO: main: Eval Epoch : batch 47 Loss: 0.04593493647791866]
[2024-04-17 13:21:24,243: INFO: main: Eval Epoch : batch 48 Loss: 0.023790480866147427]
[2024-04-17 13:21:24,441: INFO: main: Eval Epoch : batch 49 Loss: 0.011404231492795522]
[2024-04-17 13:21:24,638: INFO: main: Eval Epoch : batch 50 Loss: 0.026132245976031483]
[2024-04-17 13:21:24,840: INFO: main: Eval Epoch : batch 51 Loss: 0.04969983958590772]
[2024-04-17 13:21:25,040: INFO: main: Eval Epoch : batch 52 Loss: 0.0018946603463681252]
[2024-04-17 13:21:25,241: INFO: main: Eval Epoch : batch 53 Loss: 0.009078155340928346]
[2024-04-17 13:21:25,444: INFO: main: Eval Epoch : batch 54 Loss: 0.00966072455073064]
[2024-04-17 13:21:25,645: INFO: main: Eval Epoch : batch 55 Loss: 0.02522341851709207]
[2024-04-17 13:21:25,847: INFO: main: Eval Epoch : batch 56 Loss: 0.0011859160984590558]
[2024-04-17 13:21:26,049: INFO: main: Eval Epoch : batch 57 Loss: 0.006217706603012284]
[2024-04-17 13:21:26,249: INFO: main: Eval Epoch : batch 58 Loss: 0.000256511725478625]
[2024-04-17 13:21:26,448: INFO: main: Eval Epoch : batch 59 Loss: 0.04608101630557125]
[2024-04-17 13:21:26,646: INFO: main: Eval Epoch : batch 60 Loss: 0.005544455156843225]
[2024-04-17 13:21:26,846: INFO: main: Eval Epoch : batch 61 Loss: 0.006810843712194988]
[2024-04-17 13:21:27,048: INFO: main: Eval Epoch : batch 62 Loss: 0.00493879918544708]
[2024-04-17 13:21:27,251: INFO: main: Eval Epoch : batch 63 Loss: 0.0018333576786673184]
[2024-04-17 13:21:27,453: INFO: main: Eval Epoch : batch 64 Loss: 0.004698189494062966]
[2024-04-17 13:21:27,658: INFO: main: Eval Epoch : batch 65 Loss: 0.011323796955978195]
[2024-04-17 13:21:27,860: INFO: main: Eval Epoch : batch 66 Loss: 0.00032495924517018243]
[2024-04-17 13:21:28,060: INFO: main: Eval Epoch : batch 67 Loss: 0.014052793694646515]
[2024-04-17 13:21:28,267: INFO: main: Eval Epoch : batch 68 Loss: 0.05402900983426766]
[2024-04-17 13:21:28,474: INFO: main: Eval Epoch : batch 69 Loss: 0.011472391949006268]
[2024-04-17 13:21:28,683: INFO: main: Eval Epoch : batch 70 Loss: 0.0008184927252208897]
[2024-04-17 13:21:28,892: INFO: main: Eval Epoch : batch 71 Loss: 0.005144587633005833]
[2024-04-17 13:21:29,102: INFO: main: Eval Epoch : batch 72 Loss: 0.013994838221019731]
[2024-04-17 13:21:29,310: INFO: main: Eval Epoch : batch 73 Loss: 0.006976139841224915]
[2024-04-17 13:21:29,517: INFO: main: Eval Epoch : batch 74 Loss: 0.006046157574185047]
[2024-04-17 13:21:29,728: INFO: main: Eval Epoch : batch 75 Loss: 0.01027802294283923]
[2024-04-17 13:21:29,932: INFO: main: Eval Epoch : batch 76 Loss: 0.00201676313458557]
[2024-04-17 13:21:30,137: INFO: main: Eval Epoch : batch 77 Loss: 0.0006600195446996724]
[2024-04-17 13:21:30,342: INFO: main: Eval Epoch : batch 78 Loss: 0.0197945508041962]
[2024-04-17 13:21:30,550: INFO: main: Eval Epoch : batch 79 Loss: 0.016528136419073823]
[2024-04-17 13:21:30,756: INFO: main: Eval Epoch : batch 80 Loss: 0.014878107424322546]
[2024-04-17 13:21:30,961: INFO: main: Eval Epoch : batch 81 Loss: 0.014550146802570548]
[2024-04-17 13:21:31,168: INFO: main: Eval Epoch : batch 82 Loss: 0.002104976250228133]
[2024-04-17 13:21:31,371: INFO: main: Eval Epoch : batch 83 Loss: 0.009067739241406785]
[2024-04-17 13:21:31,577: INFO: main: Eval Epoch : batch 84 Loss: 0.0002653523849165365]
[2024-04-17 13:21:31,782: INFO: main: Eval Epoch : batch 85 Loss: 0.02394938972664898]
[2024-04-17 13:21:31,985: INFO: main: Eval Epoch : batch 86 Loss: 0.017627720135213153]
[2024-04-17 13:21:32,188: INFO: main: Eval Epoch : batch 87 Loss: 0.005430991770362658]
[2024-04-17 13:21:32,392: INFO: main: Eval Epoch : batch 88 Loss: 0.030175560959469036]
[2024-04-17 13:21:32,597: INFO: main: Eval Epoch : batch 89 Loss: 0.016853166034141106]
[2024-04-17 13:21:32,796: INFO: main: Eval Epoch : batch 90 Loss: 0.020075178204513443]
[2024-04-17 13:21:32,997: INFO: main: Eval Epoch : batch 91 Loss: 0.0034031130425140107]
[2024-04-17 13:21:33,202: INFO: main: Eval Epoch : batch 92 Loss: 0.00023768588643829322]
[2024-04-17 13:21:33,403: INFO: main: Eval Epoch : batch 93 Loss: 0.007786696083060063]
[2024-04-17 13:21:33,602: INFO: main: Eval Epoch : batch 94 Loss: 0.017962081596338032]
[2024-04-17 13:21:33,804: INFO: main: Eval Epoch : batch 95 Loss: 0.011971484807656732]
[2024-04-17 13:21:34,005: INFO: main: Eval Epoch : batch 96 Loss: 0.008287288212015607]
[2024-04-17 13:21:34,208: INFO: main: Eval Epoch : batch 97 Loss: 0.009260332960049224]
[2024-04-17 13:21:34,410: INFO: main: Eval Epoch : batch 98 Loss: 0.0018859286590492729]
[2024-04-17 13:21:34,611: INFO: main: Eval Epoch : batch 99 Loss: 0.0032593306808800997]
[2024-04-17 13:21:34,814: INFO: main: Eval Epoch : batch 100 Loss: 0.031491572913517084]
[2024-04-17 13:21:35,015: INFO: main: Eval Epoch : batch 101 Loss: 0.011349437419887533]
[2024-04-17 13:21:35,217: INFO: main: Eval Epoch : batch 102 Loss: 0.00397615609201969]
[2024-04-17 13:21:35,419: INFO: main: Eval Epoch : batch 103 Loss: 0.003914856387357576]
[2024-04-17 13:21:35,621: INFO: main: Eval Epoch : batch 104 Loss: 0.01310158168385309]
[2024-04-17 13:21:35,822: INFO: main: Eval Epoch : batch 105 Loss: 0.00037792799564753134]
[2024-04-17 13:21:36,022: INFO: main: Eval Epoch : batch 106 Loss: 0.006017014340809652]
[2024-04-17 13:21:36,225: INFO: main: Eval Epoch : batch 107 Loss: 0.0026055390729078894]
[2024-04-17 13:21:36,430: INFO: main: Eval Epoch : batch 108 Loss: 0.02485080186654776]
[2024-04-17 13:21:36,630: INFO: main: Eval Epoch : batch 109 Loss: 0.022047638354988273]
[2024-04-17 13:21:36,832: INFO: main: Eval Epoch : batch 110 Loss: 0.016978324986912593]
[2024-04-17 13:21:37,031: INFO: main: Eval Epoch : batch 111 Loss: 0.002113352424334923]
[2024-04-17 13:21:37,233: INFO: main: Eval Epoch : batch 112 Loss: 0.017881984891549702]
[2024-04-17 13:21:37,432: INFO: main: Eval Epoch : batch 113 Loss: 0.0023564078465873155]
[2024-04-17 13:21:37,632: INFO: main: Eval Epoch : batch 114 Loss: 0.024186851378628072]
[2024-04-17 13:21:37,831: INFO: main: Eval Epoch : batch 115 Loss: 0.011407417846786798]
[2024-04-17 13:21:38,034: INFO: main: Eval Epoch : batch 116 Loss: 0.019011353123002256]
[2024-04-17 13:21:38,236: INFO: main: Eval Epoch : batch 117 Loss: 0.0018850503250492957]
[2024-04-17 13:21:38,443: INFO: main: Eval Epoch : batch 118 Loss: 0.005882099727264808]
[2024-04-17 13:21:38,645: INFO: main: Eval Epoch : batch 119 Loss: 0.024291480695856276]
[2024-04-17 13:21:38,849: INFO: main: Eval Epoch : batch 120 Loss: 0.0036709916545163007]
[2024-04-17 13:21:39,048: INFO: main: Eval Epoch : batch 121 Loss: 0.006941937061257299]
[2024-04-17 13:21:39,251: INFO: main: Eval Epoch : batch 122 Loss: 0.014074102883772007]
[2024-04-17 13:21:39,456: INFO: main: Eval Epoch : batch 123 Loss: 0.01749737860616177]
[2024-04-17 13:21:39,657: INFO: main: Eval Epoch : batch 124 Loss: 0.001981759513760469]
[2024-04-17 13:21:39,853: INFO: main: Eval Epoch : batch 125 Loss: 0.0037257516475873134]
[2024-04-17 13:21:40,053: INFO: main: Eval Epoch : batch 126 Loss: 0.002411019311518564]
[2024-04-17 13:21:40,259: INFO: main: Eval Epoch : batch 127 Loss: 0.0052860477671677845]
[2024-04-17 13:21:40,462: INFO: main: Eval Epoch : batch 128 Loss: 0.008823202893114399]
[2024-04-17 13:21:40,666: INFO: main: Eval Epoch : batch 129 Loss: 0.008830901613312648]
[2024-04-17 13:21:40,862: INFO: main: Eval Epoch : batch 130 Loss: 0.01573687974968518]
[2024-04-17 13:21:41,063: INFO: main: Eval Epoch : batch 131 Loss: 0.0134277731864545]
[2024-04-17 13:21:41,268: INFO: main: Eval Epoch : batch 132 Loss: 0.003687860079784183]
[2024-04-17 13:21:41,469: INFO: main: Eval Epoch : batch 133 Loss: 0.008984684699955269]
[2024-04-17 13:21:41,672: INFO: main: Eval Epoch : batch 134 Loss: 0.01379309250646329]
[2024-04-17 13:21:41,871: INFO: main: Eval Epoch : batch 135 Loss: 0.0016347400730183218]
[2024-04-17 13:21:42,080: INFO: main: Eval Epoch : batch 136 Loss: 0.021353548518587916]
[2024-04-17 13:21:42,285: INFO: main: Eval Epoch : batch 137 Loss: 0.018027915248526667]
[2024-04-17 13:21:42,492: INFO: main: Eval Epoch : batch 138 Loss: 0.015202389620808462]
[2024-04-17 13:21:42,698: INFO: main: Eval Epoch : batch 139 Loss: 0.09333667053028671]
[2024-04-17 13:21:42,906: INFO: main: Eval Epoch : batch 140 Loss: 0.03568954523408236]
[2024-04-17 13:21:43,112: INFO: main: Eval Epoch : batch 141 Loss: 0.01569419694946699]
[2024-04-17 13:21:43,316: INFO: main: Eval Epoch : batch 142 Loss: 0.00287343079075084]
[2024-04-17 13:21:43,528: INFO: main: Eval Epoch : batch 143 Loss: 0.02534315290576519]
[2024-04-17 13:21:43,733: INFO: main: Eval Epoch : batch 144 Loss: 0.010968324277160784]
[2024-04-17 13:21:43,935: INFO: main: Eval Epoch : batch 145 Loss: 0.007720062062859456]
[2024-04-17 13:21:44,140: INFO: main: Eval Epoch : batch 146 Loss: 0.010093767048949034]
[2024-04-17 13:21:44,345: INFO: main: Eval Epoch : batch 147 Loss: 0.03636770174259734]
[2024-04-17 13:21:44,552: INFO: main: Eval Epoch : batch 148 Loss: 0.01497247555648115]
[2024-04-17 13:21:44,757: INFO: main: Eval Epoch : batch 149 Loss: 0.008096721613131234]
[2024-04-17 13:21:44,966: INFO: main: Eval Epoch : batch 150 Loss: 0.04331021859659623]
[2024-04-17 13:21:45,170: INFO: main: Eval Epoch : batch 151 Loss: 0.042646836592084354]
[2024-04-17 13:21:45,376: INFO: main: Eval Epoch : batch 152 Loss: 0.00039627033122353195]
[2024-04-17 13:21:45,584: INFO: main: Eval Epoch : batch 153 Loss: 0.0016507727266542567]
[2024-04-17 13:21:45,785: INFO: main: Eval Epoch : batch 154 Loss: 0.005470823125488861]
[2024-04-17 13:21:45,988: INFO: main: Eval Epoch : batch 155 Loss: 0.050473549150098065]
[2024-04-17 13:21:46,186: INFO: main: Eval Epoch : batch 156 Loss: 0.01469189486241001]
[2024-04-17 13:21:46,388: INFO: main: Eval Epoch : batch 157 Loss: 0.00848239727814322]
[2024-04-17 13:21:46,591: INFO: main: Eval Epoch : batch 158 Loss: 0.011452754291786915]
[2024-04-17 13:21:46,798: INFO: main: Eval Epoch : batch 159 Loss: 0.004714472340906209]
[2024-04-17 13:21:46,999: INFO: main: Eval Epoch : batch 160 Loss: 0.004886354577945577]
[2024-04-17 13:21:47,196: INFO: main: Eval Epoch : batch 161 Loss: 0.020528934252988035]
[2024-04-17 13:21:47,397: INFO: main: Eval Epoch : batch 162 Loss: 0.008820834731098478]
[2024-04-17 13:21:47,600: INFO: main: Eval Epoch : batch 163 Loss: 0.007508253128728141]
[2024-04-17 13:21:47,806: INFO: main: Eval Epoch : batch 164 Loss: 0.03810917149135556]
[2024-04-17 13:21:48,010: INFO: main: Eval Epoch : batch 165 Loss: 0.013414525507119999]
[2024-04-17 13:21:48,210: INFO: main: Eval Epoch : batch 166 Loss: 0.014476849199893883]
[2024-04-17 13:21:48,408: INFO: main: Eval Epoch : batch 167 Loss: 0.002297230062737299]
[2024-04-17 13:21:48,611: INFO: main: Eval Epoch : batch 168 Loss: 0.015779612172291724]
[2024-04-17 13:21:48,813: INFO: main: Eval Epoch : batch 169 Loss: 0.009942125124152733]
[2024-04-17 13:21:49,020: INFO: main: Eval Epoch : batch 170 Loss: 0.006099045196542756]
[2024-04-17 13:21:49,220: INFO: main: Eval Epoch : batch 171 Loss: 0.0027109874669197726]
[2024-04-17 13:21:49,422: INFO: main: Eval Epoch : batch 172 Loss: 0.012712803180678396]
[2024-04-17 13:21:49,621: INFO: main: Eval Epoch : batch 173 Loss: 0.0020604709133588825]
[2024-04-17 13:21:49,821: INFO: main: Eval Epoch : batch 174 Loss: 0.00842721220607197]
[2024-04-17 13:21:50,025: INFO: main: Eval Epoch : batch 175 Loss: 0.0033687901701995823]
[2024-04-17 13:21:50,228: INFO: main: Eval Epoch : batch 176 Loss: 0.005374000128880995]
[2024-04-17 13:21:50,427: INFO: main: Eval Epoch : batch 177 Loss: 0.0002965543619930666]
[2024-04-17 13:21:50,629: INFO: main: Eval Epoch : batch 178 Loss: 0.001171494145483235]
[2024-04-17 13:21:50,829: INFO: main: Eval Epoch : batch 179 Loss: 0.00023196927331752573]
[2024-04-17 13:21:51,034: INFO: main: Eval Epoch : batch 180 Loss: 0.012208393934519064]
[2024-04-17 13:21:51,236: INFO: main: Eval Epoch : batch 181 Loss: 0.0027196898529645206]
[2024-04-17 13:21:51,435: INFO: main: Eval Epoch : batch 182 Loss: 0.010188228941063854]
[2024-04-17 13:21:51,637: INFO: main: Eval Epoch : batch 183 Loss: 0.02149296325140996]
[2024-04-17 13:21:51,839: INFO: main: Eval Epoch : batch 184 Loss: 0.020486385028747854]
[2024-04-17 13:21:52,042: INFO: main: Eval Epoch : batch 185 Loss: 0.002682270988756833]
[2024-04-17 13:21:52,244: INFO: main: Eval Epoch : batch 186 Loss: 0.004257222824491581]
[2024-04-17 13:21:52,446: INFO: main: Eval Epoch : batch 187 Loss: 0.06506190693895506]
[2024-04-17 13:21:52,646: INFO: main: Eval Epoch : batch 188 Loss: 0.005409831598753626]
[2024-04-17 13:21:52,845: INFO: main: Eval Epoch : batch 189 Loss: 0.013595146023777356]
[2024-04-17 13:21:53,047: INFO: main: Eval Epoch : batch 190 Loss: 0.0030415927886886557]
[2024-04-17 13:21:53,250: INFO: main: Eval Epoch : batch 191 Loss: 0.01723340805820311]
[2024-04-17 13:21:53,451: INFO: main: Eval Epoch : batch 192 Loss: 0.001388251334157886]
[2024-04-17 13:21:53,653: INFO: main: Eval Epoch : batch 193 Loss: 0.007071458857212741]
[2024-04-17 13:21:53,854: INFO: main: Eval Epoch : batch 194 Loss: 0.00987835409115287]
[2024-04-17 13:21:54,058: INFO: main: Eval Epoch : batch 195 Loss: 0.012457876800391818]
[2024-04-17 13:21:54,260: INFO: main: Eval Epoch : batch 196 Loss: 0.004246450310737907]
[2024-04-17 13:21:54,462: INFO: main: Eval Epoch : batch 197 Loss: 0.012734843222899705]
[2024-04-17 13:21:54,664: INFO: main: Eval Epoch : batch 198 Loss: 0.003352467750026753]
[2024-04-17 13:21:54,863: INFO: main: Eval Epoch : batch 199 Loss: 0.005383891138400019]
[2024-04-17 13:21:55,067: INFO: main: Eval Epoch : batch 200 Loss: 0.0019396158635670094]
[2024-04-17 13:21:55,271: INFO: main: Eval Epoch : batch 201 Loss: 0.02640378026858897]
[2024-04-17 13:21:55,475: INFO: main: Eval Epoch : batch 202 Loss: 0.008630009026113631]
[2024-04-17 13:21:55,676: INFO: main: Eval Epoch : batch 203 Loss: 0.0027141596173559874]
[2024-04-17 13:21:55,882: INFO: main: Eval Epoch : batch 204 Loss: 0.012247067588134963]
[2024-04-17 13:21:56,085: INFO: main: Eval Epoch : batch 205 Loss: 0.006301133810101828]
[2024-04-17 13:21:56,296: INFO: main: Eval Epoch : batch 206 Loss: 0.00023992697318137973]
[2024-04-17 13:21:56,505: INFO: main: Eval Epoch : batch 207 Loss: 0.0005129856305437362]
[2024-04-17 13:21:56,710: INFO: main: Eval Epoch : batch 208 Loss: 0.0005879110614504387]
[2024-04-17 13:21:56,916: INFO: main: Eval Epoch : batch 209 Loss: 0.001809996793994103]
[2024-04-17 13:21:57,120: INFO: main: Eval Epoch : batch 210 Loss: 0.006632256742820156]
[2024-04-17 13:21:57,329: INFO: main: Eval Epoch : batch 211 Loss: 0.01219198528851899]
[2024-04-17 13:21:57,541: INFO: main: Eval Epoch : batch 212 Loss: 0.005623611348678936]
[2024-04-17 13:21:57,745: INFO: main: Eval Epoch : batch 213 Loss: 0.004577921717519444]
[2024-04-17 13:21:57,948: INFO: main: Eval Epoch : batch 214 Loss: 0.00645743784080151]
[2024-04-17 13:21:58,159: INFO: main: Eval Epoch : batch 215 Loss: 0.021069418637040614]
[2024-04-17 13:21:58,367: INFO: main: Eval Epoch : batch 216 Loss: 0.013430827874631327]
[2024-04-17 13:21:58,575: INFO: main: Eval Epoch : batch 217 Loss: 0.005477945981148331]
[2024-04-17 13:21:58,782: INFO: main: Eval Epoch : batch 218 Loss: 0.016687088312186762]
[2024-04-17 13:21:58,989: INFO: main: Eval Epoch : batch 219 Loss: 0.0011084708525041943]
[2024-04-17 13:21:59,194: INFO: main: Eval Epoch : batch 220 Loss: 0.0017183484248807712]
[2024-04-17 13:21:59,401: INFO: main: Eval Epoch : batch 221 Loss: 0.023692822934287767]
[2024-04-17 13:21:59,604: INFO: main: Eval Epoch : batch 222 Loss: 3.538487409545034e-05]
[2024-04-17 13:21:59,805: INFO: main: Eval Epoch : batch 223 Loss: 0.02184286531919581]
[2024-04-17 13:22:00,007: INFO: main: Eval Epoch : batch 224 Loss: 0.00026185978938645505]
[2024-04-17 13:22:00,210: INFO: main: Eval Epoch : batch 225 Loss: 0.017545938174887258]
[2024-04-17 13:22:00,409: INFO: main: Eval Epoch : batch 226 Loss: 0.03547287508714947]
[2024-04-17 13:22:00,611: INFO: main: Eval Epoch : batch 227 Loss: 0.012212287341523513]
[2024-04-17 13:22:00,813: INFO: main: Eval Epoch : batch 228 Loss: 0.0013974012224312069]
[2024-04-17 13:22:01,017: INFO: main: Eval Epoch : batch 229 Loss: 0.0026475532121562266]
[2024-04-17 13:22:01,218: INFO: main: Eval Epoch : batch 230 Loss: 0.0016306615841248867]
[2024-04-17 13:22:01,419: INFO: main: Eval Epoch : batch 231 Loss: 0.014088003773129541]
[2024-04-17 13:22:01,620: INFO: main: Eval Epoch : batch 232 Loss: 0.018125848832072213]
[2024-04-17 13:22:01,824: INFO: main: Eval Epoch : batch 233 Loss: 0.010267377979096014]
[2024-04-17 13:22:02,029: INFO: main: Eval Epoch : batch 234 Loss: 0.024493895772841355]
[2024-04-17 13:22:02,230: INFO: main: Eval Epoch : batch 235 Loss: 0.00506677302532859]
[2024-04-17 13:22:02,434: INFO: main: Eval Epoch : batch 236 Loss: 0.0282296536779786]
[2024-04-17 13:22:02,633: INFO: main: Eval Epoch : batch 237 Loss: 0.00015479794295678213]
[2024-04-17 13:22:02,833: INFO: main: Eval Epoch : batch 238 Loss: 0.004392002439894473]
[2024-04-17 13:22:03,037: INFO: main: Eval Epoch : batch 239 Loss: 0.03755416697923063]
[2024-04-17 13:22:03,242: INFO: main: Eval Epoch : batch 240 Loss: 0.00811723842728745]
[2024-04-17 13:22:03,445: INFO: main: Eval Epoch : batch 241 Loss: 0.010507554736831643]
[2024-04-17 13:22:03,644: INFO: main: Eval Epoch : batch 242 Loss: 0.03134393461292649]
[2024-04-17 13:22:03,847: INFO: main: Eval Epoch : batch 243 Loss: 0.005644387026126002]
[2024-04-17 13:22:04,051: INFO: main: Eval Epoch : batch 244 Loss: 0.0017692181958580309]
[2024-04-17 13:22:04,255: INFO: main: Eval Epoch : batch 245 Loss: 0.007794655591271939]
[2024-04-17 13:22:04,456: INFO: main: Eval Epoch : batch 246 Loss: 0.0027118161302298114]
[2024-04-17 13:22:04,656: INFO: main: Eval Epoch : batch 247 Loss: 0.011389187166843517]
[2024-04-17 13:22:04,859: INFO: main: Eval Epoch : batch 248 Loss: 0.0035976528341225555]
[2024-04-17 13:22:05,066: INFO: main: Eval Epoch : batch 249 Loss: 0.018330680588860306]
[2024-04-17 13:22:05,270: INFO: main: Eval Epoch : batch 250 Loss: 0.00047892001904274733]
[2024-04-17 13:22:05,479: INFO: main: Eval Epoch : batch 251 Loss: 0.0028023818808281795]
[2024-04-17 13:22:05,680: INFO: main: Eval Epoch : batch 252 Loss: 0.12661930812757627]
[2024-04-17 13:22:05,885: INFO: main: Eval Epoch : batch 253 Loss: 0.03204555190824898]
[2024-04-17 13:22:06,092: INFO: main: Eval Epoch : batch 254 Loss: 0.004420515912405741]
[2024-04-17 13:22:06,297: INFO: main: Eval Epoch : batch 255 Loss: 0.016502001497434117]
[2024-04-17 13:22:06,499: INFO: main: Eval Epoch : batch 256 Loss: 0.013737929601356209]
[2024-04-17 13:22:06,703: INFO: main: Eval Epoch : batch 257 Loss: 0.039634031947997986]
[2024-04-17 13:22:06,905: INFO: main: Eval Epoch : batch 258 Loss: 0.04430661499460506]
[2024-04-17 13:22:07,110: INFO: main: Eval Epoch : batch 259 Loss: 0.00811504205972147]
[2024-04-17 13:22:07,312: INFO: main: Eval Epoch : batch 260 Loss: 0.03456456115314657]
[2024-04-17 13:22:07,512: INFO: main: Eval Epoch : batch 261 Loss: 0.004616727037789169]
[2024-04-17 13:22:07,712: INFO: main: Eval Epoch : batch 262 Loss: 0.010097102392002509]
[2024-04-17 13:22:07,916: INFO: main: Eval Epoch : batch 263 Loss: 0.006702858230217575]
[2024-04-17 13:22:08,122: INFO: main: Eval Epoch : batch 264 Loss: 0.05246413837301294]
[2024-04-17 13:22:08,325: INFO: main: Eval Epoch : batch 265 Loss: 0.05480839673935053]
[2024-04-17 13:22:08,526: INFO: main: Eval Epoch : batch 266 Loss: 0.006689666797753375]
[2024-04-17 13:22:08,728: INFO: main: Eval Epoch : batch 267 Loss: 0.0029327722318487712]
[2024-04-17 13:22:08,930: INFO: main: Eval Epoch : batch 268 Loss: 0.010067626081001821]
[2024-04-17 13:22:09,133: INFO: main: Eval Epoch : batch 269 Loss: 0.019366095681866625]
[2024-04-17 13:22:09,337: INFO: main: Eval Epoch : batch 270 Loss: 0.021110573051441234]
[2024-04-17 13:22:09,545: INFO: main: Eval Epoch : batch 271 Loss: 0.0034155778807806723]
[2024-04-17 13:22:09,750: INFO: main: Eval Epoch : batch 272 Loss: 0.0204080678726783]
[2024-04-17 13:22:09,954: INFO: main: Eval Epoch : batch 273 Loss: 0.00044327090440717356]
[2024-04-17 13:22:10,159: INFO: main: Eval Epoch : batch 274 Loss: 0.0015308147925624453]
[2024-04-17 13:22:10,369: INFO: main: Eval Epoch : batch 275 Loss: 0.00037119839139575553]
[2024-04-17 13:22:10,579: INFO: main: Eval Epoch : batch 276 Loss: 0.009019868094768726]
[2024-04-17 13:22:10,791: INFO: main: Eval Epoch : batch 277 Loss: 0.00864306208441732]
[2024-04-17 13:22:10,997: INFO: main: Eval Epoch : batch 278 Loss: 0.022273987067426004]
[2024-04-17 13:22:11,202: INFO: main: Eval Epoch : batch 279 Loss: 0.0012186962529357084]
[2024-04-17 13:22:11,405: INFO: main: Eval Epoch : batch 280 Loss: 0.005953503365012259]
[2024-04-17 13:22:11,616: INFO: main: Eval Epoch : batch 281 Loss: 0.010312672004807975]
[2024-04-17 13:22:11,822: INFO: main: Eval Epoch : batch 282 Loss: 0.007900100572630395]
[2024-04-17 13:22:12,029: INFO: main: Eval Epoch : batch 283 Loss: 0.008351191126984826]
[2024-04-17 13:22:12,232: INFO: main: Eval Epoch : batch 284 Loss: 0.012901758252722162]
[2024-04-17 13:22:12,439: INFO: main: Eval Epoch : batch 285 Loss: 0.002368428788526783]
[2024-04-17 13:22:12,651: INFO: main: Eval Epoch : batch 286 Loss: 0.0008382223389556838]
[2024-04-17 13:22:12,865: INFO: main: Eval Epoch : batch 287 Loss: 0.0034208589176832824]
[2024-04-17 13:22:13,073: INFO: main: Eval Epoch : batch 288 Loss: 0.0032454219080045787]
[2024-04-17 13:22:13,275: INFO: main: Eval Epoch : batch 289 Loss: 0.0038568892379761133]
[2024-04-17 13:22:13,477: INFO: main: Eval Epoch : batch 290 Loss: 0.010249807122580308]
[2024-04-17 13:22:13,685: INFO: main: Eval Epoch : batch 291 Loss: 0.000511134940547731]
[2024-04-17 13:22:13,889: INFO: main: Eval Epoch : batch 292 Loss: 0.01731650459844004]
[2024-04-17 13:22:14,091: INFO: main: Eval Epoch : batch 293 Loss: 0.0043201312005656575]
[2024-04-17 13:22:14,288: INFO: main: Eval Epoch : batch 294 Loss: 0.009929507171038845]
[2024-04-17 13:22:14,490: INFO: main: Eval Epoch : batch 295 Loss: 0.0021662870259815037]
[2024-04-17 13:22:14,697: INFO: main: Eval Epoch : batch 296 Loss: 0.02041915379557751]
[2024-04-17 13:22:14,901: INFO: main: Eval Epoch : batch 297 Loss: 0.02469187995772456]
[2024-04-17 13:22:15,101: INFO: main: Eval Epoch : batch 298 Loss: 0.018766790622070727]
[2024-04-17 13:22:15,304: INFO: main: Eval Epoch : batch 299 Loss: 0.013688912466985104]
[2024-04-17 13:22:15,506: INFO: main: Eval Epoch : batch 300 Loss: 0.022557497610724116]
[2024-04-17 13:22:15,709: INFO: main: Eval Epoch : batch 301 Loss: 0.0004926928548048252]
[2024-04-17 13:22:15,921: INFO: main: Eval Epoch : batch 302 Loss: 0.008935349997383752]
[2024-04-17 13:22:16,124: INFO: main: Eval Epoch : batch 303 Loss: 0.00032942355727531316]
[2024-04-17 13:22:16,325: INFO: main: Eval Epoch : batch 304 Loss: 0.0012636849407386553]
[2024-04-17 13:22:16,524: INFO: main: Eval Epoch : batch 305 Loss: 0.006152794480539388]
[2024-04-17 13:22:16,726: INFO: main: Eval Epoch : batch 306 Loss: 0.005995162557045571]
[2024-04-17 13:22:16,929: INFO: main: Eval Epoch : batch 307 Loss: 0.0033975254566705314]
[2024-04-17 13:22:17,132: INFO: main: Eval Epoch : batch 308 Loss: 0.010150779510987695]
[2024-04-17 13:22:17,332: INFO: main: Eval Epoch : batch 309 Loss: 0.025801636050480328]
[2024-04-17 13:22:17,532: INFO: main: Eval Epoch : batch 310 Loss: 0.03672942954892566]
[2024-04-17 13:22:17,735: INFO: main: Eval Epoch : batch 311 Loss: 0.005430525634486367]
[2024-04-17 13:22:17,941: INFO: main: Eval Epoch : batch 312 Loss: 0.003282712251013123]
[2024-04-17 13:22:18,143: INFO: main: Eval Epoch : batch 313 Loss: 0.011593965484913822]
[2024-04-17 13:22:18,344: INFO: main: Eval Epoch : batch 314 Loss: 0.01226754321457392]
[2024-04-17 13:22:18,542: INFO: main: Eval Epoch : batch 315 Loss: 0.000905109303503743]
[2024-04-17 13:22:18,749: INFO: main: Eval Epoch : batch 316 Loss: 0.010017196222107845]
[2024-04-17 13:22:18,958: INFO: main: Eval Epoch : batch 317 Loss: 0.0008767783058961899]
[2024-04-17 13:22:19,158: INFO: main: Eval Epoch : batch 318 Loss: 0.011264383767989572]
[2024-04-17 13:22:19,354: INFO: main: Eval Epoch : batch 319 Loss: 0.0049768839875223795]
[2024-04-17 13:22:19,554: INFO: main: Eval Epoch : batch 320 Loss: 0.0007929278762401675]
[2024-04-17 13:22:19,762: INFO: main: Eval Epoch : batch 321 Loss: 0.006608028032915328]
[2024-04-17 13:22:19,964: INFO: main: Eval Epoch : batch 322 Loss: 0.04855334208663495]
[2024-04-17 13:22:20,168: INFO: main: Eval Epoch : batch 323 Loss: 0.010462408479402536]
[2024-04-17 13:22:20,367: INFO: main: Eval Epoch : batch 324 Loss: 0.029810508150371357]
[2024-04-17 13:22:20,570: INFO: main: Eval Epoch : batch 325 Loss: 0.0026212303702195868]
[2024-04-17 13:22:20,779: INFO: main: Eval Epoch : batch 326 Loss: 0.03306204951655469]
[2024-04-17 13:22:20,982: INFO: main: Eval Epoch : batch 327 Loss: 0.008587136974000058]
[2024-04-17 13:22:21,181: INFO: main: Eval Epoch : batch 328 Loss: 0.0018515080692069143]
[2024-04-17 13:22:21,380: INFO: main: Eval Epoch : batch 329 Loss: 0.008762868234321292]
[2024-04-17 13:22:21,585: INFO: main: Eval Epoch : batch 330 Loss: 0.00535254235764844]
[2024-04-17 13:22:21,790: INFO: main: Eval Epoch : batch 331 Loss: 0.015216957005304614]
[2024-04-17 13:22:21,992: INFO: main: Eval Epoch : batch 332 Loss: 0.008378476265954903]
[2024-04-17 13:22:22,195: INFO: main: Eval Epoch : batch 333 Loss: 0.028802321583208862]
[2024-04-17 13:22:22,394: INFO: main: Eval Epoch : batch 334 Loss: 0.03705798824185443]
[2024-04-17 13:22:22,598: INFO: main: Eval Epoch : batch 335 Loss: 0.0022851702965683966]
[2024-04-17 13:22:22,804: INFO: main: Eval Epoch : batch 336 Loss: 0.0028786856038077666]
[2024-04-17 13:22:23,007: INFO: main: Eval Epoch : batch 337 Loss: 0.04129023395256865]
[2024-04-17 13:22:23,208: INFO: main: Eval Epoch : batch 338 Loss: 0.0002526460460311197]
[2024-04-17 13:22:23,408: INFO: main: Eval Epoch : batch 339 Loss: 0.0013501864115979666]
[2024-04-17 13:22:23,614: INFO: main: Eval Epoch : batch 340 Loss: 0.007782646117464338]
[2024-04-17 13:22:23,822: INFO: main: Eval Epoch : batch 341 Loss: 0.02047472556400282]
[2024-04-17 13:22:24,028: INFO: main: Eval Epoch : batch 342 Loss: 0.0002551079084292192]
[2024-04-17 13:22:24,238: INFO: main: Eval Epoch : batch 343 Loss: 0.01946983124396242]
[2024-04-17 13:22:24,446: INFO: main: Eval Epoch : batch 344 Loss: 0.02523492834621172]
[2024-04-17 13:22:24,656: INFO: main: Eval Epoch : batch 345 Loss: 0.017973045440715242]
[2024-04-17 13:22:24,862: INFO: main: Eval Epoch : batch 346 Loss: 0.0034615267740047075]
[2024-04-17 13:22:25,065: INFO: main: Eval Epoch : batch 347 Loss: 0.01388879824891664]
[2024-04-17 13:22:25,278: INFO: main: Eval Epoch : batch 348 Loss: 0.013615008793020562]
[2024-04-17 13:22:25,490: INFO: main: Eval Epoch : batch 349 Loss: 0.006635950616829759]
[2024-04-17 13:22:25,698: INFO: main: Eval Epoch : batch 350 Loss: 0.00046475370502129865]
[2024-04-17 13:22:25,908: INFO: main: Eval Epoch : batch 351 Loss: 0.01716177426487189]
[2024-04-17 13:22:26,113: INFO: main: Eval Epoch : batch 352 Loss: 0.003870090329863179]
[2024-04-17 13:22:26,319: INFO: main: Eval Epoch : batch 353 Loss: 0.009084185997301649]
[2024-04-17 13:22:26,529: INFO: main: Eval Epoch : batch 354 Loss: 0.015363731258973305]
[2024-04-17 13:22:26,736: INFO: main: Eval Epoch : batch 355 Loss: 0.031062681498982458]
[2024-04-17 13:22:26,946: INFO: main: Eval Epoch : batch 356 Loss: 0.00284820559025793]
[2024-04-17 13:22:27,051: INFO: main: Eval Epoch : batch 357 Loss: 0.004742143489110265]
[2024-04-17 13:22:41,863: INFO: main: The score of the eval model is {'Accuracy': 0.9940568623412407, 'precision': 0.756737289188048, 'recall': 0.8810329371620254, 'f1': 0.8141685150049438}]
